========================================
GAVO DC Software Reference Documentation
========================================

:Author: Markus Demleitner
:Email: gavo@ari.uni-heidelberg.de

.. contents:: 
  :depth: 2
  :backlinks: entry
  :class: toc


Resource Descriptor Element Reference
=====================================

The following (XML) elements are defined for resource descriptors.  Some
elements are polymorous (Grammars, Cores).  See below for a reference
on the respective real elements known to the software.


Each element description gives a general introduction to the element's
use (complain if it's too technical; it's not unlikely that it is since
these texts are actually the defining classes' docstrings).

Within RDs, element properties that can (but need not) be written in XML
attributes, i.e., as a single string, are called "atomic".  Their types
are given in parentheses after the attribute name along with a default
value.

In general, items defaulted to Undefined are mandatory.  Failing to
give a value will result in an error at RD parse time.

Within RD XML documents, you can (almost always) give atomic children
either as XML attribute (``att="abc"``) or as child elements
(``<att>abc</abc>``).  Some of the "atomic" attributes actually contain
lists of items.  For those, you should normally write multiple child
elements (``<att>val1</att><att>val2</att>``), although sometimes it's
allowed to mash together the individual list items using a variety of
separators.

Here are some short words about the types you may encounter, together
with valid literals:

* boolean – these allow quite a number of literals; use ``True`` and
  ``False`` or ``yes`` and ``no`` and stick to your choice.
* unicode string – there may be additional syntactical limitations on
  those.  See the explanation
* integer – only decimal integer literals are allowed
* id reference – these are references to items within XML documents; all
  elements within RDs can have an ``id`` attribute, which can then be
  used as an id reference.  Additionally, you can reference elements
  in different RDs using <rd-id>#<id>.  Note that DaCHS does not support
  forward references (i.e., references to items lexically behind the
  referencing element).
* list of id references – Lists of id references.  The
  values could be mashed together with commas, but prefer multiple child
  elements.

There are also "Dict-like" attributes.  These are built from XML like::

  <d key="ab">val1</d>
  <d key="cd">val2</d>

In addition to key, other (possibly more descriptive) attributes for the
key within these mappings may also be allowed.  In special circumstances
(in particular with properties) it may be useful to add to a value::

  <property key="brokencols">ab,cd</property>
  <property key="brokencols" cumulative="True">,x</property>

will leave ``ab,cd,x`` in brokencols.

Many elements can also have "structure children".  These correspond to
compound things with attributes and possibly children of their own.
The name given at the start of each description is irrelevant to the
pure user; it's the attribute name you'd use when you have the
corresponding python objects.  For authoring XML, you use the name in
the following link; thus, the phrase "colRefs (contains Element
columnRef..." means you'd write ``<columnRef...>``.

Here are some guidelines as to the naming of the attributes:

* Attributes giving keys into dictionaries or similar (e.g., column
  names) should always be named ``key``
* Attributes giving references to some source of events or data
  should always be named ``source``, never "src" or similar
* Attributes referencing generic things should always be called
  ``ref``; of course, references to specific things like tables or
  services should indicate in their names what they are supposed to
  reference.


.. replaceWithResult getStructDocs(docStructure)


Active Tags
===========

The following tags are "active", which means that they do not directly
contribute to the RD parsed.  Instead they define, replay, or edit
streams of elements.

.. replaceWithResult getActiveTagDocs(docStructure)


Grammars Available
==================

The following elements are all grammar related.  All grammar elements
can occur in data descriptors.

.. replaceWithResult getGrammarDocs(docStructure)

Cores Available
===============

The following elements are related to cores.  All cores can only occur
toplevel, i.e. as direct children of resource descriptors.  Cores are
only useful with an id to make them referencable from services using
that core.

.. replaceWithResult getCoreDocs(docStructure)

Mixins
======

Mixins ensure a certain functionality on a table.  Typically, this is
used to provide certain guaranteed fields to particular cores.  For many
mixins, there are predefined procedures (both rowmaker applys and
grammar rowfilters) that should be used in grammars and/or rowmakers
feeding the tables mixing in a given mixin.

.. replaceWithResult getMixinDocs(docStructure)


Triggers
========

In the context of the GAVO DC, triggers are conditions on rows -- either
the raw rows emitted by grammars if they are used within grammars, or
the rows about to be shipped to a table if they are used within
tables.  Triggers may be used recursively, i.e., triggers may contain
more triggers.  Child triggers are normally or-ed together.

Currently, there is one useful top-level trigger, the `element
ignoreOn`_.  If an ignoreOn is triggered, the respective row is silently
dropped (actually, you ignoreOn has a bail attribute that allows you to
raise an error if the trigger is pulled; this is mainly for debugging).

The following triggers are defined:

.. replaceWithResult getTriggerDocs(docStructure)


Renderers Available
===================

The following renderers are available for allowing and URL creation.
The parameter style is relevant when adapting `condDescs`` or table
based cores to renderers:

* With clear, parameters are just handed through
* With form, suitable parameters are turned into vizier-like expressions
* With pql, suitable parameters are turned into their PQL counterparts,
  letting you specify ranges and such.

Unchecked renderers can be applied to any service and need not be
explicitely allowed by the service.

.. replaceWithResult getRendererDocs(docStructure)


Predefined Procedures
=====================

Procedures available for rowmaker apply
'''''''''''''''''''''''''''''''''''''''

.. replaceWithResult makeRmkProcDocs(docStructure)


Procedures available for grammar rowfilters
'''''''''''''''''''''''''''''''''''''''''''

.. replaceWithResult makeRowfilterDocs(docStructure)




Metadata
========

Various elements support the setting of metadata through meta elements.
Metadata is used for conveying RMI-style metadata used in the VO
registry.  See [RMI]_ for an overview of those.  We use the keys given
in RMI, but there are some extensions discussed in `RMI-style
Metadata`_.

The other big use of meta information is for feeding templates.  Those
"local" keys should all start with an underscore.  You are basically
free to use those as you like and fetch them from your custom templates.
The predefined templates already have some meta items built in,
discussed in `Template Metadata`.

So, metadata is a key-value mapping.  Keys may be compound like in RMI,
i.e., they may consist of period-separated atoms, like
publisher.address.email.  There may be multiple items for each meta
key.

Meta inheritance
''''''''''''''''

When you query an element for metadata, it first sees if it has this
metadata.  If that is not the case, it will ask its meta parent.  This
usually is the embedding element.  It wil again delegate the request to
its parent, if it exists.  If there is no parent, configured defaults
are examined.  These are taken from rootDir/etc/defaultmeta, where they
are given as colon-separated key-value pairs, e.g.,

::

  publisher: The GAVO DC team
  publisherID: ivo://org.gavo.dc
  contact.name: GAVO Data Center Team
  contact.address: Moenchhofstrasse 12-14, D-69120 Heidelberg
  contact.email: gavo@ari.uni-heidelberg.de
  contact.telephone: ++49 6221 54 1837
  creator.name: GAVO Data Center
  creator.logo: http://vo.ari.uni-heidelberg.de/docs/GavoTiny.png

The effect is that you can give global titles, descriptions, etc.
in the RD but override them in services, tables, etc.  The configured
defaults let you specify meta items that are probably constant for
everything in your data center, though of course you can override these
in your RD elements, too.

In HTML templates, missing meta usually is not an error.  The
corresponding elements are just left empty.  In registry documents,
missing meta may be an error.

Meta formats
''''''''''''

Metadata must work in registry records as well as in HTML pages and
possibly in other places.  Thus, it should ideally be given in formats
that can be sensibly transformed into the various formats.

The GAVO DC software knows four input formats:

literal
  The textual content of the element will not be touched.  In
  HTML, it will end up in a div block of class literalmeta.

plain
  The textual content of the element will be whitespace-normalized,
  i.e., whitespace will be stripped from the start and the end,
  runs of blanks and tabs are replaced by a single blank, and empty
  lines translate into paragraphs.  In HTML, these blocks com in
  plainmeta div elements.  

rst
  The textual content of the element is interpreted as restructured
  text.  When requested as plain text, the restructured text itself is
  returned, in HTML, the standard docutils rendering is returned.

raw
  The textual content of the element is not touched.  It will be
  embedded into HTML directly.  You can use this, probably together
  with CDATA sections, to embed HTML -- the other formats should not
  contain anything special to HTML (i.e., they should be PCDATA in
  XML lingo).  While the software does not enforce this, raw content
  should not be used with RMI-type metadata.  Only use it for items that
  will not be rendered outside of HTML templates.


Macros in Meta Elements
'''''''''''''''''''''''

Macros will be expanded in meta items using the embedding element as
macro processors (i.e., you can use the macros defined by this element).


Typed Meta Elements
'''''''''''''''''''

While generally the DC software does not care what you put into meta
items and views them all as strings, certain elements are treated
specially.  The following meta element "types" are currently defined:

.. replaceWithResult getMetaTypeDocs()

While in XML meta, you can explicitely set the type of a meta item
(using the type attribute), it is more common to just use the inferred
type by the meta name.  Currently, the following meta keys imply types:

.. replaceWithResult getMetaTypedNames()

For type inference, only the last component of a meta path is
significant, i.e., both creator.logo and publisher.logo are of type
logo.


Metadata in Standard Renderers
''''''''''''''''''''''''''''''

Certain meta keys have a data center-internal interpretation, used
in renderers or writers of certain formats.  These keys should always
start with an underscore.  Among those are:

* _intro -- used by the standard HTML template for explanatory text
  above the seach form.
* _bottominfo -- used by the standard HTML template for explanatory text
  below the seach form.
* _copyright -- used by the standard HTML template for copyright-related
  information (there's also copyright in RMI; the one with the
  underscore is intended to be less formal).
* _related -- used in the standard HTML template for links to related
  services.  As listed above, this is a link, i.e., you can give a
  title attribute.
* _longdoc -- used by the service info renderer for an explanatory
  piece of text of arbitrary length.  This will usually be in
  reStructured text, and we recommend having the whole meta body in a
  CDATA section.
* _news -- news on the service.  See above at `Typed Meta Elements`_.
* _warning -- used by both the VOTable and the HTML table renderer.
  The content is rendered as some kind of warning.  Unfortunately,
  there is no standard how to do this in VOTables.  There is no
  telling if the info elements generated will show anywhere.
* _noresultwarning -- displayed by the default response template instead
  of an empty table (use it for things like "No Foobar data for your
  query")
* _type -- on Data instances, used by the VOTable writer to set the
  ``type`` attribute on ``RESOURCE`` elements (to either "results"
  or "meta").  Probably only useful internally.
* _plotOptions – typically set on services, this lets you configure
  the initial appearance of the javascript-based quick plot.  The value
  must be a javascript dictionary literal (like ``{"xselIndex": 2}``)
  unless you're trying CSS deviltry (which you could, using this meta;
  then again, if you can inject RDs, you probably don't need CSS attacks).
  Keys evaluated include:

  * xselIndex – 0-based index of the column plotted on the x-axis 
    (default: 0)
  * yselIndex – 0-based index of the column plotted on the y-axis
    (default: length of the column list; that's "histogram on y)
  * usingIndex – 0-based index of the plotting style selector.  For
    now, that's 0 for points and 1 for lines.


RMI-Style Metadata
''''''''''''''''''

For services (and other things) that are registred in the Registry, you
must give certain metadata items (and you can give more), where we take
their keys from [RMI]_.  We provide a `explanatory leaflet
<./data_checklist.pdf>`_ for data providers.  The most common keys --
used by the registry interface and in part by HTML and VOTable
renderers -- include:

* title -- this should in general be given seperately on the resource,
  each table, and each service.  In simple cases, though, you may get by
  by just having one global title on the resource and rely on metdata
  inheritance.
* shortName -- a string that should indicate what the service is in 16
  characters or less.
* creationDate -- Use ISO format with time, UTC only, like this: 
  2007-10-04T12:00:00Z
* subject -- as noted in the explanatory leaflet, these should be taken
  from the `IVOA Vocabulary Explorer
  <http://explicator.dcs.gla.ac.uk/WebVocabularyExplorer/>`_.
* copyright -- freetext copyright notice.
* source -- bibcodes will be expanded to ADS links here.
* referenceURL -- again, a link, so you can give a title for
  presentation purposes.  If you give no referenceURL, the service's
  info page will be used.
* dateUpdated -- an ISO date.  Do not set this.  This is determined
  from timestamps in DaCHS's state directory.  There is also
  datetimeUpdated that you would have to keep in sync with dateUpdated
  if you were to change it.
* creator.name -- this should be the name of the "author" of the data
  set.  See below for multiple creators.  If you set this, you may want
  to override creator.logo as well.
* content.type – one of Other, Archive, Bibliography, Catalog, 
  Journal, Library, Simulation, Survey, Transformation, Education, 
  Outreach, EPOResource, Animation, Artwork, Background, BasicData, 
  Historical, Photographic, Press, Organisation, Project, Registry –
  it's optional and we doubt its usefulness.
* facility -- no IVOA ids are supported here yet, but probably this
  should change.
* coverage -- see the special section
* service-specific metadata (for SIA, SCS, etc.) -- see the
  documentation of the respective cores.
* utype – tables (and possibly other items) can have utypes to signify
  their role in specific data models.  For tables, this utype gets
  exported to the tap_schema.

While you can set any of these in etc/defaultmeta.txt, the following items
are usually set there:

* publisher
* publisherID
* contact.name
* contact.address
* contact.email
* contact.telephone

The creator.name meta illustrates a pitfall with our metadata
definition.  Suppose you had more than one creator.  What you'd want is
a metadata structure like this::

  +-- creator -- name (Arthur)
  |
  +-- creator -- name (Berta)

However, if you write::

  creator.name: Arthur
  creator.name: Berta

or, equivalently::

  <meta name="creator.name">Arthur</meta>
  <meta name="creator.name">Berta</meta>

by the above rules, you'll get this::

  +-- creator -- name (Arthur)
         |
         +------ name (Berta)

i.e., one creator with two names.

To avoid this, make a new creator node in between, i.e., write::

  creator.name: Arthur
  creator:
  creator.name: Berta

In DaCHS resources, it's better to be explicit about the tree structure
(though you could write it like in metastream)::

  <meta name="creator">
    <meta name="name">Arthur</meta>
  </meta>
  <meta name="creator">
   <meta name="name">Berta</meta>
  </meta>

However, for creator.name specifically, it's highly likely that people
accept things like "Arthur; Berta" anyway, and so here it might be
better to disregard the tree structure issues entirely.

Actually, the DaCHS internal author table as used by the alternative
portal interprets one special notation::

  <author1>, <inits1> {; <authorn>, <initsn>}

That is, you should write authors lists like "Foo, X.; Bar, Q.; et al".


Coverage Metadata
'''''''''''''''''

Coverage metadata probably is the most complex piece of metadata, but
also potentially the most useful, since it would allow clients to
restrict querying to services known to contain relevant material.  So,
try to get it right.

Within DaCHS, coverage metadata uses the following keys:

* coverage.profile – an STC-S string giving the coverage of the service.
  These can become rather complex.  We implement several extensions to
  STC-S.  See also the `documentation for GAVO STC`_
* coverage.waveband – One of Radio, Millimeter, Infrared, Optical, UV,
  EUV, X-ray, Gamma-ray, and you can have multiple waveband
  specifications.  Note that you can provide much more detailed
  information on the covered spectral range as part of coverage.profile
  (but it's also much less likely that there is proper support for data
  there in registries and clients).
* coverage.regionOfRegard – in essence, the "pixel size" of the service in
  degrees.  If, for example, your service gives data on a lattice of
  sampling points, the typical distance of such points should be given
  here.  Leave out if this doesn't apply to your service.
* coverage.footprint – reserved; this will probably be filled in
  automatically by the software once we have a footprint standard and
  DaCHS implements it.

Here's an example for a service covering the large and small magellanic
clouds::

  <meta name="coverage">
    <meta name="profile">
      Union ICRS (
        Box 81 69.75 14 3.25
        Box 13 -73 9 2)</meta>
    <meta name="waveband">Optical</meta>
    <meta name="waveband">Infrared</meta>
    <meta name="regionOfRegard">0.02</meta>
  </meta>


.. _Documentation for GAVO STC: http://docs.g-vo.org/DaCHS/stc.html

Meta Stream Format
''''''''''''''''''

In serveral places, most notably in the ``defaultmeta.txt`` file and in
meta elements without a ``name`` attribute, you can give metadata as a
"meta stream".  This is just a sequence of lines containing pairs of
<meta key> and <meta value>.

In addition, there are comments, empty lines, and continuations.
Continuation lines work by ending a line with a backslash.  The
following line separator and all blanks and tabs following it are
then ignored.  Thus, the following two meta keys end up having identical
values::

  meta1: A contin\
    uation line needs \
      a blank if you wan\
  t one.
  meta2: A continuation line needs a blank if you want one

Note that whitespace behind a backslash prevents it from being a
continuation character.  That is, admittedly, a bit of a trap.

Other than their use as continuation characters, backslashes have no
special meaning within meta streams as such.  Within meta elements,
however, macros are expanded after continuation line processing if the
meta parent knows how to expand macros.  This lets you write things
like::

  <meta>
    creationDate: \metaString{authority.creationDate}
    managingOrg:ivo://\getConfig{ivoa}{authority}
  </meta>


Comments and empty lines are easy: Empty lines are allowed, and a
comment is a line with a hash (#) as its first non-whitespace
character.  Both constructs are ignored, and you can even continue
comments (though you should not).

Meta information can have a complex tree structure.  With meta streams,
you can build trees by referencing dotted meta identifiers.  If you
specify meta information for an item that already exists, a sibling will
be created.  Thus, after::

  creator.name: A. Author
  creator:
  creator.name: B. Buthor

there are two creator elements, each specifying a name meta.  For the
way creators are specified within VOResource, the following would be
wrong::

  creator.name: This is wrong.
  creator.name: and will not work

-- you would have a single creator meta with two name metas, which is
not allowed by VOResource.

If you write::

  contact.address: 7 Miner's Way, Behind the Seven Mountains
  contact.email: dwarfs@fairytale.fa

you have a single contact meta giving address and email.


Display Hints
=============

Display hints use an open vocabulary.  As you add value formatters, you can 
evaluate any display hint you like.  Display hints understood by the
built-in value formatters include:

checkmark
  in HTML tables, render this column as empty or checkmark depending on
  whether the value is false or true to python.

displayUnit
  use the value of this hint as the unit to display a value in.

humanTime
  display values as h:m:s.

nopreview
  if this key is present with any value, no HTML code to generate
  previews when mousing over a link will be generated.

sepChar
  a separation character for sexagesimal displays and the like.

sf
  "Significant figures" -- length of the mantissa for this column.
  Will probably be replaced by a column attribute analoguous to what
  VOTable does.

type
  a key that gives hints what to do with the column.  Values currently
  understood include:

  bar
    display a numeric value as a bar of length value pixels.

  bibcode
    display the value as a link to an ADS bibcode query.

  humanDate
    display a timestamp value or a real number in either yr (julian
    year), d (JD, or MJD if xtype is mjd), or s (unix timestamp) as 
    an ISO string.

  humanDay
    display a timestamp or date value as an ISO string without time.

  keephtml
    lets you include raw HTML.  In VOTables, tags are removed.

  product
    treats the value as a product key and expands it to a URL for the
    product (i.e., typically image).  This is defined in
    protocols.products.  This display hint is also used by, e.g., the tar
    format to identify which columns should contribute to the tar file.

  dms
    format a float as degree, minutes, seconds.

  simbadlink
    formats a column consisting of alpha and delta as a link to query
    simbad.  You can add a coneMins displayHint to specify the search
    radius.

  suppress
    do not automatically include this column in any table (e.g.,
    verbLevel-based column selection).

  hms
    force formatting of this column as a time (usually for RA).

  url
    makes value a link in HTML tables.

  imageURL
    makes value the src of an image.  Add width to force a certain
    image size.

noxml
  if 'true' (exactly like this), do not include this column in VOTables.

width
  preview width in pixels.

Note that not any combination of display hints is correctly
interpreted.  The interpretation is greedy, and only one formatter at a
time attempts to interpret display hints.


Building Service Interfaces
===========================

Within DaCHS, a request is processed as follows:

1) The core is adapted to the renderer; this means that condDescs with
   buildFrom are converted to inputKeys according to the rules of the
   renderer.  The form renderer generates VizieR-like expressions,
   protocol renderers make PQL parameters, etc.  Also, onlyForRenderer
   and notForRenderer inputKeys are selected or deselected
2) From the core's inputTable, the service builds an input data
   descriptor (unless the service has an inputDD defined already, of
   course).  Most standard cores only get an input table's parameters
   (the exception being the computedCore), and hence the automatic
   inputDD will only have a parmaker.  The inputDD will have a
   ContextGrammar, which does not yield rows.
3) The service will build the input table using its inputDD.  The input
   is either nevow request.args, mapping each key to a sequence of strings, or
   a dictionary containing parsed values coming from nevow formal.
4) The input table is passed to the core, which produces either a table,
   a data instance, or a pair of mime-type and content.
5) From the core result, an SvcResult is built.  This is relevant when
   the service has an outputTable defined, in which case the table
   structure is adapted if the input actually is a table.
6) The renderer formats the SvcResult according to its wishes.

TBC: multiplicity, param values as defaults,


Writing Custom Cores
====================

While DaCHS provides cores for many common operations -- in particular,
database queries and wrapped external binaries --, there are of course
services needing to do things not covered by what the shipped cores do.
Some such cases still follow the basic premise of services: GET or POST
parameters in, something table-like out.  For these cases, use custom
cores (if even this does not provide sufficent functionality, write a
custom renderer).


Defining a Custom Core
''''''''''''''''''''''

To do this, you need to write a python module.  The standard location
for those is in the bin/ subdirectory of the resource directory.

You will usually want to inherit from core::

  from gavo.svcs import core

  class Core(core.Core):

The framework will always look of an object named "Core" in the module
and use this as the custom core.

The core needs an InputTable and an OutputTable like all cores.  You
*could* define it in the resource descriptor like this::

  <customCore id="createCore" module="bin/create">
    <inputTable>
      <inputKey .../>
    </inputTable>
    <outputTable>
      <column name="itemsAdded" type="integer" tablehead="Items added"/>
    </outputTable>
  </customCore>

It's probably a better idea to define it in the code, though, since
then it will work without further specifications.  The definitions
in the code can still be overridden from an RD for special effects.
Embedding the definitions is done using the class attributes
``inputTableXML`` and ``outputTableXML``::

  class Core(core.Core):
    inputTableXML = """<inputTable>
      <inputKey name="fileSrc" type="file" tablehead="Local file"
        description="A local file to upload (overrides source URL if given).">
      <inputKey name="tableName" type="text" tablehead="Target Table"
        description="Name of the table to match against.  
          Only tables available for ADQL (see there) can be used here.">
        <values fromdb="tablename from dc_tables where adql=True"/>
      </inputTable>
      """
    outputTableXML = """<outputTable/>"""

You should not override the constructor.  If you need to perform
"expensive" instanciations, override the completeElement method, as in
the following template::

  def completeElement(self):
    <your code>
    self._completeElementNext(Core)

The call to _completeElementNext ensures that the remaining
completeElement methods are executed.

Giving the Core Functionality
'''''''''''''''''''''''''''''

To have the core do something, you have to override the run method,
which has to have the following signature::

  run(service, inputTable, queryMeta) -> stuff

The stuff returned will ususally be a Table instance (that need not
match the outputTable definition -- the latter is targetted at the
registry and possibly applications like output field selection).  The
standard renderers also accept a mime type and a string containing
some data and will deliver this as-is.  With custom renderers, you could
return basically anything you want.

Services come up with some idea of the schema of the table they want to
return and adapt tables coming out of the core to this.  Sometimes, you
want to suppress this behaviour, e.g., because the service's ideas are
off.  In that case, set a noPostprocess atttribute on the table to any
value.

service is a service instance.  In particular, you can access the RD you
are running in through its rd attribute.  This is useful if you need to
resolve, e.g., table references (which, in this case, could be given as
a service property)::

  pertainingTable = service.rd.getById(
    service.getProperty("pertainingTable"))

inputTable is a Table instance. Unless the service has a fancy inputDD,
you simply find the inputKey values in the table's parameters::

  val = inputTable.getParam("fileSrc")



Errors
''''''

To bail out from processing, raise a validation error.  Construct it
with a message and the name of an input key.  At least for the form
renderer, this causes a sensible error message with some hint as the the
originating input field::

  raise base.ValidationError("Invalid file name", "rdsrc")


Database Options
''''''''''''''''

The standard DB cores receive a "table widget" on form generation,
including sort and limit options.  To make the Form renderer output this
for your core as well, define a method wantsTableWidget() -> True.

The queryMeta that you receive in run has a dbLimit key.  It contains
the user selection or, as a fallback, the global db/defaultLimit value.
These values are integers.

So, if you order a table widget, you should do something like::

  cursor.execute("SELECT .... LIMIT %(queryLimit)s", 
    {"queryLimit": queryMeta["dbLimit"],...})

In general, you should warn people if the query limit was reached; a
simple way to do that is::

  if len(res)==queryLimit:
    res.addMeta("_warning", "The query limit was reached.  Increase it"
      " to retrieve more matches.  Note that unsorted truncated queries"
      " are not reproducible (i.e., might return a different result set"
      " at a later time).")

where res would be your result table.  _warning metadata is displayed in
both HTML and VOTable output, though of course VOTable tools will not
usually display it.

Inheriting from TableBasedCore
''''''''''''''''''''''''''''''

TBD (This does not work right now; complain if you need to do it)


Manufacturing Spectra
=====================

Making SDM Tables
'''''''''''''''''

Compared to images, the formats situation with spectra is a mess.
Therefore, in all likelihood, you will need some sort of conversion
service to VOTables compliant to the spectral data model.  DaCHS has a
facility built in to support you with doing this on the fly, which means
you only need to keep a single set of files around while letting users
obtain the data in some format convenient to them.  The tutorial
contains examples on how to generate metadata records for such
additional formats.

First, you will have to define the "instance table", i.e., a table
definition that will contain a DC-internal representation of the
spectrum according to the data model.  There's a mixin for that::

  <table id="spectrum">
    <mixin ssaTable="hcdtest">//ssap#sdm-instance</mixin>
  </table>

In addition to adding lots and lots of params, the mixin also defines
two columns, ``spectral`` and ``flux``; these have units and ucds as
taken from the SSA metadata.  You can add additional columns (e.g., a
flux error depending the the spectral coordinate) as requried.

The actual spectral instances can be built by sdmCores and delivered
through DaCHS' product interface.  Note, however, that clients
`supporting getData`_ wouldn't need to do this.  You'll still have to
define the data item defined below.

sdmCores, while potentially useful with common services, are intended to
be used by the product renderer for dcc product table paths.  They
contain a data item that must yield a primary table that is basically
sdm compliant.  Most of this is done by the //ssap#feedSSAToSDM apply
proc, but obviously you need to yield the spectral/flux pairs (plus
potentially more stuff like errors, etc, if your spectrum table has more
columns.  This comes from the data item's grammar, which probably must
always be an embedded grammar, since its sourceToken is an SSA row in a
dictionary.  Here's an example::

  <sdmCore queriedTable="hcdtest" id="mksdm">
    <data id="getdata">
      <embeddedGrammar>
        <iterator>
          <code>
            labels = ("spectral", "flux")
            relPath = self.sourceToken["accref"].split("?")[-1]
            with self.grammar.rd.openRes(relPath) as inF:
              for ln in inF:
                yield dict(zip(labels,ln.split()))
          </code>
        </iterator>
      </embeddedGrammar>
      <make table="spectrum">
        <parmaker>
          <apply procDef="//ssap#feedSSAToSDM"/>
        </parmaker>
      </make>
    </data>
  </sdmCore>

Note: spectral, flux, and possibly further items coming out of the
iterator must be in the units units promised by the SSA metadata
(fluxSI, spectralSI).  Declarations to this effect are generated by the
``//ssap#sdm-instance`` mixin for the spectral and flux columns.

The sdmCores are always combined with the sdm renderer.  It passes an
accref into the core that gets turned into an row from queried table;
this must be an "ssa" table (i.e., right now something that mixes in
``//ssap#hcd``).  This row is the input to the embedded data descriptor.
Hence, this has no sources element, and you must have either a custom
or embedded grammar to deal with this input.




Supporting getData
==================

DaCHS supports the preliminary getData specification by `Demleitner and
Skoda (2012)`_.  This means that you can emit spectra in lots of
different formats without having to clutter your SSA table, do cutouts
and simple normalization, and possibly more.

Of course, to do all this, DaCHS must again be taught to understand the
spectra.  This works as explained in `Making SDM Tables`_.  In the
example there, the embedded data element already has the id
``getdata``, ready to be referenced.

To enable getData on an SSA service, just add a property called
``tablesource`` to it, pointing to this data element, like so::

  <service id="ssa">
    ...
    <property name="tablesource">getdata</property>
    ...
  </service>


Note, however, that to make that work, the spectral coordinate must be a
wavelength (but this is already true for the rest of the current spectra
handling system), and the wavelength must be in what was given as
spectralSI in the SSA mixin.

.. _Demleitner and Skoda (2012): http://docs.g-vo.org/ssaevolution.html



Adapting Obscore
================

You may want extra, locally-defined columns in your obscore tables.  To
support this, there are two hooks in obscore that you can exploit.
To fill these hooks, use ``userconfig.rd`` (TODO: more
documentation on that as we use it more; meanwhile: get a `template from
SVN`_ and put it into GAVO_ROOT/etc).  It helps to have a brief look at
the ``//obscore`` RD to get an idea where these hooks go.

Within the template ``userconfig.rd``, there are already two STREAMs
with ids starting with obscore.; these are referenced from within the
system ``//obscore`` RD.  Here's an somewhat more elaborate example::

  <STREAM id="obscore-extracolumns">
    <column name="fill_factor"
      description="Fill factor of the SED"
      verbLevel="20"/>
  </STREAM>

  <STREAM id="obscore-extraevents">
    <property name="obscoreClause" cumulate="True">
      ,
      CAST(\\fillFactor AS real) AS fill_factor,
    </property>
  </STREAM>

(to be on the safe side: there need to be four backslashes in front of
fillFactor; this is just a backslash doubly-escaped.  Sorry about this).
With this, you could, in an RD, say::

  <mixinDef original="//obscore#publishSSAPHCD" id="obscore-with-fill">
    <mixinPar name="fillFactor" description="The SED's fill factor"/>
  </mixinDef>

  <table id="specs" onDisk="True">
    <mixin ...>//ssap#hcd</mixin>
    <mixin
      ... (all the //obscore#publishSSAPHCD parameters)
      fillFactor="0.3">obscore-with-fill</mixin>
  </table>

What's going on here?  Well, ``obscore-extracolumns`` is easy – this
material is directly inserted into the definition of the obscore view
(see the table with id ``ObsCore`` within the ``//obscore`` RD).  You
could abuse it to insert other stuff than columns but probably should
not (current exception: you probably need to fix the ``viewStatement``
in //obscore to include sufficient columns; we're trying to figure out a
better solution).

The tricky part is ``obscore-extraevents``.  This goes into the
``//obscore#_publishCommon`` STREAM and ends up in all the publish
mixins in obscore.  Again, you could insert mixinPars and similar at
this point, but the only thing you really must do is add lines to the
big SQL fragment in the ``obscoreClause`` property that the mixin leaves
in the table.  This is what is made into the table's contribution to the
big obscore union. Just follow the example above and, in particular,
always CAST to the type you ave in the metadata, since individual tables
might have NULLs in the values, and you do not want misguided attempts
of postgres to do type inference then.

If you actually must know why you need to double-escape fillFactor and
what the magic with the ``cumulate="True"`` is, ask.

The second part plays in some "normal" RD.  First, we copy over one of
the obscore mixins (in this case, we want spectra).  It is then amended
with a mixin parameter that is designed to fill the slot we newly
created in the ``obscore-extraevent`` STREAM.  Then, in the table we do
the normal stuff as if nothing had happened, but instead of mixing in
``//obscore#publishSSAPHCD`` we now use our new ``obscore-with-fill`` mixin
(that you can also references externally using
``id/of/rd#obscore-with-fill``), including the new parameter.

If you change ``%#obscore-extracolumns``, you will need to re-import all
obscore-published tables (actually, importing the metadata using ``gavo
imp -m`` should do).  There currently is no automatic way to traverse
the file system, and you will probably have to first unpublish all
existing tables by connecting to the database and running ``delete from
ivoa._obscoresources``.  If obscore adaption proves a popular feature,
we'll make all this a bit smoother.

.. _template from SVN: http://svn.ari.uni-heidelberg.de/svn/gavo/python/trunk/gavo/resources/inputs/__system__/userconfig.rd





Writing Custom Grammars
=======================

A custom grammar simply is a python module located within a resource
directory defining a row iterator class derived from
gavo.grammars.customgrammar.CustomRowIterator; this class must be called
RowIterator.  You want to override the _iterRows method.  It will have
to yield row dictionaries, i.e., dictionaries mapping string keys to
something (preferably strings, but you will usually get away with
returning complete values even without fancy rowmakers).  

So, a custom grammar module could look like this::

  from gavo.grammars.customgrammar import CustomRowIterator

  class RowIterator(CustomRowIterator):
    def _iterRows(self):
      for i in xrange(10000):
        yield {'index': i, 'square': i**2}

Do not override magic methods, since you may lose row filters, sourceFields,
and the like if you do.  An exception is the constructor.  If you must,
you can override it, but you must call the parent constructor, like
this::

  class RowIterator(CustomRowIterator):
    def __init__(self, grammar, sourceToken, sourceRow=None):
      CustomRowIterator.__init__(self, grammar, sourceToken, sourceRow)
      <your code>

The sourceToken, in general, will be a file name, unless you call
makeData manually and forceSource something else.

A row iterator will be instanciated for each source processed.  Thus,
you should usually not perform expensive operations in the constructor
unless they depend on sourceToken.  In general, you should rather define
a function makeDataPack in the module.  Whatever is returned by this
function is available as self.grammar.dataPack in the row iterator.

The function receives an instance of the the customGrammar as an
argument.  This means you can access the resource descriptor and
properties of the grammar.  As an example of how this could be used,
consider this RD fragment::

  <table id="defTable">
    ...
  </table>

  <customGrammar module="res/grammar">
    <property name="targetTable">defTable</property>
  </customGrammar>

Then you could have the following in res/grammar.py::

  def makeDataPack(grammar):
    return grammar.rd.getById(grammar.getProperty("targetTable"))

and access the table in the row iterator.

Also look into EmbeddedGrammar, which may be a more convenient way to
achieve the same thing.

Dispatching Grammars
''''''''''''''''''''

With normal grammars, all rows are fed to all rowmakers of all makes
within a data object.  The rowmakers can then decide to not process a
given row by raising ``IgnoreThisRow`` or using the trigger mechanism.
However, when filling complex data models with potentially dozens of
tables, this becomes highly inefficient.

When you write your own grammars, you can to better.  Instead of just
yielding a row from ``_iterRows``, you yield a pair of a role (as
specified in the ``role`` attribute of a ``make`` element) and the row.
The machinery will then pass the row only to the feeder for the table in
the corresponding make.

Currently, the only way to define such a dispatching grammar is to use a
custom grammar or an embedded grammar.  For these, just change your
``_iterRows`` and say ``isDispatching="True"`` in the ``customGrammar``
element.  If you implement ``getParameters``, you can return either
pairs of role and row or just the row; in the latter case, the row will
be broadcast to all parmakers.

Special care needs to be taken when a dispatching grammar parses
products, because the product table is fed by a special make inserted
from the products mixin.  This make of course doesn't see the rows you
are yielding from your dispatching grammar.  This means that without
further action, your files will not end up the the product table at all.
In turn, getproducts will return 404s instead of your products.

To fix this, you need to explicitely yield the rows destined for the 
products table with a products role, from within your grammar.  Where
the grammar yield rows for the table with metadata (i.e., rows that actually
contain the fields with prodtblAccref, prodtblPath, etc), yield
to the products table, too, like this: ``yield ("products", newRow)``.


Functions Available for Row Makers
==================================

In principle, you can use arbitrary python expressions in var, map and
proc elements of row makers.  In particular, the namespace in which
these expressions are executed contains math, os, re, time, and datetime
modules as well as gavo.base, gavo.utils, and gavo.coords.

However, much of the time you will get by using the following functions
that are immediately accessible in the namespace:

.. replaceWithResult getRmkFuncs(docStructure)



Scripting
=========

As much as it is desirable to describe tables in a declarative manner,
there are quite a few cases in which some imperative code helps a lot
during table building or teardown.  Resource descriptors let you embed
such imperative code using script elements.  These are children of the
make elements since they are exclusively executed when actually
importing into a table.

Currently, you can enter scripts in SQL and python, which may be called
at various phases during the import.

SQL scripts
'''''''''''

In SQL scripts, you separate statements with semicolons.  Note that no
statements in an SQL script may fail since that will invalidate the
transaction.  This is a serious limitation since you must not commit or
begin transactions in SQL scripts as long as Postgres  does not support
nested transactions.

You can use table macros in the SQL scripts to parametrize them; the
most useful among those probably is ``\curtable`` containing the fully
qualified name of the table being processed.

Python scripts
''''''''''''''

Python scripts can be indented by a constant amount.

The table object currently processed is accessible as table.  In
particular, you can use this to issue queries using 
``table.query(query, arguments)`` (parallel to dbapi.execute) and to
delete rows using ``table.deleteMatching(condition, pars)``.  The
current RD is accessible as ``table.rd``, so you can access items from
the RD as ``table.rd.getById("some_id")``, and the recommended way to
read stuff from the resource directory is
``table.rd.openRes("res/some_file)``.

Some types of scripts may have additional names available.  Currently,
newSource and sourceDone have the name sourceToken – which is the
sourceToken as passed to the grammar.

Script types
''''''''''''

The type of a script corresponds to the event triggering its execution.
The following types are defined right now:

* preImport -- before anything is written to the table
* preIndex -- before the indices on the table are built
* postCreation -- after the table (incl. indices) is finished
* beforeDrop -- when the table is about to be dropped
* newSource -- every time a new source is started
* sourceDone -- every time a source has been processed

Note that preImport, preIndex, and postCreation scripts are not executed
when a table is updated, in particular, in data items with
``updating="True"``.  The only way to run scripts in such circumstances
is to use newSource and sourceDone scripts.


Examples
''''''''

This snippet sets a flag when importing some source (in this case,
that's an RD, so we can access sourceToken.sourceId::

      <script type="newSource" lang="python" id="markDeleted">
        table.query("UPDATE %s SET deleted=True"
          " WHERE sourceRD=%%(sourceRD)s"%id, 
          {"sourceRD": sourceToken.sourceId})
      </script>


This is a hacked way of ensuring some sort of referential integrity:
When a table containing "products" is dropped, the corresponding entries
in the products table are deleted::

  <script type="beforeDrop" lang="SQL" name="clean product table">
    DELETE FROM products WHERE sourceTable='\curtable'
  </script>

Note that this is actually quite hazardous because if the table is
dropped in any way not using the make element in the RD, this will not
be executed.  It's usually much smarter to tell the database to do the
housekeeping.  Rules are typically set in postCreation scripts::

  <script type="postCreation" lang="SQL">
    CREATE OR REPLACE RULE cleanupProducts AS 
      ON DELETE TO \curtable DO ALSO
      DELETE FROM products WHERE key=OLD.accref
  </script>

The decision if such arrangements are make before the import, before the
indexing or after the table is finished needs to be made based on the
script's purpose.

Another use for scripts is SQL function definition::

      <script type="postCreation" lang="SQL" name="Define USNOB matcher">
        CREATE OR REPLACE FUNCTION usnob_getmatch(alpha double precision, 
          delta double precision, windowSecs float
        ) RETURNS SETOF usnob.data AS $$
        DECLARE
          rec RECORD;
        BEGIN
          FOR rec IN (SELECT * FROM usnob.data WHERE 
            q3c_join(alpha, delta, raj2000, dej2000, windowSecs/3600.)) 
          LOOP
            RETURN NEXT rec;
          END LOOP;
        END;
        $$ LANGUAGE plpgsql;
      </script>

You can also load data, most usefully in preIndex scripts (although
beforeImport would work as well here)::

    <script type="preIndex" lang="SQL" name="create USNOB-PPMX crossmatch">
        SET work_mem=1000000;
        INSERT INTO usnob.ppmxcross (
          SELECT q3c_ang2ipix(raj2000, dej2000) AS ipix, p.localid 
          FROM 
            ppmx.data AS p, 
            usnob.data AS u 
          WHERE q3c_join(p.alphaFloat, p.deltaFloat, 
            u.raj2000, u.dej2000, 1.5/3600.))
    </script>



Bibliography
============

.. [RMI]  Hanisch, R., et al, "Resource Metadata for the Virtual
   Observatory", http://www.ivoa.net/Documents/latest/RM.html
