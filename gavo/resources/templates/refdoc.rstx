========================================
GAVO DC Software Reference Documentation
========================================

:Author: Markus Demleitner
:Email: gavo@ari.uni-heidelberg.de

.. contents:: 
  :depth: 2
  :backlinks: entry
  :class: toc


Resource Descriptor Element Reference
=====================================

The following (XML) elements are defined for resource descriptors.  Some
elements are polymorous (Grammars, Cores).  See below for a reference
on the respective real elements known to the software.


Each element description gives a general introduction to the element's
use (complain if it's too technical; it's not unlikely that it is since
these texts are actually the defining classes' docstrings).

Within RDs, element properties that can (but need not) be written in XML
attributes, i.e., as a single string, are called "atomic".  Their types
are given in parentheses after the attribute name along with a default
value.

In general, items defaulted to Undefined are mandatory.  Failing to
give a value will result in an error at RD parse time.

Within RD XML documents, you can (almost always) give atomic children
either as XML attribute (``att="abc"``) or as child elements
(``<att>abc</abc>``).  Some of the "atomic" attributes actually contain
lists of items.  For those, you should normally write multiple child
elements (``<att>val1</att><att>val2</att>``), although sometimes it's
allowed to mash together the individual list items using a variety of
separators.

Here are some short words about the types you may encounter, together
with valid literals:

* boolean – these allow quite a number of literals; use ``True`` and
  ``False`` or ``yes`` and ``no`` and stick to your choice.
* unicode string – there may be additional syntactical limitations on
  those.  See the explanation
* integer – only decimal integer literals are allowed
* id reference – these are references to items within XML documents; all
  elements within RDs can have an ``id`` attribute, which can then be
  used as an id reference.  Additionally, you can reference elements
  in different RDs using <rd-id>#<id>.  Note that DaCHS does not support
  forward references (i.e., references to items lexically behind the
  referencing element).
* list of id references – Lists of id references.  The
  values could be mashed together with commas, but prefer multiple child
  elements.

There are also "Dict-like" attributes.  These are built from XML like::

  <d key="ab">val1</d>
  <d key="cd">val2</d>

In addition to key, other (possibly more descriptive) attributes for the
key within these mappings may also be allowed.  In special circumstances
(in particular with properties) it may be useful to add to a value::

  <property key="brokencols">ab,cd</property>
  <property key="brokencols" cumulative="True">,x</property>

will leave ``ab,cd,x`` in brokencols.

Many elements can also have "structure children".  These correspond to
compound things with attributes and possibly children of their own.
The name given at the start of each description is irrelevant to the
pure user; it's the attribute name you'd use when you have the
corresponding python objects.  For authoring XML, you use the name in
the following link; thus, the phrase "colRefs (contains Element
columnRef..." means you'd write ``<columnRef...>``.

Here are some guidelines as to the naming of the attributes:

* Attributes giving keys into dictionaries or similar (e.g., column
  names) should always be named ``key``
* Attributes giving references to some source of events or data
  should always be named ``source``, never "src" or similar
* Attributes referencing generic things should always be called
  ``ref``; of course, references to specific things like tables or
  services should indicate in their names what they are supposed to
  reference.


.. replaceWithResult getStructDocs(docStructure)


Active Tags
===========

The following tags are "active", which means that they do not directly
contribute to the RD parsed.  Instead they define, replay, or edit
streams of elements.

.. replaceWithResult getActiveTagDocs(docStructure)


Grammars Available
==================

The following elements are all grammar related.  All grammar elements
can occur in data descriptors.

.. replaceWithResult getGrammarDocs(docStructure)

Cores Available
===============

The following elements are related to cores.  All cores can only occur
toplevel, i.e. as direct children of resource descriptors.  Cores are
only useful with an id to make them referencable from services using
that core.

.. replaceWithResult getCoreDocs(docStructure)

Mixins
======

Mixins ensure a certain functionality on a table.  Typically, this is
used to provide certain guaranteed fields to particular cores.  For many
mixins, there are predefined procedures (both rowmaker applys and
grammar rowfilters) that should be used in grammars and/or rowmakers
feeding the tables mixing in a given mixin.

.. replaceWithResult getMixinDocs(docStructure)


Triggers
========

In the context of the GAVO DC, triggers are conditions on rows -- either
the raw rows emitted by grammars if they are used within grammars, or
the rows about to be shipped to a table if they are used within
tables.  Triggers may be used recursively, i.e., triggers may contain
more triggers.  Child triggers are normally or-ed together.

Currently, there is one useful top-level trigger, the `element
ignoreOn`_.  If an ignoreOn is triggered, the respective row is silently
dropped (actually, you ignoreOn has a bail attribute that allows you to
raise an error if the trigger is pulled; this is mainly for debugging).

The following triggers are defined:

.. replaceWithResult getTriggerDocs(docStructure)


Renderers Available
===================

The following renderers are available for allowing and URL creation.
The parameter style is relevant when adapting `condDescs`` or table
based cores to renderers:

* With clear, parameters are just handed through
* With form, suitable parameters are turned into vizier-like expressions
* With pql, suitable parameters are turned into their PQL counterparts,
  letting you specify ranges and such.

Unchecked renderers can be applied to any service and need not be
explicitely allowed by the service.

.. replaceWithResult getRendererDocs(docStructure)


Predefined Procedures
=====================

Procedures available for rowmaker apply
'''''''''''''''''''''''''''''''''''''''

.. replaceWithResult makeRmkProcDocs(docStructure)


Procedures available for grammar rowfilters
'''''''''''''''''''''''''''''''''''''''''''

.. replaceWithResult makeRowfilterDocs(docStructure)


Procedures available for datalink cores
'''''''''''''''''''''''''''''''''''''''

.. replaceWithResult _makeProcsDocumenter(["//datalink#"+ s for s in 
    "fromStandardPubDID", 
    "trivialFormatter",
    "generateProduct",
    "sdm_genDesc",
    "sdm_genData",
    "fits_genDesc",
    "fits_makeWCSParams",
    "fits_makeHDUList",
    "fits_doWCSCutout",
    "fits_formatHDUs",
    ])(docStructure)



Metadata
========

Various elements support the setting of metadata through meta elements.
Metadata is used for conveying RMI-style metadata used in the VO
registry.  See [RMI]_ for an overview of those.  We use the keys given
in RMI, but there are some extensions discussed in `RMI-style
Metadata`_.

The other big use of meta information is for feeding templates.  Those
"local" keys should all start with an underscore.  You are basically
free to use those as you like and fetch them from your custom templates.
The predefined templates already have some meta items built in,
discussed in `Template Metadata`.

So, metadata is a key-value mapping.  Keys may be compound like in RMI,
i.e., they may consist of period-separated atoms, like
publisher.address.email.  There may be multiple items for each meta
key.

Meta inheritance
''''''''''''''''

When you query an element for metadata, it first sees if it has this
metadata.  If that is not the case, it will ask its meta parent.  This
usually is the embedding element.  It wil again delegate the request to
its parent, if it exists.  If there is no parent, configured defaults
are examined.  These are taken from rootDir/etc/defaultmeta, where they
are given as colon-separated key-value pairs, e.g.,

::

  publisher: The GAVO DC team
  publisherID: ivo://org.gavo.dc
  contact.name: GAVO Data Center Team
  contact.address: Moenchhofstrasse 12-14, D-69120 Heidelberg
  contact.email: gavo@ari.uni-heidelberg.de
  contact.telephone: ++49 6221 54 1837
  creator.name: GAVO Data Center
  creator.logo: http://vo.ari.uni-heidelberg.de/docs/GavoTiny.png

The effect is that you can give global titles, descriptions, etc.
in the RD but override them in services, tables, etc.  The configured
defaults let you specify meta items that are probably constant for
everything in your data center, though of course you can override these
in your RD elements, too.

In HTML templates, missing meta usually is not an error.  The
corresponding elements are just left empty.  In registry documents,
missing meta may be an error.

Meta formats
''''''''''''

Metadata must work in registry records as well as in HTML pages and
possibly in other places.  Thus, it should ideally be given in formats
that can be sensibly transformed into the various formats.

The GAVO DC software knows four input formats:

literal
  The textual content of the element will not be touched.  In
  HTML, it will end up in a div block of class literalmeta.

plain
  The textual content of the element will be whitespace-normalized,
  i.e., whitespace will be stripped from the start and the end,
  runs of blanks and tabs are replaced by a single blank, and empty
  lines translate into paragraphs.  In HTML, these blocks com in
  plainmeta div elements.  

rst
  The textual content of the element is interpreted as restructured
  text.  When requested as plain text, the restructured text itself is
  returned, in HTML, the standard docutils rendering is returned.

raw
  The textual content of the element is not touched.  It will be
  embedded into HTML directly.  You can use this, probably together
  with CDATA sections, to embed HTML -- the other formats should not
  contain anything special to HTML (i.e., they should be PCDATA in
  XML lingo).  While the software does not enforce this, raw content
  should not be used with RMI-type metadata.  Only use it for items that
  will not be rendered outside of HTML templates.


Macros in Meta Elements
'''''''''''''''''''''''

Macros will be expanded in meta items using the embedding element as
macro processors (i.e., you can use the macros defined by this element).


Typed Meta Elements
'''''''''''''''''''

While generally the DC software does not care what you put into meta
items and views them all as strings, certain elements are treated
specially.  The following meta element "types" are currently defined:

.. replaceWithResult getMetaTypeDocs()

While in XML meta, you can explicitely set the type of a meta item
(using the type attribute), it is more common to just use the inferred
type by the meta name.  Currently, the following meta keys imply types:

.. replaceWithResult getMetaTypedNames()

For type inference, only the last component of a meta path is
significant, i.e., both creator.logo and publisher.logo are of type
logo.


Metadata in Standard Renderers
''''''''''''''''''''''''''''''

Certain meta keys have a data center-internal interpretation, used
in renderers or writers of certain formats.  These keys should always
start with an underscore.  Among those are:

* _intro -- used by the standard HTML template for explanatory text
  above the seach form.
* _bottominfo -- used by the standard HTML template for explanatory text
  below the seach form.
* _copyright -- used by the standard HTML template for copyright-related
  information (there's also copyright in RMI; the one with the
  underscore is intended to be less formal).
* _related -- used in the standard HTML template for links to related
  services.  As listed above, this is a link, i.e., you can give a
  title attribute.
* _longdoc -- used by the service info renderer for an explanatory
  piece of text of arbitrary length.  This will usually be in
  reStructured text, and we recommend having the whole meta body in a
  CDATA section.
* _news -- news on the service.  See above at `Typed Meta Elements`_.
* _warning -- used by both the VOTable and the HTML table renderer.
  The content is rendered as some kind of warning.  Unfortunately,
  there is no standard how to do this in VOTables.  There is no
  telling if the info elements generated will show anywhere.
* _noresultwarning -- displayed by the default response template instead
  of an empty table (use it for things like "No Foobar data for your
  query")
* _type -- on Data instances, used by the VOTable writer to set the
  ``type`` attribute on ``RESOURCE`` elements (to either "results"
  or "meta").  Probably only useful internally.
* _plotOptions – typically set on services, this lets you configure
  the initial appearance of the javascript-based quick plot.  The value
  must be a javascript dictionary literal (like ``{"xselIndex": 2}``)
  unless you're trying CSS deviltry (which you could, using this meta;
  then again, if you can inject RDs, you probably don't need CSS attacks).
  Keys evaluated include:

  * xselIndex – 0-based index of the column plotted on the x-axis 
    (default: 0)
  * yselIndex – 0-based index of the column plotted on the y-axis
    (default: length of the column list; that's "histogram on y)
  * usingIndex – 0-based index of the plotting style selector.  For
    now, that's 0 for points and 1 for lines.


RMI-Style Metadata
''''''''''''''''''

For services (and other things) that are registred in the Registry, you
must give certain metadata items (and you can give more), where we take
their keys from [RMI]_.  We provide a `explanatory leaflet
<./data_checklist.pdf>`_ for data providers.  The most common keys --
used by the registry interface and in part by HTML and VOTable
renderers -- include:

* title -- this should in general be given seperately on the resource,
  each table, and each service.  In simple cases, though, you may get by
  by just having one global title on the resource and rely on metdata
  inheritance.
* shortName -- a string that should indicate what the service is in 16
  characters or less.
* creationDate -- Use ISO format with time, UTC only, like this: 
  2007-10-04T12:00:00Z
* subject -- as noted in the explanatory leaflet, these should be taken
  from the `IVOA Vocabulary Explorer
  <http://explicator.dcs.gla.ac.uk/WebVocabularyExplorer/>`_.
* copyright -- freetext copyright notice.
* source -- bibcodes will be expanded to ADS links here.
* referenceURL -- again, a link, so you can give a title for
  presentation purposes.  If you give no referenceURL, the service's
  info page will be used.
* dateUpdated -- an ISO date.  Do not set this.  This is determined
  from timestamps in DaCHS's state directory.  There is also
  datetimeUpdated that you would have to keep in sync with dateUpdated
  if you were to change it.
* creator.name -- this should be the name of the "author" of the data
  set.  See below for multiple creators.  If you set this, you may want
  to override creator.logo as well.
* content.type – one of Other, Archive, Bibliography, Catalog, 
  Journal, Library, Simulation, Survey, Transformation, Education, 
  Outreach, EPOResource, Animation, Artwork, Background, BasicData, 
  Historical, Photographic, Press, Organisation, Project, Registry –
  it's optional and we doubt its usefulness.
* facility -- no IVOA ids are supported here yet, but probably this
  should change.
* coverage -- see the special section
* service-specific metadata (for SIA, SCS, etc.) -- see the
  documentation of the respective cores.
* utype – tables (and possibly other items) can have utypes to signify
  their role in specific data models.  For tables, this utype gets
  exported to the tap_schema.

While you can set any of these in etc/defaultmeta.txt, the following items
are usually set there:

* publisher
* publisherID
* contact.name
* contact.address
* contact.email
* contact.telephone

The creator.name meta illustrates a pitfall with our metadata
definition.  Suppose you had more than one creator.  What you'd want is
a metadata structure like this::

  +-- creator -- name (Arthur)
  |
  +-- creator -- name (Berta)

However, if you write::

  creator.name: Arthur
  creator.name: Berta

or, equivalently::

  <meta name="creator.name">Arthur</meta>
  <meta name="creator.name">Berta</meta>

by the above rules, you'll get this::

  +-- creator -- name (Arthur)
         |
         +------ name (Berta)

i.e., one creator with two names.

To avoid this, make a new creator node in between, i.e., write::

  creator.name: Arthur
  creator:
  creator.name: Berta

In DaCHS resources, it's better to be explicit about the tree structure
(though you could write it like in metastream)::

  <meta name="creator">
    <meta name="name">Arthur</meta>
  </meta>
  <meta name="creator">
   <meta name="name">Berta</meta>
  </meta>

However, for creator.name specifically, it's highly likely that people
accept things like "Arthur; Berta" anyway, and so here it might be
better to disregard the tree structure issues entirely.

Actually, the DaCHS internal author table as used by the alternative
portal interprets one special notation::

  <author1>, <inits1> {; <authorn>, <initsn>}

That is, you should write authors lists like "Foo, X.; Bar, Q.; et al".


Coverage Metadata
'''''''''''''''''

Coverage metadata probably is the most complex piece of metadata, but
also potentially the most useful, since it would allow clients to
restrict querying to services known to contain relevant material.  So,
try to get it right.

Within DaCHS, coverage metadata uses the following keys:

* coverage.profile – an STC-S string giving the coverage of the service.
  These can become rather complex.  We implement several extensions to
  STC-S.  See also the `documentation for GAVO STC`_
* coverage.waveband – One of Radio, Millimeter, Infrared, Optical, UV,
  EUV, X-ray, Gamma-ray, and you can have multiple waveband
  specifications.  Note that you can provide much more detailed
  information on the covered spectral range as part of coverage.profile
  (but it's also much less likely that there is proper support for data
  there in registries and clients).
* coverage.regionOfRegard – in essence, the "pixel size" of the service in
  degrees.  If, for example, your service gives data on a lattice of
  sampling points, the typical distance of such points should be given
  here.  Leave out if this doesn't apply to your service.
* coverage.footprint – reserved; this will probably be filled in
  automatically by the software once we have a footprint standard and
  DaCHS implements it.

Here's an example for a service covering the large and small magellanic
clouds::

  <meta name="coverage">
    <meta name="profile">
      Union ICRS (
        Box 81 69.75 14 3.25
        Box 13 -73 9 2)</meta>
    <meta name="waveband">Optical</meta>
    <meta name="waveband">Infrared</meta>
    <meta name="regionOfRegard">0.02</meta>
  </meta>


.. _Documentation for GAVO STC: http://docs.g-vo.org/DaCHS/stc.html

Meta Stream Format
''''''''''''''''''

In serveral places, most notably in the ``defaultmeta.txt`` file and in
meta elements without a ``name`` attribute, you can give metadata as a
"meta stream".  This is just a sequence of lines containing pairs of
<meta key> and <meta value>.

In addition, there are comments, empty lines, and continuations.
Continuation lines work by ending a line with a backslash.  The
following line separator and all blanks and tabs following it are
then ignored.  Thus, the following two meta keys end up having identical
values::

  meta1: A contin\
    uation line needs \
      a blank if you wan\
  t one.
  meta2: A continuation line needs a blank if you want one

Note that whitespace behind a backslash prevents it from being a
continuation character.  That is, admittedly, a bit of a trap.

Other than their use as continuation characters, backslashes have no
special meaning within meta streams as such.  Within meta elements,
however, macros are expanded after continuation line processing if the
meta parent knows how to expand macros.  This lets you write things
like::

  <meta>
    creationDate: \metaString{authority.creationDate}
    managingOrg:ivo://\getConfig{ivoa}{authority}
  </meta>


Comments and empty lines are easy: Empty lines are allowed, and a
comment is a line with a hash (#) as its first non-whitespace
character.  Both constructs are ignored, and you can even continue
comments (though you should not).

Meta information can have a complex tree structure.  With meta streams,
you can build trees by referencing dotted meta identifiers.  If you
specify meta information for an item that already exists, a sibling will
be created.  Thus, after::

  creator.name: A. Author
  creator:
  creator.name: B. Buthor

there are two creator elements, each specifying a name meta.  For the
way creators are specified within VOResource, the following would be
wrong::

  creator.name: This is wrong.
  creator.name: and will not work

-- you would have a single creator meta with two name metas, which is
not allowed by VOResource.

If you write::

  contact.address: 7 Miner's Way, Behind the Seven Mountains
  contact.email: dwarfs@fairytale.fa

you have a single contact meta giving address and email.


Display Hints
=============

Display hints use an open vocabulary.  As you add value formatters, you can 
evaluate any display hint you like.  Display hints understood by the
built-in value formatters include:

checkmark
  in HTML tables, render this column as empty or checkmark depending on
  whether the value is false or true to python.

displayUnit
  use the value of this hint as the unit to display a value in.

humanTime
  display values as h:m:s.

nopreview
  if this key is present with any value, no HTML code to generate
  previews when mousing over a link will be generated.

sepChar
  a separation character for sexagesimal displays and the like.

sf
  "Significant figures" -- length of the mantissa for this column.
  Will probably be replaced by a column attribute analoguous to what
  VOTable does.

type
  a key that gives hints what to do with the column.  Values currently
  understood include:

  bar
    display a numeric value as a bar of length value pixels.

  bibcode
    display the value as a link to an ADS bibcode query.

  humanDate
    display a timestamp value or a real number in either yr (julian
    year), d (JD, or MJD if xtype is mjd), or s (unix timestamp) as 
    an ISO string.

  humanDay
    display a timestamp or date value as an ISO string without time.

  keephtml
    lets you include raw HTML.  In VOTables, tags are removed.

  product
    treats the value as a product key and expands it to a URL for the
    product (i.e., typically image).  This is defined in
    protocols.products.  This display hint is also used by, e.g., the tar
    format to identify which columns should contribute to the tar file.

  dms
    format a float as degree, minutes, seconds.

  simbadlink
    formats a column consisting of alpha and delta as a link to query
    simbad.  You can add a coneMins displayHint to specify the search
    radius.

  suppress
    do not automatically include this column in any table (e.g.,
    verbLevel-based column selection).

  hms
    force formatting of this column as a time (usually for RA).

  url
    makes value a link in HTML tables.

  imageURL
    makes value the src of an image.  Add width to force a certain
    image size.

noxml
  if 'true' (exactly like this), do not include this column in VOTables.

width
  preview width in pixels.

Note that not any combination of display hints is correctly
interpreted.  The interpretation is greedy, and only one formatter at a
time attempts to interpret display hints.


Building Service Interfaces
===========================

Within DaCHS, an HTTP request is processed as follows:

1) The core is adapted to the renderer; this means that condDescs with
   buildFrom are converted to inputKeys according to the rules of the
   renderer.  The form renderer generates VizieR-like expressions,
   protocol renderers make PQL parameters, etc.  Also, onlyForRenderer
   and notForRenderer inputKeys are selected or deselected
2) From the core's inputTable, the service builds an input data
   descriptor (unless the service has an inputDD defined already, of
   course).  Most standard cores only take input from an input table's
   parameters (the exception being the computedCore), and hence the
   automatic inputDD will only have a parmaker.  The automatic
   inputDD will parse using ContextGrammar without a rowKey (i.e.,
   no rows will be produced).  The parmaker within the automatic inputDD
   parses the input with the default parsers and using the getHTTPPar
   rowmaker function.
3) The service will build the input table using its inputDD.  The input
   must be like nevow request.args, mapping each key to a sequence of
   strings.
4) The input table is passed to the core, which produces either a table,
   a data instance, or a pair of mime-type and content.
5) From the core result, an SvcResult is built.  This is relevant when
   the service has an outputTable defined, in which case the table
   structure is adapted if the input actually is a table.
6) The renderer formats the SvcResult according to its wishes.

There is special handling for the form renderer, which does its parsing
using nevow formal.  For it, the input table is built by just putting
the values of the dictionary nevow formal produces into the input table
params.


TBD: multiplicity, param values as defaults,


Table-based cores
'''''''''''''''''

You will usually deal with cores querying database tables – dbCore,
ssapCore, etc.  For these, there should not be a need to define an
inputDD; the one generated from the condDescs should work fine.

To create simple constraints, just ``buildFrom`` the columns queried::

  <condDesc buildFrom="myColumn"/>

(the names are resolved in the core's queried table).  This pattern has
the advantage that the concrete parameter style is adapted to the
renderer – in the web interface, there are vizier-like expressions, in
protocol interfaces, you get fields understanding expressions as in
SSAP's "PQL", plus in addition "structured parameters" (like FOO_MIN and
FOO_MAX) where applicable.

This will generate query fields that work against data as stored in the
database, with some exceptions (columns containing MJDs will, for
example, be turned into VizieR-like date expressions for web forms).
For protocol input, this is, in general, what you want.  In web forms,
you may want to customize the apprearance, for example, to adapt to
user's unit preferences.  For this latter use case, there is the
``inputUnit`` attribute::

    <condDesc>
      <inputKey original="minDist" inputUnit="arcsec"
        type="vexpr-float"
        onlyForRenderer="form"/>
      <inputKey original="minDist" 
        type="pql-float"
        notForRenderer="form"/>
    </condDesc>

Note how in this case we adapted the types of the input keys to provide
interfaces suitable to the various renderers.  For HTML forms, we
recommend one of

* vexpr-float
* vexpr-date (dates with timestamps in the database)
* vexpr-mjd (dates with MJD in the database)
* vexpr-string (though for those, frequently generating options is
  preferable, see below)

For protocol input, the types available are

* pql-int
* pql-float
* pql-string
* pql-date (where timestamps are in the database; MHD works fine with
  pql-float since date input is not really desirable for protocol input
  anyway).

Note that you can, of course, also keep the default types where that
provides a better interface.  Flag-like integers, for example, are
classic examples where giving the possible values is preferable to
allowing parameter expressions.

For object lists and similar, it is frequently desirable to give the
possible values (unless there are too many of those; these will be
translated to option lists in forms and to metadata items for protocol
services and hence be user visible)::

   <condDesc>
      <inputKey original="source">
        <values fromdb="source from plc.data"/>
      </inputKey>
    </condDesc>


All these generate the default SQL, which is equality (or set membership
for multiple values for a parameter).  To generate custom SQL, give a
phraseMaker, like this::

   <condDesc>
      <inputKey original="confirmed" multiplicity="single"/>
      <phraseMaker>
        <code>
          if inPars.get(inputKeys[0].name, False):
            yield "confirmed"
        </code>
      </phraseMaker>
    </condDesc>

PhraseMakers work like other code embedded in RDs (and thus may have
setup).  ``inPars`` gives a dictionary of the input parameters as parsed
by the inputDD according to multiplicity (or as delivered by nevow
formal – use the ``getHTTPPar`` rowmaker function if there can be input
with differing multiplicities).  ``inputKeys`` contains a sequence of
the condDesc's inputKeys.  By using their names as above, your code will
not break if the parameters are renamed.

PhraseMakers must yield zero or more SQL fragments; multiple SQL
fragments are joined in conjunctions (i.e., end up in ANDed conditions
in the WHERE clause).

Since you are dealing with raw SQL here, *never* include material from
inPars directly in the query strings you return – this would immediately
let people to SQL injections at least when the inputKey's type is
string.  Instead, use getSQLKey as in this example::

    <condDesc>
      <inputKey original="hdwl" multiplicity="single"/>
      <phraseMaker>
        <code>
          ik = inputKeys[0]
          destRE = "^%s\\.[0-9]*$"%inPars[ik.name]
          yield "%s ~ (%%(%s)s)"%(ik.name,
            base.getSQLKey("destRE", destRE, outPars))
        </code>
      </phraseMaker>
    </condDesc>

``getSQLKey`` takes a suggested name, a value and a dictionary, which
within phraseMakers always is ``outPars``. It will enter value with the
suggested name as key into outPars or change the suggested name if there
is a name clash.  The generated name will be returned, and that is what
is entered in the SQL statement.

The ``outPars`` dictionary is shared between all condDescs entering into
a query.  Hence, if you do anything with it except passing it to
``base.getSQLKey``, you're voiding your entire warranty.

Here's how to define a condDesc doing a full text search in a column::

  <condDesc>
    <inputKey original="source" description="Words from the catalog
      description, e.g., author names or title words."/>
    <phraseMaker>
      <code>
        yield ("to_tsvector('english', source)"
          " @@ plainto_tsquery('english', %%(%s)s)")%(
        base.getSQLKey("source", inPars["source"], outPars))
      </code>
    </phraseMaker>
  </condDesc>

Incidentally, this would go with an index definition like::

  <index columns="source" method="gin"
    >to_tsvector('english', source)</index>

For the HTML form interface, you can influence the widgets chosen by the
renderer to some extent.  To get an options list allowing multiple
selections, say::

    <condDesc>
      <inputKey original="carsfield" multipliticy="multiple">
        <values fromdb="carsfield from carsarcs.meta order by carsfield"/>
      </inputKey>
    </condDesc>

Use the ``showItems="n"`` attribute of inputKeys to determine how many
items in the selector are shown at one time.

For special effects, you can group inputKeys.  This will make them show
up under a common label and in a single line in HTML forms.  Here's an
example for a simple range selector::

  <condDesc>
    <inputKey name="el" type="text" tablehead="Element"/>

    <inputKey name="mfmin" tablehead="Min. Mass Fraction \item">
      <property name="cssClass">a_min</property>
    </inputKey>

    <inputKey name="mfmax" tablehead="Max. Mass Fraction \item">
      <property name="cssClass">a_max</property>
    </inputKey>

    <group name="mf">
      <description>Mass fraction of an element. You may leave out
        either upper or lower bound.</description>
      <property name="label">Mass Fraction between...</property>
      <property name="style">compact</property>
    </group>
  </condDesc>

You will probably want to style the result of this effort using the
``service`` element's ``customCSS`` property, maybe like this::

  <service...>
    <property name="customCSS">
      input.a_min {width: 5em}
      input.a_max {width: 5em}
      input.formkey_min {width: 6em!important}
      input.formkey_max {width: 6em!important}
      span.a_min:before { content:" between "; }
      span.a_max:before { content:" and "; }
      tr.mflegend td {
        padding-top: 0.5ex;
        padding-bottom: 0.5ex;
        border-bottom: 1px solid black;
      }
    </property>
  </service>

See also the entries on `multi-line input`_, `selecting input fields
with a widget`_, and `customizing generated SCS conditions`_.

.. _multi-line input: howDoI.html#get-a-multi-line-text-input-for-an-input-key
.. _selecting input fields with a widget: howDoI.html#make-an-input-widget-to-select-which-columns-appear-in-the-output-table
.. _customizing generated SCS conditions: howDoI.html#change-the-query-issued-on-scs-queries

TBD: Say something about required.  Do we even want to mention
widgetFactory?

Formatting the output
'''''''''''''''''''''

TBD


Datalink Cores
==============

[Note: this section is in the process of implementation]

Datalink is an IVOA protocol that allows associating various products
and artifacts with a data set id.  Classical examples for functionality
exposed via Datalink include cutouts, format conversions or
recalibrations done on the fly, and associating error or mask maps with
the actual data.

Within DaCHS, you can write datalink services using specialized Datalink
cores.  Their function is twofold:

(1) when given a PUBDID only, return the access options ("Datalink
    document")
(2) when given additional arguments, perform some computation
    ("Processed data")

Function (1) is implemented by DaCHS code working on the metadata of the
Datalink core.  Function (2) requires custom code (or the assembly of
pre-provided building blocks).

A datalink core consists of 

* exactly one descriptor generator,
* zero or more data functions, generating and manipulating data
* zero or one formatters, formatting the generated and/or manipulated
  data
* zero or more meta makers, generating input parameter descriptions
  for data functions and any formatter present and/or related links

Here's how they work together in providing the Datalink functionality:

To generate the Datalink document, the descriptor generator is passed
the PUBDID and is expected to return a ``datalink.ProductDescriptor``
instance (or None, in which case the datalink request will be rejected
by a 404).  In addition to attributes named after the columns of the
product table (and potentially other attributes added by deriving from
the base ProductDescriptor), it has an attribute ``data`` defaulting to
``None``, intended to be filled by the core's data generator on data
processing runs.

The descriptor is then passed, in turn, to the meta makers, which yield
``InputKey`` or ``LinkDef`` instances to describe the retrival options
for the product.  The combination of both is then formatted to a proper
Datalink document and returned, which concludes the processing of the
metadata request.

When a request for processed data comes in, the descriptor generator is
again used to make a product descriptor, and again the input keys are
updated as before.  They are then used to build the arguments structure
described by the input keys.  

If the context grammar succeeds, the data descriptor is passed to the
first data function together with the arguments parsed.  This must fill
out the ``data`` attribute of the descriptor or raise a ValidationError
for the PUBDID; leaving it as None results in a 500 server error.
Descriptor.data could an ``rsc.InMemoryTable`` (e.g., in SSAP) or a
products.Products instance, but as long as the other data functions and
the formatter agree on what it is, anything goes.  It will usually be
fed from a database, pixels in FITS files, or the like.

This object is then handed through all remaining data functions; these
change the data in place or create a new one as convenient and
manipulate ``descriptor.data`` accordingly.

Finally, the data enters the formatter, which actually generates the
output, returning a pair of mime type and string to be delivered.

It is a design descision which manipulations are done in the data
generator, which are in later filters, and which maybe only in the
formatter.  The advantage of filters is that they are more flexible and
can more easily be reused, while doing it things in the data generator
itself will usually be more efficient, sometimes much so (e.g., sums
being computed within a database rather than in a filter after all the
data had to go through the interface of the database).


Descriptors Generators
'''''''''''''''''''''''

Descriptor generators (see `element descriptorGenerator`_) are procedure
applications that see a pubdid value and are expected to return a
``datalink.ProductDescriptor`` instance, or something derived from it.  

In the end, this usually boils down to figuring out the value of accref
in the product table and using what's there to construct the d.g.
In the simplest case, the pubdid will be in DaCHS'
"standard" format (see the ``getStandardPubDID`` rowmaker function), in
which case the default d.g. works any you don't have to specify
anything.  You could manually insert that default by
saying::
  
  <dataGenerator procDef="//datalink#fromStandardPubDID"/>

(but that would be silly since DaCHS already does this for you).  It's
functionality is equivalent to this::

  <dataGenerator>
    <code>
      return ProductDescriptor.fromAccref("/".join(pubdid.split("/")[4:]))
    </code>
  </dataGenerator>

– which might be a good place to start if you need to write your own
d.g., e.g., because you have some special logic to encode the accref in
the PubDID).

A slightly more interesting example is provided by datalink for SSA,
where cutouts and similar is generated from spectra.  The actual
definition is in ``//datalink#sdm_genDesc``, but the gist of it is::

  <descriptorGenerator>
    <setup>
      <par key="ssaTD" description="Full reference (like path/rdname#id)
        to the SSA table the spectrum's PubDID can be found in."/>

      <code>
        from gavo import rsc
        from gavo import rscdef
        from gavo import svcs

        class SSADescriptor(ProductDescriptor):
          ssaRow = None

          @classmethod
          def fromSSARow(cls, ssaRow, paramDict):
            """returns a descriptor from a row in an ssa table and
            the params of that table.
            """
            paramDict.update(ssaRow)
            ssaRow = paramDict
            res = cls.fromAccref(ssaRow['accref'])
            res.ssaRow = ssaRow
            return res
      
        ssaTD = base.resolveCrossId("myres/q#mytable, rscdef.TableDef)
      </code>
    </setup>
    
    <code>
      with base.getTableConn() as conn:
        ssaTable = rsc.TableForDef(ssaTD, connection=conn)
        matchingRows = list(ssaTable.iterQuery(ssaTable.tableDef, 
          "ssa_pubdid=%(pubdid)s", {"pubdid": pubdid}))
        if not matchingRows:
          raise svcs.UnknownURI("No spectrum with pubdid %s known here"%
            pubdid)

        # the relevant metadata for all rows with the same PubDID should
        # be identical, and hence we can blindly take the first result.
        return SSADescriptor.fromSSARow(matchingRows[0],
          ssaTable.getParamDict())
    </code>
  </descriptorGenerator>

Note how we derive from ProductDescriptor to get something that metadata
makers can later consult to figure out the spectral extent, the
calibration status, etc., by combining a row from an SSA table and its
parameter dict and stuffing that into an attribute of the derived class.
Also, since SSA tables already contain a column containing PubDIDs, we
can treat them as opaque.

Incidentally, in this case you could stuff the entire code into the the
main code element, saving on the extra setup.  However, apart from a
minor speed benefit, keeping things like function or class definitions
in setup allows easier re-use of such definitions in procedure
applications and is therefore recommended.



Meta Makers
'''''''''''

Meta makers (see `element metaMaker`_) contain code that produces pieces
of service metadata from a data descriptor.  All meta makers belonging
to a service are unconditionally executed, and all must be generator
bodies (i.e., contain a yield statement).

Meta makers may yield either input keys or link definitions.  The input
keys make up a service's interface in the usual way.
Note, however, that since DaCHS datalink services dispatch between
metadata and data processing queries by checking whether any parameters
other than PUBDID are given, you must never generate defaulted input
keys here, since the core cannot tell defaulted inputs from those passed
in by the user and would then always do data processing.  If you
actually need defaults, set them in the data formatters whenever the
parameters they expect are None.

Link definitions are instances of the `LinkDefinition` class.  These are
constructed with 

- the destination URL (as a string)
- the destination contentType (a mime type as a string)
- the relationType (another string).

The classes usually required to build whatever meta makers return 
(InputKey, Values, Option, LinkDef) are available to the code as local
names.  You should, however, not construct them directly but use the
``MS`` helper (which is really an alias for base.makeStruct, which takes
care that the special postprocessing of DaCHS structures takes place).

Parameter Definitions
.....................

Hence, a meta maker that generates SSA cutout parameters could look like
this::

  <metaMaker>
    <setup>
      <code>
        parSTC = stc.parseQSTCS('SpectralInterval "LAMBDA_MIN" "LAMBDA_MAX"')
      </code>
    </setup>
    <code>
      for ik in genLimitKeys(MS(InputKey, name="LAMBDA",
        unit="m", stc=parSTC, ucd="em.wl", 
        description="Spectral cutout interval",
        values=MS(Values, 
          min=descriptor.ssaRow["ssa_specstart"],
          max=descriptor.ssaRow["ssa_specend"]))):
        yield ik
    </code>
  </metaMaker>

(something like this is part of the ``//datalink#sdm_cutout`` predefined
stream).

The example shows two general techniques for "physical" parameters.
For one, it defines an STC structure.  This is again the "quoted STC" as
discussed in `the DaCHS tutorial`_.  It is a good idea to create the STC
structure in the setup code since parsing STC-S can be relatively CPU 
intensive.  The STC structure resulting from should then be passed as
the ``stc`` keyword parameter to each input key mentioned in the STC
clause.

The second typical technique is the use of the ``genLimitKeys``
function.  This takes a "template" key specifying names, units, and
everything else that can be generically specified, and returns a
sequence of input keys for the limits (i.e., minimal and maximal value
for this).  You'll almost always want this when accepting floating-point
valued parameters, as matching these exactly is at least tricky and
rarely useful.

When publishing FITS cubes, you will usually use the
`//datalink#fits_makeWCSParams`_ meta maker; it accepts similar QSTCS
specifications as well.  To find out what parameter names the individual
axes are mapped to, first use makeWCSParams without the STC metadata::

 	<service id="d" allowed="dlget,form">
		<datalinkCore>
			<descriptorGenerator procDef="//datalink#fits_genDesc"/>
			<metaMaker procDef="//datalink#fits_makeWCSParams"/>
		</datalinkCore>
	</service>

Then have a look at the metadata produced for a file.  Unless you did
something special, to do that you can just take the accref of a file
from the table containing the products; if the source table was
``mlqso.cubes``, you could figure one out via::

  select accref from dc.products where sourcetable='mlqso.cubes' limit 1

(talk to postgres directly for this query, dc.products is not available
via TAP).

The standard pubdid (as assigned using the ``getStandardPubDID``
rowmaker function) uses your datacenter authority (as configured in
/etc/gavo.rd, when you forget it you can also figure it out by using
``gavo config ivoa authority``) and this accref like this::

  ivo://<authority>/~/<accref>

Hence, to retrieve the datalink document for
``mlqso/data/FBQ0951_data.fits`` on the server dc.g-vo.org using the
datalink renderer on the ``mlqso/q/d`` service, you'd write::

  curl -DPUBDID=ivo://org.gavo.dc/~/mlqso/data/FBQ0951_data.fits \
    http://dc.g-vo.org/mlqso/q/d/dlget | xmlstarlet fo

(of course, ``xmlstarlet`` isn't actually necessary, and you can use
``wget`` if you want, but you get the idea).

In there you'll see the parameter names for the axes, e.g.,::

  $ curl -s -FPUBDID=ivo://org.gavo.dc/~/mlqso/data/FBQ0951_data.fits \
  >> http://dc.g-vo.org/mlqso/q/d/dlget \
  >> | xmlstarlet sel -N v=http://www.ivoa.net/xml/VOTable/v1.2 -T \
  >> -t -m "//v:PARAM" -v "@name" -nl
  serviceAccessURL
  PUBDID
  DEC_MIN
  DEC_MAX
  RA_MIN
  RA_MAX
  WAVELEN_1_MIN
  WAVELEN_1_MAX

If the image is calibrated using a catalog on ICRS, with the wavelength
given as measured, change the ``fits_makeWCSParams`` call to::

  <metaMaker procDef="//datalink#fits_makeWCSParams>
    <setup>
      <bind key="stcs"
				>('PositionInterval ICRS "RA_MIN" "DEC_MIN" "RA_MAX" "DEC_MAX"\n'
					'SpectralInterval TOPOCENTER "WAVELEN_1_MIN" "WAVELEN_1_MAX"')
       </bind>
    </setup>
  </metaMaker>

The effect should be a group like::

    <GROUP utype="stc:CatalogEntryLocation">
      <PARAM arraysize="*" datatype="char" 
        name="CoordFlavor" 
        utype="stc:AstroCoordSystem.SpaceFrame.CoordFlavor" value="SPHERICAL"/>
      <PARAM arraysize="*" datatype="char" 
        name="CoordRefFrame" 
        utype="stc:AstroCoordSystem.SpaceFrame.CoordRefFrame" value="ICRS"/>
      <PARAM arraysize="*" datatype="char" 
        name="ReferencePosition" 
        utype="stc:AstroCoordSystem.SpectralFrame.ReferencePosition" 
        value="TOPOCENTER"/>
      <PARAM arraysize="*" datatype="char" name="URI" 
        utype="stc:DataModel.URI" 
        value="http://www.ivoa.net/xml/STC/stc-v1.30.xsd"/>
      <PARAMref ref="apausoh" 
        utype="stc:AstroCoordArea.Position2VecInterval.HiLimit2Vec.C1"/>
      <PARAMref ref="aedwpnn" 
        utype="stc:AstroCoordArea.Position2VecInterval.HiLimit2Vec.C2"/>
      <PARAMref ref="asausoh" 
        utype="stc:AstroCoordArea.Position2VecInterval.LoLimit2Vec.C1"/>
      <PARAMref ref="ahgwpnn" 
        utype="stc:AstroCoordArea.Position2VecInterval.LoLimit2Vec.C2"/>
      <PARAMref ref="ahiusoh" 
        utype="stc:AstroCoordArea.SpectralInterval.HiLimit"/>
      <PARAMref ref="aeiusoh" 
        utype="stc:AstroCoordArea.SpectralInterval.LoLimit"/>
    </GROUP>

All this is explained in [VOTSTC].


Link Definitions
.................

When returning link definitions, the tricky part mostly is to come up
with the URLs.  Use the ``makeAbsoluteURL`` rowmaker function to make
them from relative URLs; the rest just depends on your URL scheme.  An
example could look like this::

  <metaMaker>
    <code>
      yield LinkDef(makeAbsoluteURL(
        "get/"+descriptor.accref[:-5]+".err.fits"),
        "image/fits", "errors")
      yield LinkDef("http://foo.bar/raw/"+descriptor.accref.split("/")[-1],
        "image/fits", "raw")
    </code>
  </metaMaker>

The vocabulary for the relation types (the last argument to a link
definition's constructor) is not finalized yet.  Complain to the DaCHS
maintainers that this sentence still is here.



Data Functions
''''''''''''''

Data functions (see `element dataFunction`_) generate or manipulate
data.  They see the descriptor and the arguments, parsed according to
the input keys produced by the meta makers, where the descriptor's
``data`` attribute is filled out by the first data function called (the
"generating data function").

As described above, DaCHS does not enforce anything on the ``data``
attribute other than that it's not None after the first data function
has run.  It is the RD author's responsibility to make sure that all
data function in a given datalink core agree on what ``data`` is.

All code in a request for processed data is also passed the input
parameters as processed by the context grammar.  Hence, the code can
rely on whatever contract is implicit in the context grammar, but not
more.  In particular, a datalink core has no way of knowing what data
functions expects which parameters.  If no value for a parameter was
provided on input, the corresponding value is None but a data function
using it still is called.

An example for a generating data function is ``//datalink#generateProduct``,
which may be convenient when the manipulations operate on plain local files;
it basically looks like this::

  <dataFunction>
    <code>
      descriptor.data = products.getProductForRAccref(descriptor.accref)
    </code>
  </dataFunction>

(the actual implementation lets you require certain mime types and is
therefore a bit more complicated).

Another generating data function, this time creating a Data instance
containing a spectral data model-compliant structure, is in
``//datalink#sdm_genData`` and looks essentially like this::

  <dataFunction>
    <code>
      from gavo import rscdef
      from gavo.protocols import sdm
      builder = base.resolveCrossId(
        "flashheros/q#buildsdm, rscdef.DataDescriptor)
      descriptor.data = sdm.makeSDMDataForSSARow(descriptor.ssaRow, builder)
    </code>
  </dataFunction>

More on this will be discussed in our secion on ``getData`` support.

Filtering data functions should always come with a corresponding
metaMaker.  As an example, continuing the spectral cutout example above,
is again in ``//datalink#sdm_cutout``.  It simply looks like this::

  <dataFunction>
    <code>
      if not args.get("LAMBDA_MIN") and not args.get("LAMBDA_MAX"):
        return

      from gavo.protocols import sdm
      sdm.mangle_cutout(
        descriptor.data.getPrimaryTable(),
        args["LAMBDA_MIN"] or -1, args["LAMBDA_MAX"] or 1e308)
    </code>
  </dataFunction>

There are situations in which a data function must shortcut, mostly
because it is doing something other than just "pushing on"
descriptor.data.  Examples include preview producers or a data function
that returns the a FITS header.  For cases like this, data functions can
raise one of DeliverNow (which means ``descriptor.data`` must be
something servable, see `Data Formatters`_ and causes that to be
immediately served) or FormatNow (which immediately goes to the data
formatter; this is less useful).

Here's an example for FormatNow; a similar thing is contained in the
STREAM ``//datalink#fits_genKindPar``::

	<dataFunction>
		<setup>
			<code>
				from gavo.utils import fitstools
			</code>
		</setup>
		<code>
			if args["KIND"]=="HEADER":
				descriptor.data = ("application/fits-header", 
					fitstools.serializeHeader(descriptor.data[0].header))
				raise DeliverNow()
		</code>
	</dataFunction>



Data Formatters
'''''''''''''''

Data formatters (see `element dataFormatter`_) take a descriptor's data
attribute and build something serveable out of it.  Datalink cores do
not absolutely need one; the default is to return ``descriptor.data``
(the ``//datalink#trivialFormatter``, which might be fine if that data is
serveable itself.

What is serveable?  The easiest thing to come up with is a pair of
content type and data in byte strings; if ``descriptor.data`` is a Table
or Data instance, the following could work::

  <dataFormatter>
    <code>
      from gavo import formats

      return "text/plain", formats.getAsText(descriptor.data)
    </code>
  </dataFormatter>
 

Another example is an excerpt from ``//datalink#sdm_cutout``::

  <dataFormatter>
    <code>
      from gavo.protocols import sdm

      if len(descriptor.data.getPrimaryTable().rows)==0:
        raise base.ValidationError("Spectrum is empty.", "(various)")

      return sdm.formatSDMData(descriptor.data, args["FORMAT"])
    </code>
  </dataFormatter>

(this goes together with a metaMaker for an input key describing
FORMAT).

An alternative is to return something that has a ``renderHTTP(ctx)``
method that works in nevow.  This is true for the Product instances that
``//datalink#generateProduct`` generates, for example.  When you write
that yourself, however, take care that this renderHTTP runs in the main
server loop.  If it blocks, the server blocks, so take care that this
doesn't happen.  The conventional way would be to return, from the
renderHTTP method, some twisted producer.


Datalink Examples
'''''''''''''''''

FITS cutout service
...................

A plain FITS cutout service is assembled like this::

  <service id="dl">
		<datalinkCore>
			<descriptorGenerator procDef="//datalink#fits_genDesc"/>
			<metaMaker procDef="//datalink#fits_makeWCSParams"/>
			<dataFunction procDef="//datalink#fits_makeHDUList"/>
			<dataFunction procDef="//datalink#fits_doWCSCutout"/>
			<dataFormatter procDef="//datalink#fits_formatHDUs"/>
		</datalinkCore>
	</service>

This works for all FITS files in the products table and has no usable
STC metadata.  Good datalink services do better; by giving more
metadata, you of course commit to certain FITS structures, which means
that you should restrict to only those files that actually match your
assumptions.  The easiest way to do this is to structure your input
directories accordingly and then filter early by using fits_getDesc's
``accrefStart`` parameter.  The STC declaration was already discussed
above, and so a more realistic datalink service might look like this::

  <service id="dl">
		<datalinkCore>
			<descriptorGenerator procDef="//datalink#fits_genDesc">
			  <bind key="accrefStart">califa/data/cubes"</bind>
			</descriptorGenerator>
			<metaMaker procDef="//datalink#fits_makeWCSParams">
        <bind key="stcs"
				  >('PositionInterval ICRS "RA_MIN" "DEC_MIN" "RA_MAX" "DEC_MAX"\n'
					  'SpectralInterval TOPOCENTER "WAVELEN_1_MIN" "WAVELEN_1_MAX"')
         </bind>
			  </metaMaker>
			<dataFunction procDef="//datalink#fits_makeHDUList"/>
			<dataFunction procDef="//datalink#fits_doWCSCutout"/>
			<dataFormatter procDef="//datalink#fits_formatHDUs"/>
		</datalinkCore>
	</service>

TODO: Custom function, return link to error file.

TODO: Simple sdm/getdata service



Writing Custom Cores
====================

While DaCHS provides cores for many common operations -- in particular,
database queries and wrapped external binaries --, there are of course
services needing to do things not covered by what the shipped cores do.
Some such cases still follow the basic premise of services: GET or POST
parameters in, something table-like out.  For these cases, use custom
cores (if even this does not provide sufficent functionality, write a
custom renderer).


Defining a Custom Core
''''''''''''''''''''''

To do this, you need to write a python module.  The standard location
for those is in the bin/ subdirectory of the resource directory.

You will usually want to inherit from core::

  from gavo.svcs import core

  class Core(core.Core):

The framework will always look of an object named "Core" in the module
and use this as the custom core.

The core needs an InputTable and an OutputTable like all cores.  You
*could* define it in the resource descriptor like this::

  <customCore id="createCore" module="bin/create">
    <inputTable>
      <inputKey .../>
    </inputTable>
    <outputTable>
      <column name="itemsAdded" type="integer" tablehead="Items added"/>
    </outputTable>
  </customCore>

It's probably a better idea to define it in the code, though, since
then it will work without further specifications.  The definitions
in the code can still be overridden from an RD for special effects.
Embedding the definitions is done using the class attributes
``inputTableXML`` and ``outputTableXML``::

  class Core(core.Core):
    inputTableXML = """<inputTable>
      <inputKey name="fileSrc" type="file" tablehead="Local file"
        description="A local file to upload (overrides source URL if given).">
      <inputKey name="tableName" type="text" tablehead="Target Table"
        description="Name of the table to match against.  
          Only tables available for ADQL (see there) can be used here.">
        <values fromdb="tablename from dc_tables where adql=True"/>
      </inputTable>
      """
    outputTableXML = """<outputTable/>"""

You should not override the constructor.  If you need to perform
"expensive" instanciations, override the completeElement method, as in
the following template::

  def completeElement(self):
    <your code>
    self._completeElementNext(Core)

The call to _completeElementNext ensures that the remaining
completeElement methods are executed.

Giving the Core Functionality
'''''''''''''''''''''''''''''

To have the core do something, you have to override the run method,
which has to have the following signature::

  run(service, inputTable, queryMeta) -> stuff

The stuff returned will ususally be a Table instance (that need not
match the outputTable definition -- the latter is targetted at the
registry and possibly applications like output field selection).  The
standard renderers also accept a mime type and a string containing
some data and will deliver this as-is.  With custom renderers, you could
return basically anything you want.

Services come up with some idea of the schema of the table they want to
return and adapt tables coming out of the core to this.  Sometimes, you
want to suppress this behaviour, e.g., because the service's ideas are
off.  In that case, set a noPostprocess atttribute on the table to any
value.

service is a service instance.  In particular, you can access the RD you
are running in through its rd attribute.  This is useful if you need to
resolve, e.g., table references (which, in this case, could be given as
a service property)::

  pertainingTable = service.rd.getById(
    service.getProperty("pertainingTable"))

inputTable is a Table instance. Unless the service has a fancy inputDD,
you simply find the inputKey values in the table's parameters::

  val = inputTable.getParam("fileSrc")



Errors
''''''

To bail out from processing, raise a validation error.  Construct it
with a message and the name of an input key.  At least for the form
renderer, this causes a sensible error message with some hint as the the
originating input field::

  raise base.ValidationError("Invalid file name", "rdsrc")


Database Options
''''''''''''''''

The standard DB cores receive a "table widget" on form generation,
including sort and limit options.  To make the Form renderer output this
for your core as well, define a method wantsTableWidget() -> True.

The queryMeta that you receive in run has a dbLimit key.  It contains
the user selection or, as a fallback, the global db/defaultLimit value.
These values are integers.

So, if you order a table widget, you should do something like::

  cursor.execute("SELECT .... LIMIT %(queryLimit)s", 
    {"queryLimit": queryMeta["dbLimit"],...})

In general, you should warn people if the query limit was reached; a
simple way to do that is::

  if len(res)==queryLimit:
    res.addMeta("_warning", "The query limit was reached.  Increase it"
      " to retrieve more matches.  Note that unsorted truncated queries"
      " are not reproducible (i.e., might return a different result set"
      " at a later time).")

where res would be your result table.  _warning metadata is displayed in
both HTML and VOTable output, though of course VOTable tools will not
usually display it.

Inheriting from TableBasedCore
''''''''''''''''''''''''''''''

TBD (This does not work right now; complain if you need to do it)


Manufacturing Spectra
=====================

TODO: Update this for Datalink

Making SDM Tables
'''''''''''''''''

Compared to images, the formats situation with spectra is a mess.
Therefore, in all likelihood, you will need some sort of conversion
service to VOTables compliant to the spectral data model.  DaCHS has a
facility built in to support you with doing this on the fly, which means
you only need to keep a single set of files around while letting users
obtain the data in some format convenient to them.  The tutorial
contains examples on how to generate metadata records for such
additional formats.

First, you will have to define the "instance table", i.e., a table
definition that will contain a DC-internal representation of the
spectrum according to the data model.  There's a mixin for that::

  <table id="spectrum">
    <mixin ssaTable="hcdtest">//ssap#sdm-instance</mixin>
  </table>

In addition to adding lots and lots of params, the mixin also defines
two columns, ``spectral`` and ``flux``; these have units and ucds as
taken from the SSA metadata.  You can add additional columns (e.g., a
flux error depending the the spectral coordinate) as requried.

The actual spectral instances can be built by sdmCores and delivered
through DaCHS' product interface.  Note, however, that clients
`supporting getData`_ wouldn't need to do this.  You'll still have to
define the data item defined below.

sdmCores, while potentially useful with common services, are intended to
be used by the product renderer for dcc product table paths.  They
contain a data item that must yield a primary table that is basically
sdm compliant.  Most of this is done by the //ssap#feedSSAToSDM apply
proc, but obviously you need to yield the spectral/flux pairs (plus
potentially more stuff like errors, etc, if your spectrum table has more
columns.  This comes from the data item's grammar, which probably must
always be an embedded grammar, since its sourceToken is an SSA row in a
dictionary.  Here's an example::

  <sdmCore queriedTable="hcdtest" id="mksdm">
    <data id="getdata">
      <embeddedGrammar>
        <iterator>
          <code>
            labels = ("spectral", "flux")
            relPath = self.sourceToken["accref"].split("?")[-1]
            with self.grammar.rd.openRes(relPath) as inF:
              for ln in inF:
                yield dict(zip(labels,ln.split()))
          </code>
        </iterator>
      </embeddedGrammar>
      <make table="spectrum">
        <parmaker>
          <apply procDef="//ssap#feedSSAToSDM"/>
        </parmaker>
      </make>
    </data>
  </sdmCore>

Note: spectral, flux, and possibly further items coming out of the
iterator must be in the units units promised by the SSA metadata
(fluxSI, spectralSI).  Declarations to this effect are generated by the
``//ssap#sdm-instance`` mixin for the spectral and flux columns.

The sdmCores are always combined with the sdm renderer.  It passes an
accref into the core that gets turned into an row from queried table;
this must be an "ssa" table (i.e., right now something that mixes in
``//ssap#hcd``).  This row is the input to the embedded data descriptor.
Hence, this has no sources element, and you must have either a custom
or embedded grammar to deal with this input.




Supporting getData
==================

DaCHS still has support the now-abandoned 2012 getData specification by
Demleitner and Skoda.   If you think you still want this, contact the
authors; meanwhile, you really should be using datalink for whatever you
think you need getData for.



Adapting Obscore
================

You may want extra, locally-defined columns in your obscore tables.  To
support this, there are three hooks in obscore that you can exploit.
To fill these hooks, use ``userconfig.rd`` (TODO: more
documentation on that as we use it more; meanwhile: get a `template from
SVN`_ and put it into GAVO_ROOT/etc).  It helps to have a brief look at
the ``//obscore`` RD to get an idea where these hooks go.

Within the template ``userconfig.rd``, there are already three STREAMs
with ids starting with obscore.; these are referenced from within the
system ``//obscore`` RD.  Here's an somewhat more elaborate example::

  <STREAM id="obscore-extracolumns">
    <column name="fill_factor"
      description="Fill factor of the SED"
      verbLevel="20"/>
  </STREAM>

  <STREAM id="obscore-extrapars">
    <mixinPar name="fillFactor" 
      description="The SED's fill factor">NULL</mixinPar>
  </STREAM>

  <STREAM id="obscore-extraevents">
    <property name="obscoreClause" cumulate="True">
      ,
      CAST(\\fillFactor AS real) AS fill_factor,
    </property>
  </STREAM>

(to be on the safe side: there need to be four backslashes in front of
fillFactor; this is just a backslash doubly-escaped.  Sorry about this).

The way this is used in an actual mixin would be like this::

  <table id="specs" onDisk="True">
    <mixin ...>//ssap#hcd</mixin>
    <mixin
      ... (all the usual parameters)
      fillFactor="0.3">//obscore#publishSSAPHCD</mixin>
  </table>

What's going on here?  Well, ``obscore-extracolumns`` is easy – this
material is directly inserted into the definition of the obscore view
(see the table with id ``ObsCore`` within the ``//obscore`` RD).  You
could abuse it to insert other stuff than columns but probably should
not (current exception: you probably need to fix the ``viewStatement``
in //obscore to include sufficient columns; we're trying to figure out a
better solution).

The tricky part is ``obscore-extraevents``.  This goes into the
``//obscore#_publishCommon`` STREAM and ends up in all the publish
mixins in obscore.  Again, you could insert mixinPars and similar at
this point, but the only thing you really must do is add lines to the
big SQL fragment in the ``obscoreClause`` property that the mixin leaves
in the table.  This is what is made into the table's contribution to the
big obscore union. Just follow the example above and, in particular,
always CAST to the type you ave in the metadata, since individual tables
might have NULLs in the values, and you do not want misguided attempts
of postgres to do type inference then.

If you actually must know why you need to double-escape fillFactor and
what the magic with the ``cumulate="True"`` is, ask.

Finally, ``obscore-extrapars`` directly goes into a core component of
obscore, one that all the various publish mixins there use.  Hence, all
of them grow your functionality.  That is also why it is important to
give defaults (i.e., element content) to all mixinPars you give in this
way – without them, all those other publish mixins would fail.

If you change ``%#obscore-extracolumns``, you will need to re-import all
obscore-published tables (actually, importing the metadata using ``gavo
imp -m`` should do).  There currently is no automatic way to traverse
the file system, and you will probably have to first unpublish all
existing tables by connecting to the database and running ``delete from
ivoa._obscoresources``.  If obscore adaption proves a popular feature,
we'll make all this a bit smoother.

.. _template from SVN: http://svn.ari.uni-heidelberg.de/svn/gavo/python/trunk/gavo/resources/inputs/__system__/userconfig.rd





Writing Custom Grammars
=======================

A custom grammar simply is a python module located within a resource
directory defining a row iterator class derived from
gavo.grammars.customgrammar.CustomRowIterator; this class must be called
RowIterator.  You want to override the _iterRows method.  It will have
to yield row dictionaries, i.e., dictionaries mapping string keys to
something (preferably strings, but you will usually get away with
returning complete values even without fancy rowmakers).  

So, a custom grammar module could look like this::

  from gavo.grammars.customgrammar import CustomRowIterator

  class RowIterator(CustomRowIterator):
    def _iterRows(self):
      for i in xrange(10000):
        yield {'index': i, 'square': i**2}

Do not override magic methods, since you may lose row filters, sourceFields,
and the like if you do.  An exception is the constructor.  If you must,
you can override it, but you must call the parent constructor, like
this::

  class RowIterator(CustomRowIterator):
    def __init__(self, grammar, sourceToken, sourceRow=None):
      CustomRowIterator.__init__(self, grammar, sourceToken, sourceRow)
      <your code>

The sourceToken, in general, will be a file name, unless you call
makeData manually and forceSource something else.

A row iterator will be instanciated for each source processed.  Thus,
you should usually not perform expensive operations in the constructor
unless they depend on sourceToken.  In general, you should rather define
a function makeDataPack in the module.  Whatever is returned by this
function is available as self.grammar.dataPack in the row iterator.

The function receives an instance of the the customGrammar as an
argument.  This means you can access the resource descriptor and
properties of the grammar.  As an example of how this could be used,
consider this RD fragment::

  <table id="defTable">
    ...
  </table>

  <customGrammar module="res/grammar">
    <property name="targetTable">defTable</property>
  </customGrammar>

Then you could have the following in res/grammar.py::

  def makeDataPack(grammar):
    return grammar.rd.getById(grammar.getProperty("targetTable"))

and access the table in the row iterator.

Also look into EmbeddedGrammar, which may be a more convenient way to
achieve the same thing.

Dispatching Grammars
''''''''''''''''''''

With normal grammars, all rows are fed to all rowmakers of all makes
within a data object.  The rowmakers can then decide to not process a
given row by raising ``IgnoreThisRow`` or using the trigger mechanism.
However, when filling complex data models with potentially dozens of
tables, this becomes highly inefficient.

When you write your own grammars, you can to better.  Instead of just
yielding a row from ``_iterRows``, you yield a pair of a role (as
specified in the ``role`` attribute of a ``make`` element) and the row.
The machinery will then pass the row only to the feeder for the table in
the corresponding make.

Currently, the only way to define such a dispatching grammar is to use a
custom grammar or an embedded grammar.  For these, just change your
``_iterRows`` and say ``isDispatching="True"`` in the ``customGrammar``
element.  If you implement ``getParameters``, you can return either
pairs of role and row or just the row; in the latter case, the row will
be broadcast to all parmakers.

Special care needs to be taken when a dispatching grammar parses
products, because the product table is fed by a special make inserted
from the products mixin.  This make of course doesn't see the rows you
are yielding from your dispatching grammar.  This means that without
further action, your files will not end up the the product table at all.
In turn, getproducts will return 404s instead of your products.

To fix this, you need to explicitely yield the rows destined for the 
products table with a products role, from within your grammar.  Where
the grammar yield rows for the table with metadata (i.e., rows that actually
contain the fields with prodtblAccref, prodtblPath, etc), yield
to the products table, too, like this: ``yield ("products", newRow)``.


Functions Available for Row Makers
==================================

In principle, you can use arbitrary python expressions in var, map and
proc elements of row makers.  In particular, the namespace in which
these expressions are executed contains math, os, re, time, and datetime
modules as well as gavo.base, gavo.utils, and gavo.coords.

However, much of the time you will get by using the following functions
that are immediately accessible in the namespace:

.. replaceWithResult getRmkFuncs(docStructure)



Scripting
=========

As much as it is desirable to describe tables in a declarative manner,
there are quite a few cases in which some imperative code helps a lot
during table building or teardown.  Resource descriptors let you embed
such imperative code using script elements.  These are children of the
make elements since they are exclusively executed when actually
importing into a table.

Currently, you can enter scripts in SQL and python, which may be called
at various phases during the import.

SQL scripts
'''''''''''

In SQL scripts, you separate statements with semicolons.  Note that no
statements in an SQL script may fail since that will invalidate the
transaction.  This is a serious limitation since you must not commit or
begin transactions in SQL scripts as long as Postgres  does not support
nested transactions.

You can use table macros in the SQL scripts to parametrize them; the
most useful among those probably is ``\curtable`` containing the fully
qualified name of the table being processed.

Python scripts
''''''''''''''

Python scripts can be indented by a constant amount.

The table object currently processed is accessible as table.  In
particular, you can use this to issue queries using 
``table.query(query, arguments)`` (parallel to dbapi.execute) and to
delete rows using ``table.deleteMatching(condition, pars)``.  The
current RD is accessible as ``table.rd``, so you can access items from
the RD as ``table.rd.getById("some_id")``, and the recommended way to
read stuff from the resource directory is
``table.rd.openRes("res/some_file)``.

Some types of scripts may have additional names available.  Currently,
newSource and sourceDone have the name sourceToken – which is the
sourceToken as passed to the grammar.

Script types
''''''''''''

The type of a script corresponds to the event triggering its execution.
The following types are defined right now:

* preImport -- before anything is written to the table
* preIndex -- before the indices on the table are built
* postCreation -- after the table (incl. indices) is finished
* beforeDrop -- when the table is about to be dropped
* newSource -- every time a new source is started
* sourceDone -- every time a source has been processed

Note that preImport, preIndex, and postCreation scripts are not executed
when a table is updated, in particular, in data items with
``updating="True"``.  The only way to run scripts in such circumstances
is to use newSource and sourceDone scripts.


Examples
''''''''

This snippet sets a flag when importing some source (in this case,
that's an RD, so we can access sourceToken.sourceId::

      <script type="newSource" lang="python" id="markDeleted">
        table.query("UPDATE %s SET deleted=True"
          " WHERE sourceRD=%%(sourceRD)s"%id, 
          {"sourceRD": sourceToken.sourceId})
      </script>


This is a hacked way of ensuring some sort of referential integrity:
When a table containing "products" is dropped, the corresponding entries
in the products table are deleted::

  <script type="beforeDrop" lang="SQL" name="clean product table">
    DELETE FROM products WHERE sourceTable='\curtable'
  </script>

Note that this is actually quite hazardous because if the table is
dropped in any way not using the make element in the RD, this will not
be executed.  It's usually much smarter to tell the database to do the
housekeeping.  Rules are typically set in postCreation scripts::

  <script type="postCreation" lang="SQL">
    CREATE OR REPLACE RULE cleanupProducts AS 
      ON DELETE TO \curtable DO ALSO
      DELETE FROM products WHERE key=OLD.accref
  </script>

The decision if such arrangements are make before the import, before the
indexing or after the table is finished needs to be made based on the
script's purpose.

Another use for scripts is SQL function definition::

      <script type="postCreation" lang="SQL" name="Define USNOB matcher">
        CREATE OR REPLACE FUNCTION usnob_getmatch(alpha double precision, 
          delta double precision, windowSecs float
        ) RETURNS SETOF usnob.data AS $$
        DECLARE
          rec RECORD;
        BEGIN
          FOR rec IN (SELECT * FROM usnob.data WHERE 
            q3c_join(alpha, delta, raj2000, dej2000, windowSecs/3600.)) 
          LOOP
            RETURN NEXT rec;
          END LOOP;
        END;
        $$ LANGUAGE plpgsql;
      </script>

You can also load data, most usefully in preIndex scripts (although
beforeImport would work as well here)::

    <script type="preIndex" lang="SQL" name="create USNOB-PPMX crossmatch">
        SET work_mem=1000000;
        INSERT INTO usnob.ppmxcross (
          SELECT q3c_ang2ipix(raj2000, dej2000) AS ipix, p.localid 
          FROM 
            ppmx.data AS p, 
            usnob.data AS u 
          WHERE q3c_join(p.alphaFloat, p.deltaFloat, 
            u.raj2000, u.dej2000, 1.5/3600.))
    </script>



Bibliography
============

.. [RMI]  Hanisch, R., et al, "Resource Metadata for the Virtual
   Observatory", http://www.ivoa.net/Documents/latest/RM.html
.. [VOTSTC] Demleitner, M., Ochsenbein, F., McDowell, J., Rots, A.:
   "Referencing STC in VOTable", Version 2.0,
   http://www.ivoa.net/Documents/Notes/VOTableSTC/20100618/NOTE-VOTableSTC-2.0-20100618.pdf
.. _the DaCHS tutorial: http://docs.g-vo.org/DaCHS/tutorial.html
