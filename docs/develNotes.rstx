==========================================
Development notes for the GAVO Data Center
==========================================

This is basically a heap of stuff I intend to amend with informal docs
on what I'm doing to the software.  While I hope that at some point
it'll grow into a useful introduction to further developing the stuff,
right now it's a random collection that may even contain wrong
information.  Caveat emptor.

Parsing Anything
----------------

Here's a short version of what to do to come from almost any data to a
table; for an example with complete, handmade resource
descriptor, see votabletest.VotableTest:

#. Set up a DataTransformer to parse the data.  You can either read
	 it from an XML file within a resource descriptor or build it by hand.
	 There's an example for how that's done in tests/votabletest.py
#. Constuct a resource.InternalDataSet.  It takes the data descriptor
   you created, table.Table (unless you want a weird sort of table) and
   the data you want parsed.  This can be a file, a table, a file name
	 and possibly other things parsed.   Note, however, that the input
   given must match the grammar on your DataTransformer.  By default,
	 this will create all tables.  If you only want a subset of tables
	 created, pass a sequence of their names in tablesToBuild.


Internally, the system does the following:

#. DataSet calls its _fillTables method that will create a tableMaker
   instance for each desired table.
#. DataSet then calls its _parseSources method.[#booster]_ 
#. _parseSources iterates of DataSet._iterParseContexts.  A parse
   context is a wrapper for obtaining input to grammars and exporting
   the parsed data (see resource.ParseContext).
#. _parseSources calls each parse context's parse method
#. It, in turn, calls the grammar's parse method with itself as
   argument.
#. grammar.parse first calls the _getDocdict method of its concrete
   implementation and passes the result on to the parse context's
   processDocdict method, or, if the grammars docIsRow attribute is
   true, to its own handleRowdict method (see below)
#. grammar.parse then iterates over the implementation's _iterRows.
   The result is passed to grammar.handleRowdict.
#. handleRowdict arranges for row processors to run, expands macros and
   hands over the resulting row dictionary/ies over to the parse context's 
   processRowdict method
#. This then uses ParseContext._buildRecord to build a record with
   correct python types.  Here, validation and constraint checking takes
   place.
#. Finally, processRowdict calls the addData method of every table it
   knows as a destination.
 
I know this is horribly overengineered.


Conversion of Types and Values
------------------------------

Internally, the software obviously uses the python type system, but the
"official" interface is defined in terms of PostgresSQL's (one should
try to keep it down to SQL's).  The knowledge of how to transform the
types should be kept in gavo.typesystems to keep things maintainable.

With values, it's much more of a pain, even more, because some given
value may be represented differently in, say, HTML than in a VOTable.

For VOTables, the software tries to infer the proper representation from
the sql type, the python type, ucds and units, for HTML it uses
displayHints.


Testing
-------

I'm a big fan of doctests.  Pity there are so few of them in the code
right now.

I'll write pyunit-based test code to the tests subdirectory.  I haven't
yet decided how I'll integrate those and the docstests.

Many of the tests will require database connectivity.  I'll try and
design the tests so they don't need resources apart from the
availability of a test db profile.  I have this set up on my private
machine like this:

::

	[profiles]
	test:test

in ~/.gavorc and

::

	database=gavo
	user=msdemlei

in ~/.gavo/etc -- of course, for this to work, you need a postgresql
engine running on your machine with a database gavo in which your role
is superuser (well, has sufficient rights...).


Metadata
--------

Within the framework, there are two main sources of metadata.  For one,
the fields (via datadef.DataField) of a table or a document record carry
metadata on their types, ucds, etc.

Metadata pertaining to other entities than fields is kept with these
entities, viz., ResourceDescriptor, DataDescriptor, DataSet, Table, and
RecordDef, instances.  All these mix in the parsing.meta.MetaMixin
providing getMeta and addMeta methods.

It is the metadata containers' responsibility to choose their parents
and children.  They or their parents have to call the setMetaParent
method.

MetaMixin helps a bit: When the setMetaParent method is called, it
calls the registerAsMetaParent() method that can be overridden to
register the object as a parent to all children.  In this way the meta
relations within a ResourceDescriptor are established after construction
is finished.


Describing Metadata
'''''''''''''''''''

We don't have types on metadata, and thus metadata is (at least at
first) flat.  To introduce hierarchy, there is attachment of
metadata to (hierarchically organized) entities (see `Getting
Metadata`_) and there is paths.  By convention, items in paths are
seperated by dots, such that something like curation.creator.name would
be a valid key.  The metadata machinery doesn't know of this convention,
though, it's entirely a matter of templates and "user code".

Here's a list of keys I use, mostly referring to the metadata papers
from IVOA or local (leading underscore) metadata:

* _type -- on DataSets, this becomes the type attribute of the VOTable.
* _query_status -- on DataSets, this can be used to communicate the
	value of an INFO element in the VOTable (see SIAP spec).
* _legal -- human-readable unstructured information on the legal 
  status of the data.
* _infolink -- a URL pointing to further unstructured human-readable
  information to the data content

Getting Metadata
''''''''''''''''

Metadata are accessed by name (or "key", if you will).

The getMeta method usually follows the enclosure hierarchy up, meaning
that if a meta item is not found in the current instance, it will ask
its parent for that item, and so on.  If no parent is known, the meta
information contained in the configuration will be consulted.  If all
fails, None is returned.  As an example, querying metadata on a Table
will ask DataSet (XXX shouldn't it ask the RecordDef?  Right now, that
won't work because Tables don't get RecordDefs but fieldDefs XXX),
DataDescriptor, RecordDef and finally config.

If you require metadata exactly for the item you are querying, call
getMeta(key, propagate=False).


Setting Metadata
''''''''''''''''

You can programmatically set metadata on any metadata container by
calling container.addMeta(args).  You can pass addMeta a dictionary as
first argument.  This dictionary will be copied and amended with any
keyword arguments to arrive at the constructor arguments for a MetaItem
instance.   If no dictionary is passed, only keyword arguments are used. 
Supported keywords include:

* name (required): the name of the meta item.  For conventional names,
  see above.
* content (required): a string.
* format (default None): a string giving how the content is to be
  interpreted.  Currently defined are "literal" and "plain".  Literal
  values are not touched and rendered in pre elements in HTML.  Plain
  values are whitespace-normalized and get p tags in HTML.  In the
  future, we will probably support ReStructured text.
* compute, combine -- XXX Document

Note that content should be a unicode string as soon as it contains
non-ascii characters.


VOTables
--------

table.Table instances are primarily meant to be serialized into VOTable
tables.  Since most of the metadata of tables will be contained in the
parent DataSet's docRec, PARAM elements of tables will be taken from
there. XXX TODO: there's also INFO and LINK.  Have some convention as to
what goes where.

However, Tables are meta containers and can contain meta information.

With VOTables, correct formatting of values becomes a particular
problem.  While presentation is largely a non-issue, it is paramount
that the literals actually match what the ucds, units and types give.
Therefore, displayHints (apart from "suppress", which by default is
honored) are ignored.  Instead, votable.py defines MapperFactories.
These are just callables taking ColProperties (in a pinch, dicts having
"sufficient" keys will do too, where sufficient at least includes
``ucd``, ``unit``, ``datatype``, ``arraysize``, and ``dbtype``, possibly
more) and returning either None (meaning they won't handle values for
this column) or a callable returning a string.

These MapperFactories are organized in a Registry that can be queried
for a mapper.  If you need to do some special mapping, get a copy of
the default mapper registry by calling ``votable.getMapperRegistry``,
write mappers (a couple of the keys available are listed above, but 
votable calls the mappers with properly filled out votable.ColProperties
instances, so you can, e.g., look at min, max, and hasNulls), and
register them using the ``registerFactory`` method of the registry.  The
mappers will be called in reverse order of registration, so you can
override default behaviour, and you should register the most special
mappers last.

Mapper factories may decide to alter the type they're returning (in
fact, for things like date they'll in all likelihood need to).  To do
that, change the datatype and arraysize attributes.  After the mappers
have run, nothing will look at the dbtype any more.

To add new default mappers, add one in votable.py and call
_registerDefaultMF on them.


The New Web Structure
---------------------

The new web structure is based on resource descriptors.  These define
data, adapters and services.  A service is a combination of a
(possibly optional) input adapter, a core, and optional output adapters.

Adapters are basically datadef.DataTransformers, i.e., they generate
tables using grammars.  For input adapters, these grammars will
typically be context grammars, for output adapters, table grammars.

Cores can be many things.  Right now, there is a core for computed
tables and one for the simple image access protocol.  Further cores will
be added for SCS, a querulator-like service, etc.  Cores must have a run
method receiving an input table.  They must return a twisted.deferred
the fires an output table or a file-like object that can immediately be
delivered in cases where it generates a single file.

Usually, there will be ContextGrammars in front of cores.  However, when
a core defines a method getInputFields, it can work "naked".  The method
must return a sequence of datadef.DataField instances describing what
input fields they need.


Services
''''''''

I'm trying to develop service.Service into a generic container for
querying the data holdings.  The rough idea is that whatever comes from
the web is first transformed into a data set (probably mostly
constisting of a document record only), then stuffed into a core service
that does the actual query and afterwards possibly modified again by
output filters.  

This is reflected in Service.getResult: It gets inputData from an
inputFilter (which will usually be an adapter with a ContextGrammar),
or, lacking one, by stuffing the input into a makeshift DataSet with a
document record keeping the arguments we got from the web (and possibly
a bit more). Let's call this the inputTable.

Then, the core is run on this data set -- each core must
bring its run method that takes the inputTable and returns a twisted
Deferred object.  The callback for this object is set to _parseResult,
which in turn builds whatever is defined as the output table by the
service, which again may be filtered by an outputFilter.

The inputFilter of a service has to support:

* a getInputFields method returning datadef.DataField fields for every
	input it needs (we'll need to think about "bulk data" later, but I
	guess we may get by using file upload-like things).
* a datadef.DataTransformer interface


The core of a service has to provide:

* a run method that receives an InternalDataset instance and returns
	something.
* a parse method that receives whatever run returns and returns an
	InternalDataset.


Default Output Filter
'''''''''''''''''''''

Services should have the default output filter.  It looks at the
input key "output".  There should be a dict in there giving the output
format (basically VOTable or HTML).  If it is HTML, the default output
filter only keeps columns that have a displayHint.  If it is VOTable,
the default output filter also looks at VERB, which may be 1 (maximally
terse) through 3 (maximally verbose).  Only fields with
verbLevel<=VERB*10 and displayHint!="suppress" are retained in the output.

Fields with no explicit verbLevel are assumed to have verbLevel 30, so
they'll only appear in maximally verbose tables.  You'll definitely want
to set explicit verbLevels if you plan to use the default output filter.


Interfaces
----------

One nice architectural feature would be to have interfaces actually
implement functionality -- a positional interface could tell how to run
cone searches on it, etc.  I haven't yet done this, because in effect
we'd like to have another indirection here: various interfaces could
implement SIAP-like queries, etc.  Also, we'd have to keep interfaces
around at runtime.  While it's quite clear that interfaces would always
be tied to table definitions (I should rename RecordDef at some
point...), the actual location of the interfaces requires some
thought.

Meanwhile, all interfaces have the getNodes method, which is nifty for
testing -- it (almost) returns a list of datadef.DataField instances for
the fields defined in the interface.  See tests/testsiap.py for an
example of testing against an interface if you're lazy and don't want an
actual table implementing the interface.


User management
---------------

The rights model used here is simple: There are users and groups, where
for each user there's a group and vice versa.  Access restrictions are
stated in terms of groups, and users can belong to groups.  Access to a
protected resource is given to any user that belongs to that group.

Users and groups are stored in two database tables generated from
user.vord.  Admittedly, it's overkill to use the gavo framework for
these tables, but there you are.


Memoization
-----------

The resourcecache module should be the central point for all kinds of
memoization/caching issues.  To keep dependencies and risks of
recursive imports low, it is the providing modules' responsibility to
register caching functions.  The idea is that, e.g., importparser wants
a cache of resource descriptors.  It should then call

resourcecache.makeCache("getRd", getRd)

Clients would then call

resourcecache.getRd(id).

This mechanism for now is restricted to items that come with a unique
id (the argument).  It would be easy to extend this to multiple-argument
functions, but I don't think that's a good idea -- the "identities" of
the cached objects should be kept simple.

No provision is made to prevent accidental overwriting of function names.

This scheme has the central flaw that you need to make sure that you or
somebody else actually imports the module defining the resource you want
to access.

All caches can be cleared by calling clearCaches.  This only affects
caches make via makeCache.  There probably should be similar mechanisms
for other shared resources, since we want to be able to respond to
"reload"-like requests.


Coordinate Systems
------------------

This is a very weak point right know.  All those transforms aren't
rocket science, but still need to be worked out.

Cartesian Coordinates
'''''''''''''''''''''

For many geometrical operations, we use cartesian coordinates (a.k.a.
c_x, c_y, c_z).  These correspond to the point at which the radius
vector to RA, Dec crosses the unit sphere.  These coordinates have many
desirable properties (e.g., there are no "stich" lines for them).

The cartesian coordinates are oriented such that the x axis is aligned
with alpha=0, the y axis with alpha=90 degrees or 6 hours, and the z
axis with delta=90 degrees.  Most of this is implemented in coords.py.

If you're doing more with this that just compute it and hand it over to
the DB engine, you may want to use the Vector3 class in coords.  I wrote
it when I tried to do siap-style matches using cartesians.  Since that
doesn't work, the class is, as it were, orphaned right now.


SIAP
----

Our plan for coping with the nasty stitching and the degeneracy at the
poles with polar coordinates and SIAP is as follows:

Every table supporting siap queries has two fields, primaryBox and
secondaryBox; in SQL both are of the type BOX, and I have a local class
Box in coords that these may be converted from/to.  

For "harmless" areas, only primaryBox is non-NULL and contains a
bounding box for the area in RA and DEC.  A field that covers the stitch
line at 360/0 has a secondaryBox; the primary box is left of the stich
line (i.e., box.x1<360, box.x0=360), the secondary box is right of the
stitch line (i.e., box.x1=0, box.x0>0).

The ROI works analogously.

With this plan, the SIAP conditions become (in Postgres notation):

* COVERS: primaryBox ~ roiPrimary AND (secondaryBox IS NULL OR
  secondaryBox ~ roiSecondary)
* ENCLOSED: roiPrimary ~ primaryBox AND (roiSecondary IS NULL OR
	roiSecondary ~ secondaryBox)
* CENTER: roiCenter ~ primaryBox OR roiCenter ~ secondaryBox
* OVERLAPS: primaryBox && roiPrimary OR (
	secondaryBbox AND secondaryBox && roiSecondary) OR (
	secondaryBbox AND secondaryBox && roiPrimary) OR (
	roiSecondary AND roiSecondary && primaryBox)

There's code in siap.py that computes these boxes.


Databases
---------

The code started out using pypgsql, but after handling multimillon-row
datasets, I largely gave up on it, instead going for psycopg2.  Most
things still should work with both interfaces.  However, psycopg2
by default uses python's native DateTime rather that egenix' mxDateTime.
psycopg2 needs to be compiled with mxDateTime support, or bad things
will happen.

Currently, the main thing that doesn't work for pypgsql is SIAP.  I use
boxes in there and only adapt them for psycopg2.  It probabaly wouldn't
be hard to retrofit it to pypgsql, but I can't see much sense in doing
this.


Scrap
-----

To get a value from a rowdict and a recordDef, use the getValueIn method of
datadef.DataField.  This way, you'll have consistent semantics of source and 
value.  You may pass in an @-expanding function, but if you're sure there are
not @s or you don't want them expanded, it doesn't hurt if you don't.


Warts
-----

The whole system has lots and lots of warts, and the main thing I can
say in the way of justification is that most of the time, I didn't
really know where the whole thing was to be going.  At the few times I
really knew what I was going for, I, admittedly, went for q'n'd
"solutions" quite frequently.

Grep for XXX in the sources to find (sometimes minor) warts.  Here, I'll
collect some higher level warts:

* Table names have a horribly defined semantics.  They are used as table
	names in SQL, as ids and, in the context of web services, as function
	designators ("output").  At least the last function is incompatible
	with the others (because more than one "output" will be present within
	a schema).  We should untangle this, probably introducing a "function"
	attribute for RecordDef and allowing tables without names.
* ...and RecordDef is a lousy name for what really is a table
	definition.
* The whole business of passing InternalDataSets around when processing
	web queries and usually just using the first table in them is a pain.
	I think DataSets should have a designated "interesting" table when
	something insists on having a single table, and instances where things
	insist on that should be dramatically reduced in the first place.


Lessons learnt
--------------

There's an old saying "Get your data structures right, and the rest of
the program will write itself".  This couldn't be more true here, and
unfortunately, I didn't get them right, mostly, because I started out
doing something utterly useless.  

Anyway, I think one should set out doing a "pure python" model of a
VOTable, i.e., a set of classes modelling *easily accessible* and
*easily constructable* (possibly along the lines of stan?) data tables
containing all the information necessary to build a VOTable.  The
VOTable data model isn't all that bad.  Also, when it comes to
generating VOTables according to DAL protocols, having abstracted away
LINK, INFO, and PARAM is a major pain.

These VOTables should be able to exist standalone, but having
descriptors containing standard metadata certainly is a good idea.  I'm
still not sure how I'd do this.  Resource descriptors like I have them
here are too fat, anyway.

And, think of the metadata from the very start.  Having to glue them on
later is a pain.  I guess that's less of a problem once there are
self-contained Tables that don't (necessarily) refer to external
objects.

Also, in your data trees, make sure you always have a parent reference.

The managed-attribute things (record.Record) probably were not a good
idea.  I liked the idea because it *may* help when parsing from XML
(that is what they still are used for almost exclusively), but I never
actually used the stuff.  Having accessor methods generated
automatically is nice, but if you absolutely want this, do it properly
and use metaclasses.

You can *either* have a common interface for HTML and VOTables *or* use
automated form generating and validating libraries.  The simple reason
is that forms libraries (like formal) insist on redisplaying the form,
and that's not acceptable for standard services like SIAP or SCS.

.. [#booster]  DataSet._parseSources can take an
   optional parseSwitcher that you can pass in when you construct a
   DataSet directly.  The parseSwitcher, if defined, can override the parse
   method of the grammar, which is used for parser boosting (e.g., when
   filling the table through copy statements).
