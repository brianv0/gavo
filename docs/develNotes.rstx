==========================================
Development notes for the GAVO Data Center
==========================================

This is basically a heap of stuff I intend to amend with informal docs
on what I'm doing to the software.  While I hope that at some point
it'll grow into a useful introduction to further developing the stuff,
right now it's a random collection that may even contain wrong
information.  Caveat emptor.

Parsing Anything
----------------

Here's a short version of what to do to come from almost any data to a
table; for an example with complete, handmade resource
descriptor, see votabletest.VotableTest:

#. Set up a DataTransformer to parse the data.  You can either read
	 it from an XML file within a resource descriptor or build it by hand.
#. Constuct a resource.InternalDataSet.  It takes the data descriptor
   you created, table.Table (unless you want a weird sort of table) and
   the data you want parsed.  This can be a file, a table, a file name
	 and possibly other things parsed.  This can be a file, a table, a
   file name and possibly other things.  Note, however, that the input
   given must match the grammar on your DataTransformer.


Conversion of Types and Values
------------------------------

Internally, the software obviously uses the python type system, but the
"official" interface is defined in terms of PostgresSQL's (one should
try to keep it down to SQL's).  The knowledge of how to transform the
types should be kept in gavo.typesystems to keep things maintainable.

With values, it's much more of a pain, even more, because some given
value may be represented differently in, say, HTML than in a VOTable.

For VOTables, the software tries to infer the proper representation from
the sql type, the python type, ucds and units, for HTML it uses
displayHints.


Testing
-------

I'm a big fan of doctests.  Pity there are so few of them in the code
right now.

I'll write pyunit-based test code to the tests subdirectory.  I haven't
yet decided how I'll integrate those and the docstests.

Many of the tests will require database connectivity.  I'll try and
design the tests so they don't need resources apart from the
availability of a test db profile.  I have this set up on my private
machine like this:

::

	[profiles]
	test:test

in ~/.gavorc and

::

	database=gavo
	user=msdemlei

in ~/.gavo/etc -- of course, for this to work, you need a postgresql
engine running on your machine with a database gavo in which your role
is superuser (well, has sufficient rights...).

The New Web Structure
---------------------

The new web structure is based on resource descriptors.  These define
data, adapters and services.  A service is a combination of a
(possibly optional) input adapter, a core, and optional output adapters.

Adapters are basically DataDefinitions, i.e., they generate tables using
grammars.  For input adapters, these grammars will typically be context
grammars, for output adapters, table grammars.

Cores can be many things.  Right now, there is a core for computed
tables and one for the simple image access protocol.  Further cores will
be added for SCS, a querulator-like service, etc.

The service code will ask either the input adapter or the core for the
input keys it requires and presents the user with a form for these input
keys.  The input keys are typically generated from a context grammar on input
adapters.

Cores should usually have some way of returning input keys as well,
unless they definitely require a full-fledged table that needs to be
provided by an input adapter.  In that way, "naked" cores can make up a
service.  To do that, they have to provide a method getInputFields
returning a sequence of datadef.DataField instances describing what
input fields they need.


Metadata
--------

Within the framework, there are two main sources of metadata.  For one,
the fields (via datadef.DataField) of a table or a document record carry
metadata on their types, ucds, etc.

Metadata pertaining to other entities than fields is kept with these
entities, viz., ResourceDescriptor, DataDescriptor, DataSet, Table, and
RecordDef, instances.  All these mix in the parsing.meta.MetaMixin
providing getMeta and addMeta methods.

It is the metadata containers' responsibility to choose their parents
and children.  They or their parents have to call the setMetaParent
method.

MetaMixin helps a bit: When the setMetaParent method is called, it
calls the registerAsMetaParent() method that can be overridden to
register the object as a parent to all children.  In this way the meta
relations within a ResourceDescriptor are established after construction
is finished.


Describing Metadata
'''''''''''''''''''

We don't have types on metadata, and thus metadata is (at least at
first) flat.  To introduce hierarchy, there is attachment of
metadata to (hierarchically organized) entities (see `Getting
Metadata`_) and there is paths.  By convention, items in paths are
seperated by dots, such that something like curation.creator.name would
be a valid key.  The metadata machinery doesn't know of this convention,
though, it's entirely a matter of templates and "user code".

Here's a list of keys I use, mostly referring to the metadata papers
from IVOA or local (leading underscore) metadata:

* _type -- on DataSets, this becomes the type attribute of the VOTable.
* _query_status -- on DataSets, this can be used to communicate the
	value of an INFO element in the VOTable (see SIAP spec).
* _legal -- human-readable unstructured information on the legal 
  status of the data.
* _infolink -- a URL pointing to further unstructured human-readable
  information to the data content

Getting Metadata
''''''''''''''''

Metadata are accessed by name (or "key", if you will).

The getMeta method usually follows the enclosure hierarchy up, meaning
that if a meta item is not found in the current instance, it will ask
its parent for that item, and so on.  If no parent is known, the meta
information contained in the configuration will be consulted.  If all
fails, None is returned.  As an example, querying metadata on a Table
will ask DataSet (XXX shouldn't it ask the RecordDef?  Right now, that
won't work because Tables don't get RecordDefs but fieldDefs XXX),
DataDescriptor, RecordDef and finally config.

If you require metadata exactly for the item you are querying, call
getMeta(key, propagate=False).


Setting Metadata
''''''''''''''''

You can programmatically set metadata on any metadata container by
calling container.addMeta(args).  You can pass addMeta a dictionary as
first argument.  This dictionary will be copied and amended with any
keyword arguments to arrive at the constructor arguments for a MetaItem
instance.   If no dictionary is passed, only keyword arguments are used. 
Supported keywords include:

* name (required): the name of the meta item.  For conventional names,
  see above.
* content (required): a string.
* format (default None): a string giving how the content is to be
  interpreted.  Currently defined are "literal" and "plain".  Literal
  values are not touched and rendered in pre elements in HTML.  Plain
  values are whitespace-normalized and get p tags in HTML.  In the
  future, we will probably support ReStructured text.
* compute, combine -- XXX Document

Note that content should be a unicode string as soon as it contains
non-ascii characters.

VOTables
--------


table.Table instances are primarily meant to be serialized into VOTable
tables.  Since most of the metadata of tables will be contained in the
parent DataSet's docRec, PARAM elements of tables will be taken from
there.

However, Tables are meta containers and can contain meta information.

Services
--------

I'm trying to develop service.Service into a generic container for
querying the data holdings.  The rough idea is that whatever comes from
the web is first transformed into a data set (probably mostly
constisting of a document record only), then stuffed into a core service
that does the actual query and afterwards possibly modified again by
output filters.  

This is reflected in Service.getResult: It gets inputData from an
inputFilter (which will usually be an adapter with a ContextGrammar),
or, lacking one, by stuffing the input into a makeshift DataSet with a
document record keeping the arguments we got from the web (and possibly
a bit more). Let's call this the inputTable.

Then, the core is run on this data set -- each core must
bring its run method that takes the inputTable and returns a twisted
Deferred object.  The callback for this object is set to _parseResult,
which in turn builds whatever is defined as the output table by the
service, which again may be filtered by an outputFilter.

The inputFilter of a service has to support:

* a getInputFields method returning datadef.DataField fields for every
	input it needs (we'll need to think about "bulk data" later, but I
	guess we may get by using file upload-like things).
* a datadef.DataTransformer interface


The core of a service has to provide:

* a run method that receives an InternalDataset instance and returns
	something.
* a parse method that receives whatever run returns and returns an
	InternalDataset.


Interfaces
----------

One nice architectural feature would be to have interfaces actually
implement functionality -- a positional interface could tell how to run
cone searches on it, etc.  I haven't yet done this, because in effect
we'd like to have another indirection here: various interfaces could
implement SIAP-like queries, etc.  Also, we'd have to keep interfaces
around at runtime.  While it's quite clear that interfaces would always
be tied to table definitions (I should rename RecordDef at some
point...), the actual location of the interfaces requires some
thought.

Meanwhile, all interfaces have the getNodes method, which is nifty for
testing -- it (almost) returns a list of datadef.DataField instances for
the fields defined in the interface.  See tests/testsiap.py for an
example of testing against an interface if you're lazy and don't want an
actual table implementing the interface.

Memoization
-----------

The resourcecache module should be the central point for all kinds of
memoization/caching issues.  To keep dependencies and risks of
recursive imports low, it is the providing modules' responsibility to
register caching functions.  The idea is that, e.g., importparser wants
a cache of resource descriptors.  It should then call

resourcecache.makeCache("getRd", getRd)

Clients would then call

resourcecache.getRd(id).

This mechanism for now is restricted to items that come with a unique
id (the argument).  It would be easy to extend this to multiple-argument
functions, but I don't think that's a good idea -- the "identities" of
the cached objects should be kept simple.

No provision is made to prevent accidental overwriting of function names.

This scheme has the central flaw that you need to make sure that you or
somebody else actually imports the module defining the resource you want
to access -- however, you may never actually grab a name from that
namespace in the rest of the module.

All caches can be cleared by calling clearCaches.  This only affects
caches make via makeCache.  There probably should be similar mechanisms
for other shared resources, since we want to be able to respond to
"reload"-like requests.


Coordinate Systems
------------------

This is a very weak point right know.  All those transforms aren't
rocket science, but still need to be worked out.

Cartesian Coordinates
'''''''''''''''''''''

For many geometrical operations, we use cartesian coordinates (a.k.a.
c_x, c_y, c_z).  These correspond to the point at which the radius
vector to RA, Dec crosses the unit sphere.  These coordinates have many
desirable properties (e.g., there are no "stich" lines for them).

The cartesian coordinates are oriented such that the x axis is aligned
with alpha=0, the y axis with alpha=90 degrees or 6 hours, and the z
axis with delta=90 degrees.  Most of this is implemented in coords.py.

If you're doing more with this that just compute it and hand it over to
the DB engine, you may want to use the Vector3 class in coords.  I wrote
it when I tried to do siap-style matches using cartesians.  Since that
doesn't work, the class is, as it were, orphaned right now.


SIAP
----

Our plan for coping with the nasty stitching and the degeneracy at the
poles with polar coordinates and SIAP is as follows:

Every table supporting siap queries has two fields, primaryBox and
secondaryBox; in SQL both are of the type BOX, and I have a local class
Box in coords that these may be converted from/to.  

For "harmless" areas, only primaryBox is non-NULL and contains a
bounding box for the area in RA and DEC.  A field that covers the stitch
line at 360/0 has a secondaryBox; the primary box is left of the stich
line (i.e., box.x1<360, box.x0=360), the secondary box is right of the
stitch line (i.e., box.x1=0, box.x0>0).

The ROI works analogously.

With this plan, the SIAP conditions become (in Postgres notation):

* COVERS: primaryBox ~ roiPrimary AND (secondaryBox IS NULL OR
  secondaryBox ~ roiSecondary)
* ENCLOSED: roiPrimary ~ primaryBox AND (roiSecondary IS NULL OR
	roiSecondary ~ secondaryBox)
* CENTER: roiCenter ~ primaryBox OR roiCenter ~ secondaryBox
* OVERLAPS: primaryBox && roiPrimary OR (
	secondaryBbox AND secondaryBox && roiSecondary) OR (
	secondaryBbox AND secondaryBox && roiPrimary) OR (
	roiSecondary AND roiSecondary && primaryBox)

There's code in siap.py that computes these boxes.


Databases
---------

The code started out using pypgsql, but after handling multimillon-row
datasets, I largely gave up on it, instead going for psycopg2.  Most
things still should work with both interfaces.  However, psycopg2
by default uses python's native DateTime rather that egenix' mxDateTime.
psycopg2 needs to be compiled with mxDateTime support, or bad things
will happen.

Currently, the main thing that doesn't work for pypgsql is SIAP.  I use
boxes in there and only adapt them for psycopg2.  It probabaly wouldn't
be hard to retrofit it to pypgsql, but I can't see much sense in doing
this.


Scrap
-----

To get a value from a rowdict and a recordDef, use the getValueIn method of
datadef.DataField.  This way, you'll have consistent semantics of source and 
value.  You may pass in an @-expanding function, but if you're sure there are
not @s or you don't want them expanded, it doesn't hurt if you don't.


Warts
-----

The whole system has lots and lots of warts, and the main thing I can
say in the way of justification is that most of the time, I didn't
really know where the whole thing was to be going.  At the few times I
really knew what I was going for, I, admittedly, went for q'n'd
"solutions" quite frequently.

Grep for XXX in the sources to find (sometimes minor) warts.  Here, I'll
collect some higher level warts:

* Table names have a horribly defined semantics.  They are used as table
	names in SQL, as ids and, in the context of web services, as function
	designators ("output").  At least the last function is incompatible
	with the others (because more than one "output" will be present within
	a schema).  We should untangle this, probably introducing a "function"
	attribute for RecordDef and allowing tables without names.
* ...and RecordDef is a lousy name for what really is a table
	definition.
* The whole business of passing InternalDataSets around when processing
	web queries and usually just using the first table in them is a pain.
	I think DataSets should have a designated "interesting" table when
	something insists on having a single table, and instances where things
	insist on that should be dramatically reduced in the first place.
