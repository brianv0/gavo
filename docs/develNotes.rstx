==========================================
Development notes for the GAVO Data Center
==========================================

This is basically a heap of stuff I intend to amend with informal docs
on what I'm doing to the software.  While I hope that at some point
it'll grow into a useful introduction to further developing the stuff,
right now it's a random collection that may even contain wrong
information.  Caveat emptor.

Testing
-------

I'm a big fan of doctests.  Pity there are so few of them in the code
right now.

I'll write pyunit-based test code to the tests subdirectory.  I haven't
yet decided how I'll integrate those and the docstests.

Many of the tests will require database connectivity.  I'll try and
design the tests so they don't need resources apart from the
availability of a test db profile.  I have this set up on my private
machine like this:

::

	[profiles]
	test:test

in ~/.gavorc and

::

	database=gavo
	user=msdemlei

in ~/.gavo/etc -- of course, for this to work, you need a postgresql
engine running on your machine with a database gavo in which your role
is superuser (well, has sufficient rights...).

The New Web Structure
---------------------

The new web structure is based on resource descriptors.  These define
data, adapters and services.  A service is a combination of a
(possibly optional) input adapter, a core, and optional output adapters.

Adapters are basically DataDefinitions, i.e., they generate tables using
grammars.  For input adapters, these grammars will typically be context
grammars, for output adapters, table grammars.

Cores can be many things.  Right now, there is a core for computed
tables and one for the simple image access protocol.  Further cores will
be added for SCS, a querulator-like service, etc.

The service code will ask either the input adapter or the core for the
input keys it requires and presents the user with a form for these input
keys.  The input keys are typically generated from a context grammar on input
adapters.

Cores should usually have some way of returning input keys as well,
unless they definitely require a full-fledged table that needs to be
provided by an input adapter.  In that way, "naked" cores can make up a
service.  To do that, they have to provide a method getInputFields
returning a sequence of datadef.DataField instances describing what
input fields they need.


Services
--------

I'm trying to develop service.Service into a generic container for
querying the data holdings.  The rough idea is that whatever comes from
the web is first transformed into a data set (probably mostly
constisting of a document record only), then stuffed into a core service
that does the actual query and afterwards possibly modified again by
output filters.  

This is reflected in Service.getResult: It gets inputData from an
inputFilter (which will usually be an adapter with a ContextGrammar),
or, lacking one, by stuffing the input into a makeshift DataSet with a
document record keeping the arguments we got from the web (and possibly
a bit more). Let's call this the inputTable.

Then, the core is run on this data set -- each core must
bring its run method that takes the inputTable and returns a twisted
Deferred object.  The callback for this object is set to _parseResult,
which in turn builds whatever is defined as the output table by the
service, which again may be filtered by an outputFilter.

The inputFilter of a service has to support:

* a getInputFields method returning datadef.DataField fields for every
	input it needs (we'll need to think about "bulk data" later, but I
	guess we may get by using file upload-like things).
* a datadef.DataTransformer interface


The core of a service has to provide:

* a run method that receives an InternalDataset instance and returns
	something.
* a parse method that receives whatever run returns and returns an
	InternalDataset.


Interfaces
----------

One nice architectural feature would be to have interfaces actually
implement functionality -- a positional interface could tell how to run
cone searches on it, etc.  I haven't yet done this, because in effect
we'd like to have another indirection here: various interfaces could
implement SIAP-like queries, etc.  Also, we'd have to keep interfaces
around at runtime.  While it's quite clear that interfaces would always
be tied to table definitions (I should rename RecordDef at some
point...), the actual location of the interfaces requires some
thought.

Meanwhile, all interfaces have the getNodes method, which is nifty for
testing -- it (almost) returns a list of datadef.DataField instances for
the fields defined in the interface.  See tests/testsiap.py for an
example of testing against an interface if you're lazy and don't want an
actual table implementing the interface.

Memoization
-----------

The resourcecache module should be the central point for all kinds of
memoization/caching issues.  To keep dependencies and risks of
recursive imports low, it is the providing modules' responsibility to
register caching functions.  The idea is that, e.g., importparser wants
a cache of resource descriptors.  It should then call

resourcecache.makeCache("getRd", getRd)

Clients would then call

resourcecache.getRd(id).

This mechanism for now is restricted to items that come with a unique
id (the argument).  It would be easy to extend this to multiple-argument
functions, but I don't think that's a good idea -- the "identities" of
the cached objects should be kept simple.

No provision is made to prevent accidental overwriting of function names.

This scheme has the central flaw that you need to make sure that you or
somebody else actually imports the module defining the resource you want
to access -- however, you may never actually grab a name from that
namespace in the rest of the module.

All caches can be cleared by calling clearCaches.  This only affects
caches make via makeCache.  There probably should be similar mechanisms
for other shared resources, since we want to be able to respond to
"reload"-like requests.


Coordinate Systems
------------------

This is a very weak point right know.  All those transforms aren't
rocket science, but still need to be worked out.

Cartesian Coordinates
'''''''''''''''''''''

For many geometrical operations, we use cartesian coordinates (a.k.a.
c_x, c_y, c_z).  These correspond to the point at which the radius
vector to RA, Dec crosses the unit sphere.  These coordinates have many
desirable properties (e.g., there are no "stich" lines for them).

The cartesian coordinates are oriented such that the x axis is aligned
with alpha=0, the y axis with alpha=90 degrees or 6 hours, and the z
axis with delta=90 degrees.  Most of this is implemented in coords.py.

If you're doing more with this that just compute it and hand it over to
the DB engine, you may want to use the Vector3 class in coords.  I wrote
it when I tried to do siap-style matches using cartesians.  Since that
doesn't work, the class is, as it were, orphaned right now.


SIAP
----

Our plan for coping with the nasty stitching and the degeneracy at the
poles with polar coordinates and SIAP is as follows:

Every table supporting siap queries has two fields, primaryBox and
secondaryBox; in SQL both are of the type BOX, and I have a local class
Box in coords that these may be converted from/to.  

For "harmless" areas, only primaryBox is non-NULL and contains a
bounding box for the area in RA and DEC.  A field that covers the stitch
line at 360/0 has a secondaryBox; the primary box is left of the stich
line (i.e., box.x1<360, box.x0=360), the secondary box is right of the
stitch line (i.e., box.x1=0, box.x0>0).

The ROI works analogously.

With this plan, the SIAP conditions become (in Postgres notation):

* COVERS: primaryBox ~ roiPrimary AND (secondaryBox IS NULL OR
  secondaryBox ~ roiSecondary)
* ENCLOSED: roiPrimary ~ primaryBox AND (roiSecondary IS NULL OR
	roiSecondary ~ secondaryBox)
* CENTER: roiCenter ~ primaryBox OR roiCenter ~ secondaryBox
* OVERLAPS: primaryBox && roiPrimary OR (
	secondaryBbox AND secondaryBox && roiSecondary) OR (
	secondaryBbox AND secondaryBox && roiPrimary) OR (
	roiSecondary AND roiSecondary && primaryBox)

There's code in siap.py that computes these boxes.


Databases
---------

The code started out using pypgsql, but after handling multimillon-row
datasets, I largely gave up on it, instead going for psycopg2.  Most
things still should work with both interfaces.  However, psycopg2
by default uses python's native DateTime rather that egenix' mxDateTime.
psycopg2 needs to be compiled with mxDateTime support, or bad things
will happen.

Currently, the main thing that doesn't work for pypgsql is SIAP.  I use
boxes in there and only adapt them for psycopg2.  It probabaly wouldn't
be hard to retrofit it to pypgsql, but I can't see much sense in doing
this.


Scrap
-----

To get a value from a rowdict and a recordDef, use the getValueIn method of
datadef.DataField.  This way, you'll have consistent semantics of source and 
value.  You may pass in an @-expanding function, but if you're sure there are
not @s or you don't want them expanded, it doesn't hurt if you don't.


Warts
-----

The whole system has lots and lots of warts, and the main thing I can
say in the way of justification is that most of the time, I didn't
really know where the whole thing was to be going.  At the few times I
really knew what I was going for, I, admittedly, went for q'n'd
"solutions" quite frequently.

Grep for XXX in the sources to find (sometimes minor) warts.  Here, I'll
collect some higher level warts:

* Table names have a horribly defined semantics.  They are used as table
	names in SQL, as ids and, in the context of web services, as function
	designators ("output").  At least the last function is incompatible
	with the others (because more than one "output" will be present within
	a schema).  We should untangle this, probably introducing a "function"
	attribute for RecordDef and allowing tables without names.
* ...and RecordDef is a lousy name for what really is a table
	definition.
* The whole business of passing InternalDataSets around when processing
	web queries and usually just using the first table in them is a pain.
	I think DataSets should have a designated "interesting" table when
	something insists on having a single table, and instances where things
	insist on that should be dramatically reduced in the first place.
