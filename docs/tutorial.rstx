===================
GAVO DaCHS Tutorial
===================

.. contents:: 
  :depth: 2
  :backlinks: entry
  :class: toc


Ingesting Data
==============

Starting the RD
---------------

To ingest data, you will have to write a resource descriptor (RD).  We
recommend to keep everything handled by a specific RD together in on
directory that is a direct child of your inputs directory (see
installation and configuration), though you could group resources in
deeper subdirectories.  So, go to your inputs directory and say::

  mkdir lmcextinct

The directory name will (normally) appear in URLs, so it's a good idea
to choose something descriptive and short.  This directory is called the
resource directory.

We recommend to put the RD in the root of this directory.  A good
default name for the RD is "q.rd"; the "q" will appear in the default
URLs as well and usually looks good in there::

  cd lmcextinct
  vi q.rd

(where you can substitute vi with your favourite editor, of course).
Writing resource descriptors is what most of the operation of a data
center is about.  Let's start slowly by giving some metadata::

  <?xml version="1.0" encoding="iso-8859-1"?>

  <resource schema="lmcextinct">
    <meta name="title">Extinction within the LMC</meta>
    <meta name="creationDate">2009-06-02T08:42:00Z</meta>
    <meta name="description" format="plain">
      Extinction values in the area of the LMC...
    </meta>
    <meta name="copyright">Free to use.</meta>
    <meta name="creator.name">S. Author</meta>

    <meta name="subject">Large Magellanic Cloud</meta>
    <meta name="subject">Interstellar medium, nebulae</meta>
    <meta name="subject">Extinction</meta>

  </resource>

You need to adapt the encoding attribute in the prefix to match what you
are actually using if you plan on using non-ASCII-characters.  
You may want to use utf-8 instead of the iso-8859-1 given below
depending on your computer's setup.

The schema attribute on resource gives the schema tables for this
resource will turn up in.  You should, in general, use the subdirectory
name.  If you don't, you have to give the subdirectory name in a
resdir attribute.  This attribute must be the name of the resource
directory relative to the inputs directory specified in the
configuration.

Otherwise, there is only meta information so far.  This metadata is
cruicial for later registration of the service.  In HTML forms, it is
displayed in a sidebar.  See `RMI-style metadata
<./ref.html#rmi-style-metadata>`_ in the reference documentation.


Defining Target Tables
----------------------

Within the DC, data is represented in database tables, while metadata is
mostly kept within the resource descriptors.  A major part of this
metadata is the table structure.  It is defined in table elements, which
usually are direct children of the resoource element.  A resource
element may contain multiple table definitions.

Such a table definition might look like this::

  <table id="exts" onDisk="True" adql="True">
    <meta name="description">
      Extinction values within certain areas on the sky.
    </meta>
    <column name="bbox" type="box" unit="deg"
      description="Bounding box for the extinction data"/>
    <column name="centerAlpha" unit="deg" tablehead="RA"
      description="Area center RA ICRS" ucd="pos.eq.ra;meta.main"
      required="True"/>
    <column name="centerDelta" unit="deg" tablehead="Dec"
      description="Area center Declination ICRS" ucd="pos.eq.dec;meta.main"
      required="True"/>
    <column name="ev_i" unit="mag" tablehead="E(V-I)"
      ucd="arith.diff;phys.absorption;em.opt.V;em.opt.I"
      description="Difference in extinction between V and I bands"/>
    <column name="a_v" unit="mag" tablehead="A_V"
      ucd="phys.absorption;em.opt.V"
      description="Extinction in V"/>
    <column name="a_i" unit="mag" tablehead="A_I"
      ucd="phys.absorption;em.opt.I"
      description="Extinction in I"/>
  </table>

The table element has some attributes.  You must give id, which will
double as the table name within the database.  The onDisk attribute
specifies that the table is to reside on the disk as opposed to in
memory (in-memory tables have applications in advanced operations).  The
adql attribute specifies that no access restrictions are to be
placed on the table; if you run an ADQL or TAP service, users can access
this table.

Table elements may contain metadata.  You do not need to repeat metadata
given for the resource, because (in most cases) the DC performs metadata
inheritance.  This means that if a table is asked for a piece of
metadata it does not have, it forwards that request to the embedding
resource.

The main content of table is a sequence of column elements.  These
contain a description of a single table column.  You must give a name.
Type, if not given, defaults to real, and can otherwise take values in
valid SQL datatypes.  The DC software knows how to handle, in addition
to real,

* text -- a string.  You can also use types like char(7) and the like,
  but since that does not help postgres (or much anything else within
  the DC), this is not recommended.
* double precision (or double) -- a floating point number.  You should use
  in doubles if you need to keep more than about 7 digits of mantissa.
* integer (or int) -- typically a 32-bit integer
* bigint -- typically a 64-bit integer
* smallint -- typically a 16-bit integer
* timestamp -- a combination of date and time.  While postgres can
  process a very large range of dates, the DC stores timestamps in
  datetime.datetime objects, which means that for "astronomical" times
  (like 10000 B.C. or 10000 A.D. you may need to use custom
  representations. Also, the DC assumes all times to be without time
  zones.  Further time metadata (like distinguishing TT from UT) is
  given through STC specifications.
* date -- a date.  See timestamp.
* time -- a time.  See timestamp
* box -- a rectangle.

Some more types (like raw and file) are available to tables in service
definitions, but they should, in general, not appear in database tables.

Futher metadata on columns includes:

* unit -- the unit the column values are in.  The syntax is that defined
  by Vizier, but that may change pending further standardization in the
  VO.  Unit is left out for unitless values.
* tablehead -- a very short string designating the content.  This string
  is typically used for display purposes, e.g., as table headings or
  labels on input fields.
* description -- a longer string characterizing the content.  This may
  be in bubble help or VOTable descriptions.
* ucd -- a Unified Content Descriptor as defined by IVOA.  To figure out
  "good" UCDs, the UCD resolver at
  http://dc.zah.uni-heidelberg.de/ucds/ui/ui/form can help.
* requried -- True if value must be set in order for the record to be
  valid.  By default, NULL (which in python is None) is a valid value
  for any column.  For required columns, that is no longer the case.
  This is particularly important in connection with foreign keys.


Parsing Input Data
------------------

After you have defined the table, you will want to fill it.  You will
usually have one or more input files with "raw" data.
We recommend putting such input data files into a subdirectory of their
own named "data".  Let's assume we have one input file for the table
above, called lmc_extinction_values.txt.  Suppose it looks like this,
where tabs in the input are shown as "\\t"::

  RA_min\\tRA_max\\tDEC_min\\tDEC_max\\tE(V-I)\\tA_V\\tA_I
  78.910625\\t78.982146\\t-69.557417\\t-69.480639\\t0.04\\t0.092571\\t0.123429
  78.910625\\t78.982146\\t-69.480639\\t-69.403861\\t0.05\\t0.115714\\t0.154286
  78.910625\\t78.982146\\t-69.403861\\t-69.327083\\t0.05\\t0.115714\\t0.154286

The first step for ingestion is lexical analysis.  In the DC software,
this is performed by grammars.  There are many grammars defined, e.g.,
for getting values from FITS files, VOTables, or using column-based
formats; you can also write specialized grammars in python.   

All grammars read "something" and emit a mapping from names to (mostly)
string values.

In this case the easiest grammar to use probably is the `reGrammar
<./ref.html#element-regrammar>`_..  The idea here is that you give two
regular expressions to separate the file into records and the records
into fields, and that you simply enumerate the names used in the
mapping.

For the file given above, the RE grammar definition could look like
this::

  <reGrammar topIgnoredLines="1">
    <names>raMin, raMax, decMin, decMax, EVI, AV, AI</names>
  </reGrammar>

If you checked the documentation on reGrammar, you will have noticed
that "names" is an "atomic child" of reGrammar. Atomic children are
usually written as attributes, since their values can always be
represented as strings.  However, if strings become larger, it's more
convenient to write them in elements.  The DC software allows you to
do just that in general: All attributes can be written as elements with
tags named like the attribute.  So,

::

  <reGrammar topIgnoredLines="1"
    names="raMin, raMax, decMin, decMax, EVI, AV, AI"/>

would have worked just fine, as would::

  <reGrammar><topIgnoredLines>1</topIgnoredLines>
    <names>raMin, raMax, decMin, decMax, EVI, AV, AI</names></reGrammar>

Structured children, in contrast, cannot be written as plain strings and
thus can only be written in element notation.

Though grammars can be direct children of resource, they are usually
written as children of data elements (see below).


Mapping data
------------

A grammar produces a sequence of mappings from names to strings, the
rawdicts.  The database, on the other hand, wants typed values, i.e.,
integers, time stamps, etc, internally represented as dictionaries
mapping column names to values called rowdicts.  Also, data in input
tables is frequently given in inconvenient formats (e.g., sexagesimal
angles), units not suited to further processing, or distributed over
multiple columns (e.g., date and time of an observation when we want a
single timestamp).  It is the job of row makers to transform the rough
data coming from a grammar to whatever the table defines.

Basically a row maker consists of

* `var <./ref.html#element-var>`_ s -- assignments of expression values 
  names in the rawdict.
* procedure applications (see `apply <./ref.html#element-apply>`_) --
  procedural manipulations of both rawdicts and rowdicts.
* maps -- rowdict definition.

When building a rowdict for ingestion into the database, a rowmaker first
binds var names, then applies procedures and finally runs the mappings.

For simple cases, maps will suffice; you may actually even be able to do
without them.  Maps must specify a dest attribute giving the rowdict key
that is defined.  To specify the value, the can 

* either give a src attribute specifying a rawdict key that will then be
  converted to a typed value using "sane" defaults (e.g., integers will
  be converted by python's int constructor, where empty strings are
  mapped to None)
* or give a python expression in the character content, the value of
  which is then directly used as value for dest.  No implicit
  conversions are performed.

In the case above, you could start by saying::

  <rowmaker id="build_exts">
    <map dest="EVI" src="EVI"/>
    <map dest="AV" src="AV"/>
    <map dest="AI" src="AI"/>
  </rowmaker>

to copy over the rawdict (grammar) keys that directly map to table
column names.  Since this is a bit unwieldy, the DC provides a
shortcut::
  
  <rowmaker id="build_exts">
    <simplemaps>EVI:EVI,AV:AV,AI:AI</simplemaps>
  </rowmaker>

which expands to exactly what is written above.  The keys in each pair do not 
need to be identical; the first item of each pair is the table column
name, the second the rawdict key.

The case where the names of rawdict and rowdict keys are identical is so
common (since the RD author controls both) that there is yet another
shortcut for this::

  <rowmaker id="build_exts">
    <idmaps>EVI,AV,AI</idmaps>
  </rowmaker>

Idmaps sets up one map element each with both dest and src set to the
value for every name in the comma separated list idmaps.

You can abbreviate this further to::

  <rowmaker idmaps="*"/>

idmaps values can contain shell patterns.  They will be matched to the
column names in the target table.  For every column for which there is
no explicit mapping, an identity mapping (with type conversion) will be
set up.

This leaves the bbox, centerAlpha, and centerDelta keys to be defined.
No literals for those appear in the rawdicts since they are not part of
the input data.  We need to compute them.

To facilitate computations, we first turn the bounds to floats; this can
be done using vars::

  <var name="raMin">float(raMin)</var>
  <var name="raMax">float(raMax)</var>
  <var name="decMin">float(decMin)</var>
  <var name="decMax">float(decMax)</var>

No shortcut is available here, since this is a relatively rare thing.
You could use procDef/apply to save on keystrokes if you find yourself
having to do such simple conversions more frequently.

As you can see, var elements have a name attribute that gives the name
in the rawdict the value is to be bound to.  Their character content is
a python expression in which you can access the rawdict values by their
names.

The remaining computations can be performed in mappings::

	<map dest="centerAlpha">(raMin+raMax)/2.</map>
	<map dest="centerDelta">(decMin+decMax)/2.</map>
	<map dest="bbox">coords.Box((raMin, decMin),
		(raMax, decMax))</map>

As in vars, the rawdict values can be accessed by their keys in the
mapping expressions.  coords.Box is the internal type for SQL Box
values; you will not usually see those.  Still, you can access basically
the whole DC code in this mapping.  At some point we will define an API
of "safe operations" that you can use without having to fear changes in
the DC code.


Data elements
-------------

We now have a table definition, a grammar, and a rowmaker.  For purposes
of importing, these three come together in a data element.  These
elements define what could be seen as the equivalent of a VOTable
resource together with a recipe of how to build it.  For onDisk tables,
a side effect of building the data is that the tables are created in the
database; in that sense, data elements also define operations, a notion
that will become more pronounced as we discuss incremental processing.

Let us assemble the pieces we have so far::

  <?xml version="1.0" encoding="iso-8859-1"?>

  <resource schema="lmcextinct">
    <meta name="title">Extinction within the LMC</meta>
    <meta name="creationDate">2009-06-02T08:42:00Z</meta>
    <meta name="description" format="plain">
      Extinction values in the area of the LMC...
    </meta>
    <meta name="copyright">Free to use.</meta>
    <meta name="creator.name">S. Author</meta>

    <meta name="subject">Large Magellanic Cloud</meta>
    <meta name="subject">Interstellar medium, nebulae</meta>
    <meta name="subject">Extinction</meta>

    <table id="exts" onDisk="True" adql="True">
      <meta name="description">
        Extinction values within certain areas on the sky.
      </meta>
      <column name="bbox" type="box"
        description="Bounding box for the extinction data"/>
      <column name="centerAlpha" unit="deg" tablehead="RA"
        description="Area center RA ICRS" ucd="pos.eq.ra;meta.main"/>
      <column name="centerDelta" unit="deg" tablehead="Dec"
        description="Area center Declination ICRS" ucd="pos.eq.dec;meta.main"/>
      <column name="ev_i" unit="mag" tablehead="E(V-I)"
        ucd="arith.diff;phys.absorption;em.opt.V;em.opt.I"
        description="Difference in extinction between V and I bands"/>
      <column name="a_v" unit="mag" tablehead="A_V"
        ucd="phys.absorption;em.opt.V"
        description="Extinction in V"/>
      <column name="a_i" unit="mag" tablehead="A_I"
        ucd="phys.absorption;em.opt.I"
        description="Extinction in I"/>
    </table>

    <data id="import">
      <sources pattern="data/*.txt"/>
      <reGrammar topIgnoredLines="1">
        <names>raMin, raMax, decMin, decMax, ev_i, a_v, a_i</names>
      </reGrammar>

      <rowmaker id="build_exts" idmaps="*">
        <var name="raMin">float(raMin)</var>
        <var name="raMax">float(raMax)</var>
        <var name="decMin">float(decMin)</var>
        <var name="decMax">float(decMax)</var>

        <map dest="centerAlpha">(raMin+raMax)/2.</map>
        <map dest="centerDelta">(decMin+decMax)/2.</map>
        <map dest="bbox">coords.Box((raMin, decMin),
              (raMax, decMax))</map>
      </rowmaker>
      
      <make table="exts" rowmaker="build_exts"/>
    </data>
  </resource>

As you can see, we have put the grammar and the rowmaker into a data
element.  While this is not exactly necessary (they could be direct
children of resource as well, which might be a good idea if they are
used in more than one data), this is good practice since they, in some
sense, belong to that data element.

There are two new elements in data.  For one, there's sources.  Sources
specify where the data will find its input files in its pattern
attribute.  This contains shell patterns that are interpreted relative
to the resource directory.  You can give multiple patterns if necessary
like this::

  <sources>
    <pattern>inp2/*.txt</pattern>
    <pattern>inp1/*.txt</pattern>
  </sources>

There also is a recurse boolean attribute you can use when your sources
are distributed over subdirectories of the path part of the pattern.


Indices and Mixins
------------------

Now, let's assume the input table is large.  You will want to define
indices on the table.  To do this, use the `index
<./ref.html#element-index>`_ element.  It is a child of table.  In
general, index specifications can be rather involved, but simple cases
remain simple.  If you just wanted to define an index on EVI, you could
say::


  <table id="exts" onDisk="True" adql="True">
    <index columns="EVI"/>
    ...

(the columns attribute would be "A_V,EVI" if you wanted an index on both
columns).

However, indices are not always that simple.  For example, for a spatial
index on centerAlpha, centerDelta, with the q3c scheme used by the DC
software you would have to write something like::

  <index columns="centerAlpha,centerDelta" cluster="True">
    q3c_ang2ipix(centerAlpha,centerDelta)
  </index>

The DC software has a mechanism that helps in this case: `Mixins
<./ref.html#mixins>`_.  A mixin conceptually is a guarantee of certain
table properties, typically of the presence of certain columns; here, it
is just the presence of an index.

So, all you need to do to have a spatial index on the table is::

  <table id="exts" onDisk="True" adql="True" mixin="q3cindex">
    ...

This is UCD magic at work -- q3cindex selects the columns with
pos.eq.*;meta.main as index columns.  If you are curious how it does
this, check scs.rd in the system RD directory.


Starting the Ingestion
----------------------

At this point, you can run the ingestion::

  gavoimp q

By default, gavoimp creates all data defined in a resource.  If this is
not what you want, you can explicitely specify a data id to process::

  gavoimp q import

For larger data sets, it may be wise to first try a couple of rows::

  gavoimp --stop-after=300 q


Try ``gavoimp --help`` to see more options (most of which are probably
irrelevant to you now.

Note that gavoimp interprets the RD argument as a file first and then as
an RD id.  An RD id is the inputs-relative path of the RD with the
extension stripped.  Our example RD thus has the RD id lmcextinct/q, and
you could have said::

  gavoimp lmcextinct/q

from anywhere in the file system.
  

Debugging
---------

If nothing else helps you can watch what the software actually sends to
the database.  To do that, set the GAVO_SQL_DEBUG environment variable
to any value.  This could look like this::

  env GAVO_SQL_DEBUG=1 gavoimp q create

The first couple of requests are for internal use (like checking that some
meta tables are present).


Publishing Data
===============

Once a table is in the database, it needs to get out again.  Within
DaCHS, there are three parties involved in delivering data to the user:

* The core; it actually does the computation
* The renderer; it formats the result in some way requested by the user
  and delivers it.  There are renderers for web forms, VO protocols,
  imges, etc.
* The serviice; it holds together the core and the renderer, can
  reformat core results, controls the metadata, etc.

You will usually use pre-specified renderers, so these are not defined
in resource descriptors.  What you have to define are cores and
services.

For core, you will usually use the `dbCore <./ref.html#element-dbcore>`_
in custom services, though `many other cores
<./ref.html#cores-available>`_ are predefined and you can `define your
own <./ref.html#writing-custom-cores>`_.  

The dbCore generates a (single-table) query from condition descriptors
and returns a table that you describe through an output table.  Cores
are defined as direct children of the resource.  For the lmcextinction
table above, it could look like this::
  
  <core id="conecore" queriedTable="exts">
    <condDesc predefined="humanScs"/>
    <condDesc buildFrom="ev_i"/>
  </core>

Cores always need an id.  dbCores need a queriedTable attribute, the
value of which must be a table reference.  This is the table the query
will run against.

CondDescs can be defined in all kinds of ways.  The most common modes,
however, are using predefined condDescs (which mostly come from
protocols; in this case, humanScs comes from SCS and lets you do cone
searches), and just deriving condDescs from table columns.  You can
refer to columns from your table definition by name in the buildFrom
attribute, and the software tries to make some useful input definition
from that column.

In web forms, these input definitions become form items; other renderers
will expose them differently.  In all cases, however, the condDescs of
the dbCore define what fields can be queried.

The service now ties the core together with a renderer.  It might look
like this::

	<service id="cone" core="conequery">
    <meta name="shortname">lmcext_web</meta>
	</service>

While services can run without shortnames, it can lead to trouble later, so you
should make a habit of assigning short names.  See `the data checklist
<./data_checklist.html>`_ for more information on short names.

A service must have an id as well, and its core attribute must contain
the id of a core.

With this minimal specification, the service exposes a web form-based
interface.  To try this, run a server::

  gavoserve debug

and point a browser to http://localhost:8080/lmcextinct/q/cone/form (the
host part, of course, depends on your configuration.  If you did not
change anything there, you should find the data at the given URL).


More on Grammars
================

Row Generators
--------------

TBD

Source Fields
-------------

Grammars can have a sourceFields element.  It contains a standard
procedure definition (i.e., you could predefine those and bind
parameters), but usually you will just fill in the code.

This code is called once for each source processed, and receives the
sourceToken as argument.  It must return a dictionary, the key/value
pairs of which will be added to all rows returned by the row iterator.

The purpose of sourceFields is to precompute values that depend on the
source ("file") and are constant for all rows within it.  An example for
where you need this is when you want to create backlinks to the file a
piece of data came from::

  <xygrammar>
    <sourceFields>
      <code>
        srcKey = utils.getRelativePath(sourceToken,
						base.getConfig("inputsDir"))
        return locals()
      </code>
    </sourceFields>
  </xygrammar>

You can then retrieve the path to the source file via srcKey key in
rawdicts (and then, using render functions and static renderers, turn
this into links).

In addition to the sourceToken, you also have access to the data that
will be fed from the grammar.  This can be used to, e.g., retrieve the
resource directory (``data.dd.rd.resdir``) or data descriptor properties
(``data.dd.getProperty("whatever")``).

Sometimes you want to do database queries from within sourceFields.
This is tricky when you access the table being written or otherwise
being accessed.  This is because sourceTokens run in the midst of a
transaction updating the table.  So, something like::

  <code> 
    <!-- will deadlock, don't do it like this -->
    base.SimpleQuerier().query(...)
  </code>

will wait for the transaction to finish.  But the transaction is waiting
for data that will only come when the query finishes -- this is a
deadlock, and gavoimp will just sit there and wait (see also `Deadlocks
<commonproblems.html#deadlocks>`_).

To get around this, you need to query using the data's connection.  So,
instead write::

  <code>
    base.SimpleQuerier(connection=data.connection).query(...)
  </code>


Preparing Data
==============

Sometimes you want to change something on the input files you are
receiving.  While usually we recommend coping with the input through
grammars, rowmakers, and the like since this helps maitaining
consistency with what the scientists intended and also stability when
new data arrives, there are cases when you deliver data to users,
most frequently, with FITS files.  There, you may need to add or change
headers.

However, sometimes you just want to traverse all sources.  Let's cover
this case first.


Processors
----------

The basic infrastructure for manipulating sources is the FileProcessor
class, available from gavo.helpers.

Here is an example checking whether the sizes of files match what an
(externally defined) function ``_getExpectedSize(fName) -> int`` returns::

  import os

  from gavo import api
  from gavo.helpers import FileProcessor, procmain

  class SizeChecker(FileProcessor):

    def process(self, srcName):
      found = os.path.getsize(srcName)
      expected = _getExpectedSize(srcName)
      if found!=expected:
        print "%s: is %s, should be %s"%(srcName, found, expected)


  if __name__=="__main__":
    procmain(SizeChecker, "potsdam/q", "import")


Do not forget the "from gavo import api" in this kind of script.  It
makes sure all protocols you may use in your RDs are loaded when the RD
is loaded.

More interesting is procmain.  It arranges for the command line to be
parsed and expects, in addition to the processor *class*, an id for 
the resource descriptor for the data it should process, and the id of
the data descriptor that ingests the files.

The processors can define command line options of their own.  You could,
for example, read the expected sizes from some sort of catalogue.  To do
that, define an addOptions static method, like this::

  class Processor(helpers.FileProcessor):
    @staticmethod 
    addOptions(optParser):
      helpers.FileProcessor.addOptions(optParser)
      optParser.add_option("--cat-name", help="Resdir-relative path to"
        " the plate catalogue", action="store", type="str", 
        dest="catPath", default="res/plates.cat")

Make sure you always do the upward call.  Cf. the optparse documentation
for what you can do.  The options object returned by optParser is
available as the opts attribute on your processor.  To keep the chance
of name clashes in this sort of inheritance low, always use long options
only.

Simple FileProcessors support the following options:

--filter
  It takes a value, a substring that has to be in the
  source's name for it to be processed.  This is for when you want to try
  out new code on just one file or a small subset of files.
--bail
  Rather than going on when a process method lets an exception escape,
  abort the processing at the first error and dump a traceback.  Use 
  this to figure out bugs in your (or our) code.

Once you have the catalogue name, you will want to read it and make it
available to the process method.  To allow you to do this, you can
override the _createAuxillaries(dd) method.  It receives the data
descriptor of the data to be processed.  Here's an example::

  class Processor(helpers.FileProcessor):
    def _createAuxillaries(self, dd):
      self.catEntriesUsed = 0
      catPath = os.path.join(dd.rd.resdir, self.opts.catPath)
      self.catalogue = {}
      for ln in open(catPath):
        id, val = ln.split()
        self.catalogue[id] = val


As you can see, you can access the options given on the command line
as self.opts here.

If you want your processor to gather data, you can use the fact that
procmain returns the processor it created.  Here is a version of the
simple size checker above that outputs a sorted list of bad files::

class SizeChecker(FileProcessor):

	def _createAuxillaries(self, dd):
		self.mess = []

	def process(self, srcName):
		found = os.path.getsize(srcName)
		expected = _getExpectedSize(srcName)
		if found!=expected:
			self.mess.append((srcName, expected, found))


if __name__=="__main__":
	res = procmain(SizeChecker, "potsdam/q", "import")
	res.mess.sort(key=lambda rec: abs(rec[1]-rec[2]))
	for name, expected, found in res.mess:
		print "%10d %10d %8d %s"%(expected, found, expected-found, name)


Basic FITS Manipulation
-----------------------

For manipulating FITS headers, there is the HeaderProcessor class.  It
is a FileProcessor, so everything said there applies here as well,
except that you do not want to override the process method.
Rather, you will probably want to override _mungeHeader(srcName, header) ->
header and _isProcessed(srcName) -> boolean methods.

_mungeHeader is supposed to return a new pyfits header for the file named
in the argument, _isProcessed must return True if you think the name
file already has your new headers, False otherwise.

_mungeHeader should in general raise a processing.CannotComputeHeader
exception if it cannot generate a header (e.g., missing catalogue entry,
nonsensical input data).  If you return None form _getHeader, a
generic CannotComputeHeader exception will be raised.

Note that you have to return a *complete* header, i.e., including all
cards you want to keep from the original header (but see 
`Header Selection`_).

A somewhat silly example could look like this::

  from gavo import api
  from gavo import helpers

  class SillyProcessor(helpers.FileProcessor):
    def _isProcessed(self, srcName):
      return self.getPrimaryHeader(srcName).has_key("NUMPIXELS")

    def _mungeHeader(self, srcName, hdr):
      hdr.update("NUMPIXELS") = hdr["NAXIS1"]*hdr["NAXIS2"]
      return hdr

  if __name__=="__main__":
    helpers.procmain(SillyProcessor, "testdata/theRD", "sillyData")

Processors are expected  to have an addOptions static method receiving
an optparser.OptionParser instance and adding options it wants to see.
Call --help on the program above to see FileProcessor's options.  Things
are arranged like this (check out the process and _makeCache methods in
the source code), where proc stands of the name of the ingesting program:

 * ``proc`` computes headers for all input files not yet having "cached"
   headers.  Cached headers live alongside the fits files and have
   ".hdr" attached to them.  The headers are *not* applied to the
   original files.
 * ``proc --apply --no-compute`` applies cached headers to the input
   files that do not yet have headers.  In particular when processing 
   is lengthy (e.g., astrometrical calibration), it is probably a good 
   idea to keep processing and header application a two-step process.
 * ``proc --apply`` in addition tries to compute header caches and
   applies them.  This could be the default operation when header
   computation is fast
 * ``proc --reprocess`` recreates caches (without this option, cached
   headers are never touched).  You want this option if you found a
   bug in your _getHeader method and need to to recompute all the
   headers.
 * ``proc --reheader --apply`` replaces processed headers on the source
   files.  This is necessary when you want to apply reprocessed headers.
   Without --reheader, to header that looks like it is "fixed"
   (according to your _isProcessed code) is ever touched.

Admittedly, this logic is a bit convolved, but the fine-grained
manipulation intensity is nice when your operations are expensive.

By default, files for which the processing code raises exceptions are
ignored; the number of files ignored is shown when procmain is finished.

If you want to run more than one processor over a given dataset, you
will have to override the headerExt class attribute of your processors
so all are distinct.  By default, the attribute contains ".hdr".
Without overriding it, your processors would overwrite the other's
cached headers.

By the way, if the original FITS header is badly broken or you don't
want to use it anyway, you can override the _getHeader(srcName) ->
header method.  Its default implementation is something like::

  def _getHeader(self, srcName):
    return self._mungeHeader(srcName, self.getPrimaryHeader(srcName))

The readPrimaryHeader(srcName) -> pyfits header method is a convenience
method of FITSProcessors with obvious functionality.



Header Selection
----------------

Due to the way pyfits manipulates header fields without data, certain
headers must be taken from the original file, overwriting values in the
cached headers.  These are the headers actually describing the data
format, available in the processor's keepKeys attribute.  Right now,
this is::

  keepKeys = set(["SIMPLE", "BITPIX", "NAXIS", "NAXIS1", "NAXIS2",
      "EXTEND", "BZERO", "BSCALE"])

You can amend this list as necessary.

Since these operations may mess up the sequence of header cards in a
way that violates the FITS standard, after this the new headers are
sorted.  This is done via fitstools.sortHeaders.  This function can take
two additional functions commentFilter and historyFilter, both receiving
the card value and returning True to keep the card and False to discard
it.  

Processors take these from like-named methods that you can override.
The default implementation keeps all comments and history items.  For
example, to nuke all comment cards not containing "IMPORTANT", you could
define::

  def commentFilter(self, comment):
    return "IMPORTANT" in comment


Astrometry.net
--------------

Calibration using Astrometry.net
''''''''''''''''''''''''''''''''

If you have uncalibrated optical images, you can try to
automatically calibrate them using astrometry.net.  The DC software
comes with an interface to it in helpers.anet, and the file processing
infrastructure is what you want to use here.

You probably want to inherit from AnetHeaderProcessor, more or less like
this::

  class MyProcessor(helpers.AnetHeaderProcessor):
    solverParameters = {
      "indices": ["index-208.fits"],
      "lower_pix": 0.1,
      "upper_pix": 0.2,
      "endob": 50,
     }

    def _getHeader(self, srcName):
      hdr = helpers.AnetHeaderProcessor._getHeader(self, srcName)
      if hdr is not None:
        hdr.update("LOCATION", "Unknown")
      return hdr


See helpers.anet for an explanation of what you can do with solver
parameters.  endob is important because it instructs anet to give up
when no identification has been possible within the first endob objects.
It keeps the solver from wasting enormous amounts of time on potentially
thousands of spurious detections, e.g., on photographic plates.

If you want to use SExtractor for source extraction, add a sexScript
class attribute.  An example could be::

  sexScript = """CATALOG_TYPE     FITS_1.0
    CATALOG_NAME     out.xyls
    PARAMETERS_NAME  xylist.param
    DETECT_TYPE      CCD
    DETECT_MINAREA   100
    DETECT_THRESH    8
    SEEING_FWHM      1.2
    #VERBOSE_TYPE     QUIET
    """

-- do not change CATALOG_TYPE, CATALOG_NAME, and PARAMETERS_NAME.

If you want to postprocess the output of the source extraction, define
an objectFilter method, maybe like this::

  from gavo.utils import pyfits
  ...

  def objectFilter(inName):
    """throws out funny-looking objects from inName and throws out objects
    near the border.
    """
    hdulist = pyfits.open(inName)
    data = hdulist[1].data
    width = max(data.field("X_IMAGE"))
    height = max(data.field("Y_IMAGE"))
    badBorder = 0.3
    data = data[data.field("ELONGATION")<1.2]
    data = data[data.field("X_IMAGE")>width*badBorder]
    data = data[data.field("X_IMAGE")<width-width*badBorder]
    data = data[data.field("Y_IMAGE")>height*badBorder]
    data = data[data.field("Y_IMAGE")<height-height*badBorder]
    hdu = pyfits.new_table(data)
    hdu.writeto("foo.xyls")
    hdulist.close()
    os.rename("foo.xyls", inName)

Note that we take pyfits from gavo.utils.  You should never import
pyfits directly, since this may pull in the numpy version of pyfits,
which is incompatible with what the rest of the DC software expects.

If you need more control over the parameters of astrometry.net, override
_solveAnet and call _runAnet;  here's an example::

  def _solveAnet(self, srcName):
    for minArea in [300, 50, 150, 800, 2000, 8000]:
      res = self._runAnet(srcName, self.solverParameters, 
        self.sexScript%minArea, self.objectFilter)
      if res is not None:
        return res

where the example sexScript above has been changed to have a %d for
DETECT_MINAREA.


Analyzing calibration failures
''''''''''''''''''''''''''''''

If astrometry.net fails to solve fields, you can get a copy of the
"sandbox" in which the helpers.anet runs the the software by passing
your processing script the --copy-to=path option. Caution: If
the directory path already exists, it will be removed.

If you run your processor with --bail, it will stop at the first
non-solvable field.  Going to anetcheck, you will find:

 * in.fits -- a copy of the input file
 * out.xyls -- the extracted source positions in a binary FITS table
 * out.log -- the solver log
 * blind.control -- the control file for the blind solver.

To figure out what's wrong, a look into out.log might help.

You can also change blind.control and re-run the solver using

::

  $ANET_PATH/blind < blind.control

To rerun SExtractor, say::

  sextractor -c anet.sex -FILTER N in.fits
  $ANET_PATH/tabsort MAG_ISO out.xyls out.fits

To get an idea what the source extraction has done, you can try anet's
plotxy.  You could use anet's solve-field, but this probably will not
reflect what is actually going on within the helper, in particular not if
sextractor is in use.

Instead, do something like::

  gm convert -flip -scale 6.25% in.fits pnm:- | $ANET_PATH/plotxy -I - -i out.xyls -C red -P -w 2  -S 0.0625 -X X_IMAGE -Y Y_IMAGE > ws.png

We use gm (from GraphicsMagick) here since netpbm's fitstopnm has issues with
large files.  You will want to use different scales for larger or
smaller images both in gm convert's scale and plotxy's -S option, i.e.,
maybe::

  gm convert -flip -scale 12.5% in.fits pnm:- | $ANET_PATH/plotxy -I - -i out.xyls -C red -P -w 2  -S 0.125 -X X_IMAGE -Y Y_IMAGE > ws.png

for smaller plates.


.. _`Resource Metadata for the Virtual Observatory`: http://www.ivoa.net/Documents/latest/RM.html
