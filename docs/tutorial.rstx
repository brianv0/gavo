=========================
GAVO DC Software Tutorial
=========================

.. contents:: 
  :depth: 2
  :backlinks: entry
  :class: toc


Ingesting Data
==============

Starting the RD
---------------

To ingest data, you will have to write a resource descriptor (RD).  We
recommend to keep everything handled by a specific RD together in on
directory that is a direct child of GAVODATA, though nothing forces you
to do that.  So, in GAVODATA, say::

  mkdir lmc

(where lmc would be the directory name you chose).  The directory name
will (normally) appear in URLs, so it's a good idea to choose something
descriptive and short.

We recommend to put the RD in the root of this directory.  A good
default name for the RD is "q.rd"; the "q" will appear in the default
URLs as well and usually looks good in there::

  cd lmc
  vi q.rd

(where you can substitute vi with your favourite editor, of course).
Writing resource descriptors is what most of the operation of a data
center is about.  Let's start slowly by giving some metadata::

  <?xml version="1.0" encoding="iso-8859-1"?>

  <resource schema="lmcextinct">
    <meta name="title">Extinction within the LMC</meta>
    <meta name="creationDate">2009-06-02T08:42:00Z</meta>
    <meta name="description" format="plain">
      Extinction values in the area of the LMC...
    </meta>
    <meta name="copyright">Free to use.</meta>
    <meta name="creator.name">S. Author</meta>

    <meta name="subject">Large Magellanic Cloud</meta>
    <meta name="subject">Interstellar medium, nebulae</meta>
    <meta name="subject">Extinction</meta>

  </resource>

You need to adapt the encoding attribute in the prefix to match what you
are actually using if you plan on using non-ASCII-characters.  
You may want to use utf-8 instead of the iso-8859-1 given below
depending on your computer's setup.

The schema attribute on resource gives the schema tables for this
resource will turn up in.  You should, in general, use the subdirectory
name.  If you don't, you have to give the subdirectory name in a
resdir attribute.

Otherwise, there is only meta information so far.  This metadata is
cruicial for later registration of the service.  In HTML forms, it is
displayed in a sidebar.

While you can add metadata for any key you like, only some "magic" keys
are treated specially by the DC software.  These primarily are the keys
defined by `Resource Metadata for the Virtual Observatory` by Robert
Hanisch.

Items you should definitely set include:

* title -- A terse phrase giving an idea what to expect.  Apart from
  other uses, this will be the head of HTML forms
* creationDate -- 


Parsing Input Data
------------------

We recommend putting input data files into a data subdirectory.  Put it
there.  Let's assume we have one input file for now, calle
lmc_extinction_values.txt.  Suppose it looks like this, where tabs in
the input are shown as "\\t"::

  RA_min\\tRA_max\\tDEC_min\\tDEC_max\\tE(V-I)\\tA_V\\tA_I
  78.910625\\t78.982146\\t-69.557417\\t-69.480639\\t0.04\\t0.092571\\t0.123429
  78.910625\\t78.982146\\t-69.480639\\t-69.403861\\t0.05\\t0.115714\\t0.154286
  78.910625\\t78.982146\\t-69.403861\\t-69.327083\\t0.05\\t0.115714\\t0.154286

The first step for ingestion is lexical analysis.  In the DC software,
this is performed by grammars.  There are many grammars defined, e.g.,
for getting values from FITS files, VOTables, or using column-based
formats.   All of them read "something" an emit a mapping from names to
(mostly) string values.

In this case the easiest grammar to use probably is the reGrammar.
The idea here is that you give two regular expressions to separate the
file into records and the records into fields, and that you simply
enumerate the names used in the mapping.

For the file given above, the RE grammar definition could look like
this::

  <reGrammar topIgnoredLines="1">
    <names>raMin, raMax, decMin, decMax, EVI, AV, AI</names>
  </reGrammar>

If you checked the documentation on reGrammar, you will have noticed
that "names" is an "atomic child" of reGrammar. Atomic children are
usually written as attributes, since their values can always be
represented as strings.  However, if strings become larger, it's more
convenient to write them in elements, the the DC software allows you to
do just that.  So,

::

  <reGrammar topIgnoredLines="1"
    names="raMin, raMax, decMin, decMax, EVI, AV, AI"/>

would have worked just fine, as would::

  <reGrammar><topIgnoredLines>1</topIgnoredLines>
    <names>raMin, raMax, decMin, decMax, EVI, AV, AI</names></reGrammar>

Structured children, in contrast, cannot be written as plain strings and
thus can only be written in element notation.

Defining Target Tables
----------------------

This data is to be represented in a database table using appropriate
metadata.  The metadata part is kept in a table structure in the RD.  In
this case, it could look like this::

	<table id="exts" onDisk="True">
		<meta name="description">
			Extinction values within certain areas on the sky.
		</meta>
		<column name="bbox" type="box" unit="deg"
			description="Bounding box for the extinction data"/>
		<column name="centerAlpha" unit="deg" tablehead="RA"
			description="Area center RA ICRS" ucd="pos.eq.ra;meta.main"/>
		<column name="centerDelta" unit="deg" tablehead="Dec"
			description="Area center Declination ICRS" ucd="pos.eq.dec;meta.main"/>
		<column name="ev_i" unit="mag" tablehead="E(V-I)"
			ucd="arith.diff;phys.absorption;em.opt.V;em.opt.I"
			description="Difference in extinction between V and I bands"/>
		<column name="a_v" unit="mag" tablehead="A_V"
			ucd="phys.absorption;em.opt.V"
			description="Extinction in V"/>
		<column name="a_i" unit="mag" tablehead="A_I"
			ucd="phys.absorption;em.opt.I"
			description="Extinction in I"/>
	</table>

To figure out "good" UCDs, the UCD resolver at
http://dc.zah.uni-heidelberg.de/ucds/ui/ui/form can help.

Mapping data
------------


Debugging
=========

If nothing else helps you can watch what the software actually sends to
the database.  To do that, set the GAVO_SQL_DEBUG environment variable
to any value.  This could look like this::

  env GAVO_SQL_DEBUG=1 gavoimp q create

The first couple of requests are for internal use (like checking that some
meta tables are present).


Preparing Data
==============

Sometimes you want to change something on the input files you are
receiving.  While usually we recommend coping with the input through
grammars, rowmakers, and the like since this helps maitaining
consistency with what the scientists intended and also stability when
new data arrives, there are cases when you deliver data to users,
most frequently, with FITS files.  There, you may need to add or change
headers.

Basic FITS Manipulation
-----------------------

To make this as painless and safe as possible, there is the processing
module in gavo.helpers.  For manipulating FITS headers, there is the
HeaderProcessor class.  To use it, derive from it and override at least
the _getHeader(srcName) -> header and _isProcessed(srcName) -> boolean
methods.

_getHeader is supposed to return a new pyfits header for the file named
in the argument, _isProcessed must return True if you think the name
file already has your new headers, False otherwise.

_getHeader should in general raise a processing.CannotComputeHeader
exception if it cannot generate a header (e.g., missing catalogue entry,
nonsensical input data).  If you return None form _getHeader, a
generic CannotComputeHeader exception will be raised.

Note that you have to return a *complete* header, i.e., including all
cards you want to keep from the original header (but see 
`Header Selection`_).

A somewhat silly example could look like this::

  from gavo import api
  from gavo import helpers

  class SillyProcessor(helpers.FileProcessor):
    def _isProcessed(self, srcName):
      return self.getPrimaryHeader(srcName).has_key("NUMPIXELS")

    def _getHeader(self, srcName):
      hdr = self.getPrimaryHeader(srcName)
      hdr.update("NUMPIXELS") = hdr["NAXIS1"]*hdr["NAXIS2"]
      return hdr

  if __name__=="__main__":
    helpers.procmain(SillyProcessor, "testdata/theRD", "sillyData")

Do not forget the "from gavo import api" in this kind of script.  It
makes sure all protocols you may use in your RDs are loaded when the RD
is loaded.

The getPrimaryHeader(srcName) -> pyfits header method is a convenience
method of FileProcessors with obvious functionality.

More interesting is procmain.  It arranges for the command line to be
parsed and expects, in addition to the processor *class*, an id for 
the resource descriptor for the data it should process, and the id of
the data descriptor that ingests the files.

Processors are expected  to have an addOptions static method receiving
an optparser.OptionParser instance and adding options it wants to see.
Call --help on the program above to see FileProcessor's options.  Things
are arranged like this (check out the process and _makeCache methods in
the source code), where proc stands of the name of the ingesting program:

 * ``proc`` computes headers for all input files not yet having "cached"
   headers.  Cached headers live alongside the fits files and have
   ".hdr" attached to them.  The headers are *not* applied to the
   original files.
 * ``proc --apply --no-compute`` applies cached headers to the input
   files that do not yet have headers.  In particular when processing 
   is lengthy (e.g., astrometrical calibration), it is probably a good 
   idea to keep processing and header application a two-step process.
 * ``proc --apply`` in addition tries to compute header caches and
    applies them.  This could be the default operation when header
    computation is fast
 * ``proc --reprocess`` recreates caches (without this option, cached
   headers are never touched).  You want this option if you found a
   bug in your _getHeader method and need to to recompute all the
   headers.
 * ``proc --reheader --apply`` replaces processed headers on the source
   files.  This is necessary when you want to apply reprocessed headers.
   Without --reheader, to header that looks like it is "fixed"
   (according to your _isProcessed code) is ever touched.

Admittedly, this logic is a bit convolved, but the fine-grained
manipulation intensity is nice when your operations are expensive.

By default, files for which the processing code raises exceptions are
ignored; the number of files ignored is shown when procmain is finished.

For debugging, pass a --bail option.  It will abort the processing at
the first error, dumping a traceback.



Header Selection
----------------

Due to the way pyfits manipulates header fields without data, certain
headers must be taken from the original file, overwriting values in the
cached headers.  These are the headers actually describing the data
format, available in the processor's keepKeys attribute.  Right now,
this is::

	keepKeys = set(["SIMPLE", "BITPIX", "NAXIS", "NAXIS1", "NAXIS2",
			"EXTEND", "BZERO", "BSCALE"])

You can amend this list as necessary.

Since these operations may mess up the sequence of header cards in a
way that violates the FITS standard, after this the new headers are
sorted.  This is done via fitstools.sortHeaders.  This function can take
two additional functions commentFilter and historyFilter, both receiving
the card value and returning True to keep the card and False to discard
it.  

Processors take these from like-named methods that you can override.
The default implementation keeps all comments and history items.  For
example, to nuke all comment cards not containing "IMPORTANT", you could
define::

  def commentFilter(self, comment):
    return "IMPORTANT" in comment


Advanced FITS manipulation
--------------------------

It is quite common that additional information is required to fix
header, e.g., from a plate catalogue.  To have those available in your
processor, add keyword arguments to the constructor and pass them in
the procmain code, like this::

  class Processor(helpers.FileProcessor):
    def __init__(self, opts, dd, catalogue=None):
      helpers.FileProcessor.__init__(opts, dd)
      self.catalogue = catalogue
      self.catEntriesUsed = 0
    ...
    def _getHeader(self, srcName):
      ...
      self.catEntriesUsed += 1

  proc = procmain(Processor, "foo", "bar", catalogue=_loatCat())
  print "Catalogue entries used:", proc.catEntriesUsed

This fragment also shows how to use the fact that procmain returns the
processor it instantiated, to return information from the processor to
the controlling program.

The next customization step is to add options.  To let the user specify
the catalogue name, you could do something like this::

  class Processer(helpers.FileProcessor):
    @staticmethod 
    addOptions(optParser):
      helpers.FileProcessor.addOptions(optParser)
      optParser.add_option("--cat-name", help="Resdir-relative path to"
        " the plate catalogue", action="store", type="str", 
        dest="catPath", default="res/plates.cat")

Make sure you always do the upward call.  Cf. the optparse documentation
for what you can do.  The options object returned by optParser is
available as the opts attribute on your processor.


Astrometry.net
--------------

Calibration using Astrometry.net
''''''''''''''''''''''''''''''''

If you have uncalibrated optical images, you can try to
automatically calibrate them using astrometry.net.  The DC software
comes with an interface to it in helpers.anet, and the file processing
infrastructure is what you want to use here.

You probably want to inherit from AnetHeaderProcessor, more or less like
this::

  class MyProcessor(helpers.AnetHeaderProcessor):
    solverParameters = {
      "indices": ["index-208.fits"],
      "lower_pix": 0.1,
      "upper_pix": 0.2,
     }

    def _getHeader(self, srcName):
      hdr = helpers.AnetHeaderProcessor._getHeader(self, srcName)
      if hdr is not None:
        hdr.update("LOCATION", "Unknown")
      return hdr


See helpers.anet for an explanation of what you can do with solver
parameters.

If you want to use SExtractor for source extraction, add a sexScript
class attribute.  An example could be::

  sexScript = """CATALOG_TYPE     FITS_1.0
    CATALOG_NAME     out.xyls
    PARAMETERS_NAME  xylist.param
    DETECT_TYPE      CCD
    DETECT_MINAREA   100
    DETECT_THRESH    8
    SEEING_FWHM      1.2
    #VERBOSE_TYPE     QUIET
    """

-- do not change CATALOG_TYPE, CATALOG_NAME, and PARAMETERS_NAME.

If you want to postprocess the output of the source extraction, define
an objectFilter method, maybe like this::

from gavo.utils import pyfits
...

  def objectFilter(inName):
    """throws out funny-looking objects from inName and throws out objects
    near the border.
    """
    hdulist = pyfits.open(inName)
    data = hdulist[1].data
    width = max(data.field("X_IMAGE"))
    height = max(data.field("Y_IMAGE"))
    badBorder = 0.3
    data = data[data.field("ELONGATION")<1.2]
    data = data[data.field("X_IMAGE")>width*badBorder]
    data = data[data.field("X_IMAGE")<width-width*badBorder]
    data = data[data.field("Y_IMAGE")>height*badBorder]
    data = data[data.field("Y_IMAGE")<height-height*badBorder]
    hdu = pyfits.BinTableHDU(data)
    hdu.writeto("foo.xyls")
    hdulist.close()
    os.rename("foo.xyls", inName)

Note that we take pyfits from gavo.utils.  You should never import
pyfits directly, since this may pull in the numpy version of pyfits,
which is incompatible with what the rest of the DC software expects.

If you need more control over the parameters of astrometry.net, override
_solveAnet and call _runAnet;  here's an example::

	def _solveAnet(self, srcName):
		for minArea in [300, 50, 150, 800, 2000, 8000]:
			res = self._runAnet(srcName, self.solverParameters, 
				self.sexScript%minArea, self.objectFilter)
			if res is not None:
				return res

where the example sexScript above has been changed to have a %d for
DETECT_MINAREA.


Analyzing calibration failures
''''''''''''''''''''''''''''''

If astrometry.net fails to solve fields, you can get a copy of the
"sandbox" in which the helpers.anet runs the the software by passing
your processing script the --copy-to=path option. Caution: If
the directory path already exists, it will be removed.

If you run your processor with --bail, it will stop at the first
non-solvable field.  Going to anetcheck, you will find:

 * in.fits -- a copy of the input file
 * out.xyls -- the extracted source positions in a binary FITS table
 * out.log -- the solver log
 * blind.control -- the control file for the blind solver.

To figure out what's wrong, a look into out.log might help.

You can also change blind.control and re-run the solver using

::

  $ANET_PATH/blind < blind.control

To rerun SExtractor, say::

  sextractor -c anet.sex -FILTER N in.fits
  $ANET_PATH/tabsort MAG_ISO out.xyls out.fits

To get an idea what the source extraction has done, you can try anet's
plotxy.  You could use anet's solve-field, but this probably will not
reflect what is actually going on within th helper, in particular not if
sextractor is in use.

Instead, do something like::

  gm convert -flip -scale 6.25% in.fits pnm:- | plotxy -I - -i out.xyls -C red -P -w 2  -S 0.0625 -X X_IMAGE -Y Y_IMAGE > ws.png

We use gm (from GraphicsMagick) here since netpbm's fitstopnm has issues with
large files.  You will want to use different scales for larger or
smaller images both in gm convert's scale and plotxy's -S option, i.e.,
maybe::

  gm convert -flip -scale 12.5% in.fits pnm:- | plotxy -I - -i out.xyls -C red -P -w 2  -S 0.125 -X X_IMAGE -Y Y_IMAGE > ws.png

for smaller plates.
.. _`Resource Metadata for the Virtual Observatory`: http://www.ivoa.net/Documents/latest/RM.html
