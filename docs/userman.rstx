==================================
GAVO python software user's manual
==================================

python-gavo is a package containing support code and a couple of entry
points (i.e., user-visible scripts) for building and running servers
with astronomical data ("VO nodes").

Installation
============

The preferred way to run python-gavo is on a dpkg-based system (apt-get
yaddayadda -- actually, packaging and setting up a repository still
needs to be done).

If this is not an option for you, you can install python-gavo based on
easy_install.  You first need a working python installation that can
build python extensions.  If you installed python yourself and have a C
compiler, that should already be the case, on some distributions you
need packages like python-dev, python-devel or somesuch installed in
addition to the binary python.  Once you have that, get the source
tar.gz, unpack it somewhere, cd into resulting directory and then say

::

  sudo python setup.py install

(there are various options to get the stuff installed when you're not
root; refer to the `setuptools
documentation <http://peak.telecommunity.com/DevCenter/EasyInstall>`_ if
necessary).

Q3C
***

If at some point you have large data sets (more than 20000 rows, say)
and you want to do positional searches on them (cone search, crossmatch,
siap and such), you will want the Q3C library by Sergey Koposov et al,
http://www.sai.msu.su/~megera/oddmuse/index.cgi/SkyPixelization.  It's
not particularly tricky to install, but you should skip it for now.

To install it, get the source from the web site given above.  You need a
couple of dependencies (on debian etch systems, that would at least be
libpam-dev, libreadline-dev, and postgresql-server-dev-8.1).  Then
follow the instructions in the q3c distribution.

To automatically create the necessary indices, use the `q3cpositions`_ interface.

Setup
=====

The tools want a "root directory" for GAVO, a directory in which the
sources reside, ancillary data is kept, etc.  By default, this is
``/var/gavo``.  If you want something else, set the environment variable
``$GAVO_HOME``.

Static Files
************

Within that directory, you need some data files: static files for the
web server, interface definitions, etc.  In an svn checkout, you can go
to the resources subdirectory, say "make install" (while having
GAVO_HOME defined if you don't like /var/gavo) and be done if you have
rsync (you should, it's a fine and useful piece of software).  Otherwise,
we provide a tar file of the necessary data that you can just untar in
$GAVO_HOME.

Logging
*******

The tools will write a log (by default ``$GAVO_HOME/logs/gavoops``).  It
may be a good idea to keep that log on a local file system and not share
it with other instances, not the least because we do not provide for any
kind of locking on that file.  To do that, set the environment variable
GAVO_LOGDIR to a directory you want the logs in.  That directory should,
of course, only be writable by you.

If you insist, it wouldn't be hard to make the tools log to syslog.

Database Setup
**************

To make things work, you will also need to set up a database and create
at least one Settings_ file.

Currently, we only support postgres.  You'll need a postgres
installation.  You should have one user that may create tables and even
schemas, so you probably need a dedicated database.

We can't help you much with postgres (if you have an administrator, she
should know how to set a database and the necessary users for you), but 
with a more or less usual installation something like the following should 
work:

::

  sudo -u postgres createdb gavo  # creates the database that'll hold your data
  # create the user that feeds the db...
  sudo -u postgres createuser -P -ADS gavo-admin
  # and a user that usually has no write privileges
  sudo -u postgres createuser -P -ADS gavo

Note down both passwords, you will need them when setting up your
gavosettings files.

Note that running a database server always is a potential security
liability.  You should make sure you understand what your pg_hba.conf
says.

Settings
========

Most operations will eventually lead to database accesses.  The tools
will get access information from a gavosettings file.  By default, it is
searched in ``$HOME/.gavosettings`` (or ``/no_home/.gavosettings`` if
``HOME`` is not defined).  You can override the path to this file via
the environment variable GAVOSETTINGS.

This file contains ini-style configuration, i.e., sections (in square
brackets) or key-value pairs.  Currently, there is only one section, db.
Within that section, you can set

* dsn -- the "path" to the database, in the format <host>:<port>:<db>.
  Example: ``computer.doma.in:5432:test``
* user -- the user through which the db is accessed.  Example:
  ``me``
* password -- the password of user.  Example: ``mySecretPassword``
* allRoles -- a comma seperated list of database user ids that will get
  all privileges on newly created schemas and tables.  Example:
  ``me, admin1, devel``.
* readRoles -- a comma seperated list of database user ids that will
  get usage/read access on newly created schemas and tables.  Example:
  ``gavo,op0``

A gavosettings file that sets all mentioned parameters would then look
like this:

::

  [db]
  dsn: computer.doma.in:5432:test
  user: me
  password: mySecretPassword
  allRoles: me, admin1, devel
  readRoles: gavo,op0

If you provide web services, you should at least have one non-privileged
account (gavo in this case).  Web services should always use a
gavosettings-File of their own, logging them in as that non-privileged
user.

If you followed the instructions above, your private .gavosettings file
should look like this:

::

  [db]
  dsn: localhost:5432:gavo
  user: gavo-admin
  password: (whatever you've noted above)
  allRoles: gavo-admin
  readRoles: gavo

You'll need a gavosettings file for the querulator later; you might put
it under $GAVO_HOME/web/gavosettings and make it readable to the
webserver and your group only.  If your web server runs on the machine
with the database (which is recommended until you know your way around), 
it would look like this:

::

  [db]
  dsn: localhost:5432:gavo
  user: gavo
  password: (whatever you've noted above)

Note that you don't need any role specifications here since the
querulator will never write to the database.

Importing a resource
====================

To import a resource into the database, you should first make a
subdirectory of your gavo inputs directory (GAVO_HOME/inputs), the
resource directory.  The
recommended layout within this directory is:

* res/ -- a directory containing the resource descriptor plus
  additional metadata (like mapping files), if applicable
* bin/ -- if applicable, a directory containing code that may be needed
  for preprocessing and the like.  Use with care and provide explanatory
  make targets.
* <anything convenient>/ -- directory containing input source file(s)
* Makefile -- a makefile with at least one target, update, that
  rebuilds the tables generated by this resoure (details pending).
* README -- a short description of the source: what it is, where it came
  from, etc.

After unpacking the raw data, you need to create a resource
descriptor (see `Resource Descriptors`_).  Once you have that, call

::

  gavoimp <path-to-rd>

do start parsing and uploading the data.

You can give gavoimp a couple of options, mostly to aid in debugging; 
see ``gavoimp --help``.

Resource Descriptors
====================

The resource descriptors (suggested extension: .vord, for VO resource
descriptor) are XML files speficying both the syntax and the semantics
of a source.

Since they have to express quite a bit, they're a a little cumbersome.
We may want to write wrappers for standard cases.  Let's see.

The main source of information on the gory details of RDs should be the
schema file which should be lavishly commented (ahem).  The following is
meant as  a high-level introduction, but also covers topics that don't
really fit into the schema file.  In short: You'll need to inspect both
for the full story.  Start working with examples :-)

The root element defines the directory most other paths are relative to,
like this:

::

  <ResourceDescriptor srcdir="&master-name;/raw">

(it's usually a good idea to define a couple of entities in RDs,
master-name being one: that should be the name of your resource
directory).

Roughly, an RD mainly consists of one or more data sections.  A data
section describes one source (or set of sources) with a homogenous
structure (i.e., they share a "common data model").   

A Data section contains a grammar element describing how to parse the
input file in terms of preterminals and literals, and a Semantics
element that explains how to interpret the preterminals from the Grammar
section. The term "preterminal" here corresponds to nonterminal symbols
that directly expand into the terminal data, i.e., they give names to
certain sequences of terminal tokens.

The mappings from preterminals to token sequences (actually, strings)
are called rowdicts in the document.  The semantics describe how these
rowdicts are transformed into records.  These map names (column names
for data bases, field names for VOTables) to parsed values -- rowdicts
only have strings as values, records contain ints, floats, DateTime
objects, strings, whatever.

Grammars
********

Depending on what the input is, we want different types of grammars.
Thus, there are various grammar elements, but each Data element only has
exactly one of them.  Currently defined are:

* NullGrammar -- when you don't want to do any parsing at all.  You'll
  want that if you just need to define a table through a resource
  descriptor.  The product table (see `Products`_) is an 
  example of why this is necessary: No single resource can "own" it, but 
  something needs to own it.
* CFGrammar -- when you need context free grammars.  This is based on
  pyparsing and is currently unmaintained, mainly because I have no
  source that actually needs it.
* REGrammar -- when regular expressions plus a bit will do.  These are
  what you'll want to use for text tables.  Basically, you specify some
  way to tokenize your input and REs for the individual tokens.
* KeyValueGrammar -- used for files that give key-value pairs, one file
  per row.
* ColumnGrammar -- used for ASCII tables, somewhat analogous to the
  byte-by-byte descriptions Vizier uses.
* FitsGrammar -- used for fits files.  Currently, only the fields of the
  primary header are used, but once I have fits tables, it shouldn't be
  hard to retrofit access to other headers.

All grammars support macros (see Macros_).

ColumnGrammars
..............

ColumnGrammars expose preterminals as ranges of character indices, where
counting starts with 1 and ranges are inclusive.  You can also give a
single number, and you'll get a character.  The basic unit is always a
single physical line.

ColumnGrammars have the following attributes:

* topIgnoredLines -- skip that many lines at the top (for table headings
  and similar detritus.
* booster -- this allows using a C program to stand in for the grammar.
  This is only interesting for large data sets.  There's a perliminary
  implementation for these, but it's still very rough.

Note that ColumnGrammars support symbolic names if you give them in
Macros.  If everything in a preterminal is whitespace, a NULL value is
returned.

As an example, consider

::

  <ColumnGrammar topIgnoredLines="1">
    <Macro name="linearMap" destination="mag" factor="1e-3">
      <arg name="val" source="14-18"/>
    </Macro>
  </ColumnGrammar>

A file containing

::

  12345678901234567890
  x-55 55.7689 34233 a
  x-43 43.3483       b
  
would then produce two records.  Within the first, the preterminal
``mag`` would have the value 34.233, the preterminal 1-4 would be "X-55"
the preterminal 20 would be a, etc.  In the second record, ``mag`` would
be ``NULL``, as would be 13-14, 15, etc., because all of them only
contain whitespace.

FitsGrammars
............

These grammars expose the primary FITS header keywords as preterminals.
Contrary to other grammars, the values of the preterminals are not
always strings, which may cause trouble when applying macros.  This
could be solved using type conversion macros if necessary.

FitsGrammars have one attribute

* qnd -- if True, a quick-and-dirty header parser will be used.  It
  only works for the primary header but is much faster for
  gzipped FITS files.  If your FITS files are not compressed, don't
  bother with it.

FitsGrammars currently do not support fits tables or the like.  If they
do at some point, the header keywords will be available in the document
record when not in docIsRow mode (in other words, FitsGrammars only make
sense in docIsRow mode at the moment).

KeyValueGrammars
................

These grammars parse files organized in key-value pairs, i.e., files
that somewhat look like this:

::

  field1: bla
  field2= 23  # or something

KeyValueGrammars have the following attributes:

* kvSeperators -- a string containing the admissable seperator
  characters between key and value (default: colon and equals sign).
  Note that whitespace around key and value is ignored.
* pairSeperators -- a string containing characters that seperate
  individual key-value pairs (default: Linefeed)
* commentPattern -- a regular expression defining comments (default:
  ``"(?m)#.$*"``)

Note that KeyValueGrammars currently only work in docIsRow mode.  It is
unclear if and how they will ever support multiple rows in one file.

CFGrammars
..........

Don't use these for now.  If you think you need context free grammars to
parse your source, contact the author.

REGrammars
..........

Don't use these for now.  They work, but I don't like the way they do.
If you really need them, contact the the author, and we'll try to figure out a
good way of describing them.

We currently have three types of tokenizers (given in the type attribute of the element):

* split -- the content matches a separator between the fields (each row is processed by re.split()
* match -- the content matches an entire row, the fields being a groups defined. This probably is convenient with the {}-notation: (.{4}) (.{8}) (.{3}) would match a table with 4, 8, and 3 wide fields, separated by single spaces. Since this is so common, we'll probably provide an abbreviated syntax for this.
* colranges -- the content is a whitespace-seperated list of columns or
  column ranges (#-style comments allowed).  Example: 1 4-5 10-20 produces
  three tokens, one from column 1 (the start of the string), one
  consisting of the characters at positions 4 and 5 (note the difference
  to python slice notation: the end of the range is *included*), and one
  of length 11, spanning columns 10 through 20.

Macros
******

Unfortunately, the input data isn't always in the format we'd like it to
have.  Some of these issues can be solved by giving literalForms (see
Semantics_) and the like, but frequently manipulations involve more than
one token or require.  To get a halfway declarative way of defining
these, macros are used.  Macros take a set of arguments and work on
rowdicts.  Their effects are reflected in rowdicts.

To understand macros, let's look at two examples:

::

      <Macro name="mapValue" sourceName="maidanak/res/maydanak_targets.txt">
        <arg name="value" source="OBJECT"/>
        <arg name="destination" value="ARI_OBJC"/>
      </Macro>

Here, we specify that a macro called mapValue is to be applied to the
rowdicts.  This specific macro needs a file specifying the mapping,
which is given as a "construction argument" sourceName.

During the application of the macro, it takes two arguments, the value
to be mapped and a destination field.  Macro arguments can have two
sources -- either a literal, were the value given is directly passed to
the macro (the argument is the same for all rowdicts), or the value of a
preterminal.  In the former case, you use the value attribute, in the
latter, the source.  Thus, the macro above always has ARI_OBJC as value
for its destination argument, whereas the value argument is taken from
the OBJECT field of the rowdict.

::

      <Macro name="handleEquatorialPosition">
        <arg name="alpha" source="ALPHA"/>
        <arg name="delta" source="DELTA"/>
      </Macro>

This specification says: "Take the values of ALPHA and DELTA in the
rowdict and pass them as arguments alpha and delta to the macro
handleEquatorialPosition".  As a result in this case, you'll have new fields
alphaFloat, deltaFloat, c_x, c_y, c_z, and htmid in the rowdict -- this
is not visible from the macro definiton but is hardcoded in the macro.

Macros live in gavo/parsing/macros.py.  The following is documentation
generated from there.  To figure out what arguments to give in XML,
look at the constructors in the test cases.  Their second argument gives
the arguments in triples, where the first item is the argument name, the
second is a field name (attribute "source"), and the last is a constant
(attribute "value").  So, if the constructor is called like this:

::

    >>> r = ReSubstitutor(None, [("destination", "", "fixed"),
  ... ("data", "broken", ""), ("srcRe", "", r"(.) \(([^)]*)\)"),
  ... ("destRe", "", r"\2 \1")])

the corresponding XML would be

::

  <Macro name="subsRe">
    <arg name="destination" value="fixed"/>
    <arg name="data" source="broken"/>
    <arg name="srcRe" value="(.) \(([^)]*)\)"/>
    <arg name="destRe" value="\2 \1"/>
  </Macro>

Currently defined macros are:

.. !!python ../gavo/parsing/macros.py docs


Constraints
***********

XXX TBD

Semantics
*********

The semantics contain one or more record defintions, where each of these
mainly consists of field definitions.

Field Definitions
.................

The field definition defines a single field within a table. It is
described by the following attributes (only the ones marked with [*] are
strictly necessary):

* dest -- a name that is used as an identifier for this column (should be an SQL identifier) [*]
* source -- the source from which to fill the column. This can be a
  simple name, which is then used as the preterminal the field is fed
  from. If the value of source starts with an @, the rest is interpreted
  as the name of a field computer, which is called when the value of the
  field is required (see `At Expansions`_).
* dbtype -- the type of the value. We use the sql type system and map it to the others involved (python, votable). Defaults to real.
* default -- a literal used as a default value.
* unit -- the unit of the value, given as given in http://vizier.u-strasbg.fr/doc/catstd-3.2.htx, defaults to empty.
* optional -- if true (in any capitalization), empty values (Python: None, SQL: NULL) are allowed for this field, defaults to True.
* ucd -- a description of the field content, as per http://www.ivoa.net/Documents/latest/UCDlist.html.
* description -- human-digestable text that describes the contents of this field.
* nullvalue -- a literal that is interpreted as SQL NULL.
* literalForm -- an identifier that tells ImportTools's type converter
  about special literal forms (e.g. hour angle for float, weird date
  formats). These identifiers are discussed below
  Probably the most important one is "do not touch" which instructs the
  type converter to leave the value alone.  This is important when,
  e.g., macros or field computers already deliver python types as
  values.  These typically do not appreciate being treated as strings.
* verbLevel -- an integer, usually between 0 and 30, giving the
  "importance" of the column for VOTables and the like.  Protocols like
  Simple Cone Search have parameters like ``VERB``, where ``VERB=1``
  requests a maximally terse output, ``VERB=3`` a maximally verbose
  output.  In that spirit, columns are usually only included in VOTables
  when their ``verbLevel<=VERB*10``.  Similar rules will apply to
  protocols other than SCS.  The default is 30, which means that
  the column is only included in the most verbose VOTables.
* references -- this is used in SQL data definition and follows the SQL
  syntax.
* primary -- declares this field as belonging to the primary key.

Literal Forms
.............

In addition to the special "do not touch" discussed above, literal forms
avaliable for a Field's literalForm attribute include:

.. !!python ../gavo/parsing/typeconversion.py docs "'"

At Expansions
.............

In particular values introspecting on the parsing process (like finding
out the name of the source file) are handled through at expansions. The
reason there aren't macros for the things mentioned here is a technical
one -- macros shouldn't need to know the things the at-expansions
insert.   At expansions can be used in the value attributes of Fields
and Macros.

To use an at expansion, simply say something like <Field
value="@someexp"/>, i.e. a value with an @ in front.  If you really ever
need a literal @ in front of a "real" value, type the @ twice.

The following at expansions are known:

.. !!python ../gavo/parsing/importparser.py docs "'"

Shared Tables
.............

In general, if a resource has a Record element with a table name in it,
it will assume it "owns" this table, i.e., it will manipulate the table
as it sees fit, up to and including dropping it.  This is not desirable
if a table contains material from more than one resource.  Though it's
frequently a good idea to solve this thorough a view-only resource as
discussed in the section on `Dependent Resources`_, this would be at least
inconvenient if it is not known in advance what resources the table
material may come from.  The classic example is the product table.

For cases like these, you can have Shared Tables.  These are defined
using the SharedRecord element.  This works like the usual RecordDef in
that it contains Field elements, but differs in two aspects:

- They will never create a table and never drop it
- They require an owning condition enabling the importer to know which
  items it should delete when recreating the table.

Owning conditions
'''''''''''''''''

The onwingCondition element has two attributes.  The first, colName,
gives the name of a column to match in the shared table, the second,
value, the value this column must have to mark it as generated by the
resource.  The importer will delete all rows matching this owning
condition from the shared table when it re-imports the resource.  For
example, an owning condition like

::

  <owningCondition colName="sourceTable" value="foo.bar"/>

translate into SQL like

::

  DELETE * from <shared table> WHERE sourceTable='foo.bar'

Dealing with shared tables
''''''''''''''''''''''''''

Each shared table needs an owning resource.  As a rule, that owning
resource will not enter any data of its own.  Here is the resource
definition for the product table as an example:

::

.. !!sed -e 's/^/  /' ../resources/inputs/products/products.vord

Since the record definition of a shared table is present in all
resource descriptors contributing to the table, it is likely
that you want to define a new interface for whatever you want to do.  
It would then take care of keeping the definitions in sync.
However, you need to change the python source to do that.  If you don't
want to do that, you can get by using entities.  If you want to go that
way, contact the authors, and we'll tell you how to go about it.


Dependent Resources
*******************

It is not an uncommon situation that resource A needs to be updated when
resource B changes.  When that happens, you can use the recreateAfter
element, which is expected as a direct child of ResourceDescriptor.  
It has one attribute, the name of a project (which is the name of the
directory relative to gavo.inputsDir).  This may look like this.

::

  <recreateAfter project="lensdemo"/>

With this specification, the importer will, after finishing its work on
the resource, enter the named directory and call "make" there.  If you
follow the recommended setup, this will cause the recreation of the
named resource's table(s).

As an example, supposed we have various tables having quite a few
fields in common without warranting an interface of their own.  You may
want to create a view over them.  To do this, write a resource
descriptor like

::

  <?xml version="1.0" encoding="utf-8"?>

  <ResourceDescriptor srcdir="lensdemo">
    <schema>views</schema>
      <Semantics>
        <Record table="lenses2" create="False">
          <Field dest="date" dbtype="date" 
            tablehead="Creation Date" ucd="time.creation"/>
          <Field dest="date_obs" dbtype="date" 
            tablehead="Obs. Date"/>
          <Field dest="type" dbtype="varchar(10)" 
            optional="false" nullvalue="None" tablehead="Obs. Type" 
            ucd="meta.code.class" 
            description="Type of observation (science, flat, bias, calib...)"/>
          <Field dest="object" dbtype="text" 
            nullvalue="" tablehead="Target Object" 
            description="Object being observed, Simbad-resolvable form"
            ucd="meta.id;meta.main"/>
          <Field dest="raw_object" source="OBJECT" dbtype="text" 
            tablehead="Target Object (Header)" description=
            "Object being observed, from fits header (unreliable)"/>
          <Field dest="telescope" source="TELESCOP" dbtype="text" 
            tablehead="Telescope" ucd="instr.tel"/>
          <Field dest="startTime" source="startTime" dbtype="timestamp"
            literalForm="do not touch" tablehead="UT of obs. start"
            ucd="time.start" unit="yr"/>
          <Field dest="filter" source="FILTER" dbtype="text" 
            tablehead="Filter" ucd="instr.filter"/>
          <Field dest="exposure" source="EXPTIME" dbtype="float" 
            tablehead="Exp. time" ucd="time.interval;obs.exposure"
            unit="s"/>
          <implements name="products"/>
        </Record>
      </Semantics>
    </Data>

    <script type="postCreation">
      -DROP VIEW views.lenses2
      CREATE VIEW views.lenses2 AS ((SELECT date, date_obs, type, object, \
        raw_object, datapath, telescope, startTime, filter, exposure, owner,\
        embargo, fsize FROM maidanak.rawframes)\
        UNION ( SELECT date, date_obs, type, object, raw_object, datapath, \
        telescope, startTime, filter, exposure, owner, embargo, fsize \
        FROM apo.frames)\
        UNION ( SELECT date, date_obs, type, object, raw_object, datapath,\
        telescope, startTime, filter, exposure, owner, embargo, fsize \
        FROM liverpool.rawframes))
      GRANT SELECT ON views.lenses2 TO gavo
    </script>

  </ResourceDescriptor>

Let this file reside in gavo.inputsDir/views/views.vord.

It defines a set of fields (the create="false" in the opening
RecordDef tag inhibits the creation of the table lenses2 -- this is
necessary here because we'll use that name later for our view) to
provide the information necessary to format the field appropriately
later on.  

In the script (see `Scripts`_), we create a view over some tables
created elsewhere.  This view will be dropped whenever one of the tables
is dropped, since gavoimp always drops CASCADE.  It therefore needs to 
be re-made when one of the tables is re-made.  To make sure it is,
create a Makefile in alongside the resource descriptor, containing

:

  update:
    gavoimp res/apo.vord

(as usual with make files, make sure the white space in front of gavoimp
is a Tab and not a space).  You can then simply say

::

  <recreateAfter project="views"/>

in each of the resource descriptors for maidanak, apo, and liverpool.

Scripts
*******

Some things simply aren't worth abstracting away.  Therefore, you can
embed raw code in resource descriptors.  Clearly, you should only do
that when there's no better choice, in particular if you you
non-portable SQL and the like.  All script elements have one attribute,
type.  It identifies what kind of script we're dealing with and when to
run it.  Right now, there's only one type, postCreation.

SQL scripts
...........

In general, try to write portable SQL in scripts, and always remember
that at some point you may want to export your data to, say, VOTables
and they may never see SQL.  So, only do database housekeeping stuff in
these scripts (I'm aware that the two advices sort of contradict each
other).

Anyway, SQL scripts are executed one line at a time.  Lines can be
continued by ending them with a backslash (\).  If there is a minus
("-") in front of a line, errors will be ignored, otherwise a line
causing an error aborts the entire script.

SQL scripts may come in various script elements, but right now only one
is defined:

* postCreation -- the script is executed after all tables are imported.


Interfaces
**********

Row interfaces work a bit like java interfaces in that they define
certain columns the presence of which is guaranteed, and possibly
further aspects of a resource's appearance in the database like entries
in certain tables.   You basically say something like "this table
describes things with positions" or "this table describes files
(products)".

The reason for defining interfaces is that you don't just put stuff into
the database, you want to get them out as well.  The interfaces allow
clients to rely on certain data structures for tables satisfying an
interface.

In a resource descriptor, you declare that you implement an interface
using the impements element within the Record definition of the table
that should implement the interface.  For example, you'd say

::

    <Semantics>
      <Record table="frames">
        <Field dest="date" source="DATE" dbtype="date" 
        [...]
        <implements name="positions"/>
        <implements name="products"/>
      </Record>
    [...]

to have the frames table implement both the positions and the products
interface.

The following interfaces are defined:

.. !!python ../gavo/interfaces.py docs


TODO: row processors, schema, meta, constraints,


.. vi:et:tw=72:
