==================================
GAVO python software user's manual
==================================

python-gavo is a package containing support code and a couple of entry
points (i.e., user-visible scripts) for building and running servers
with astronomical data ("VO nodes").

Installation
============

The preferred way to run python-gavo is on a dpkg-based system (apt-get
yaddayadda -- actually, packaging and setting up a repository still
needs to be done).

If this is not an option for you, you can install python-gavo based on
easy_install.  You first need a working python installation that can
build python extensions.  If you installed python yourself and have a C
compiler, that should already be the case, on some distributions you
need packages like python-dev, python-devel or somesuch installed in
addition to the binary python.  Once you have that, get the source
tar.gz, unpack it somewhere, cd into resulting directory and then say

::

  sudo python setup.py install

(there are various options to get the stuff installed when you're not
root; refer to the `setuptools
documentation <http://peak.telecommunity.com/DevCenter/EasyInstall>`_ if
necessary).

Optional components
*******************

Q3C
...

If at some point you have large data sets (more than 20000 rows, say)
and you want to do positional searches on them (cone search, crossmatch,
siap and such), you will want the Q3C library by Sergey Koposov et al,
http://www.sai.msu.su/~megera/oddmuse/index.cgi/SkyPixelization.  It's
not particularly tricky to install, but you should skip it for now.

To install it, get the source from the web site given above.  You need a
couple of dependencies (on debian etch systems, that would at least be
libpam-dev, libreadline-dev, and postgresql-server-dev-8.1).  Then
follow the instructions in the q3c distribution.

To automatically create the necessary indices, use the `q3cpositions`_ interface.

psycopg2
........

psycopg2 is an alternative binding for postgresql.  It's faster than the
default pgsql that you get by setup.py.  If you want to use it, you'll
have to install mxDateTime and psycopg2 manually, and then say
interface=psycopg2 in the [db] section your /etc/gavo.rc.

We are aware this isn't nice.  If you really use these tools and want
psycopg2, contact us and we'll improve this situation (which isn't
actually hard, but a bit messy).

VOPlot
......

The software can generate code to load VOTable into VOPlot, a Java
applet for working with VOTables.  Due to the applet security model, the
applet has to originate from the server that will deliver the data, and
so you will need to install VOPlot locally if you want to use this
feature.

Do do this, `download VOPlot
<http://vo.iucaa.ernet.in/~voi/voplot.htm>`_ and unpack the distribution
at a convenient place.  In the resulting folder, you'll find a
subdirectory binaries/VOPlot.  Copy that and (less importantly)
docs/user_guide to some location your webserver can serve from, e.g.,

::

  mkdir /var/www/htdocs/VOPlot
  cp -r binaries/VOPlot  /var/www/htdocs/VOPlot/bin
  cp -r docs/user_guide/  /var/www/htdocs/VOPlot/doc


Assuming that /var/www/htdocs corresponds to the root of your webserver,
you can then tell the GAVO software where it can find VOPlot in your
/etc/gavo.rc (see below):

::

  [web]
  voPlotEnable: True
  voplotCodeBase: /VOPlot/bin
  voplotUserman: /VOPlot/doc


wcstools
........

To support cutout services, you need getfits from the wcstools package
available at http://tdc-www.harvard.edu/software/wcstools/.   After
building wcstools, the binary is in <build-dir>/bin/getfits.

The system expects the binary getfits in inputsDir/cutout/bin/getfits,
you can optionally add a platform suffix.


Setup
=====

The tools want a "root directory" for GAVO, a directory in which the
sources reside, ancillary data is kept, etc.  By default, this is
``/var/gavo``.  If you want something else, set the environment variable
``$GAVO_HOME`` or (probably better) adapt your /etc/gavorc.

Static Files
************

Within that directory, you need some data files: static files for the
web server, interface definitions, etc.  In an svn checkout, you can go
to the resources subdirectory, say "make install" (while having
GAVO_HOME defined if you don't like /var/gavo) and be done if you have
rsync (you should, it's a fine and useful piece of software).  Otherwise,
we provide a tar file of the necessary data that you can just untar in
$GAVO_HOME.

Logging
*******

The tools will write a log (by default ``rootDir/logs/gavoops``).  It
may be a good idea to keep that log on a local file system and not share
it with other instances, not the least because we do not provide for any
kind of locking on that file.  To do that, set the environment variable
GAVO_LOGDIR to a directory you want the logs in.  That directory should,
of course, only be writable by you.

If you insist, it wouldn't be hard to make the tools log to syslog.

Database Setup
**************

To make things work, you will also need to set up a database and create
at least one profile file (see `The profiles Section`_).

Currently, we only support postgres.  You'll need a postgres
installation.  You should have one user that may create tables and even
schemas, so you probably need a dedicated database.

We can't help you much with postgres (if you have an administrator, she
should know how to set a database and the necessary users for you), but 
with a more or less usual installation something like the following should 
work (nothing of this is hardwired, but if you need a different setup,
you'll need to adjust the DB configuration files mentioned below):

::

  sudo -u postgres createdb --encoding=UTF-8 gavo  # creates the database that'll hold your data
  # create the user that feeds the db...
  sudo -u postgres createuser -P -ADSr gavoadmin
  # and a user that usually has no write privileges
  sudo -u postgres createuser -P -ADSR gavo
  # and a user for "scratch tables" writable from the world
  sudo -u postgres createuser -P -ADSR untrusted

Note down the passwords you enter, you will need them when setting up your
database profiles.

Note that running a database server always is a potential security
liability.  You should make sure you understand what your pg_hba.conf
(it's in the config directory of your postgres installation, on debian
that would be ``/etc/postgresql/<version>/main/``)
says.  As a minimum, you should have a line like

::

  local   gavo        gavo,gavoadmin,untrusted          md5

in there; it allows password authentication for the three users above
from the local machine.  If you wanted to feed or query the database from remote
machines, you'd have to provide similar lines for those machines; for
details, see the postgres documentation.

Finally, you need to let the various roles you just created access the 
database; you do this using the command line interface to postgres:

::

  sudo -u postgres psql gavo
  GRANT ALL ON DATABASE gavo TO gavoadmin;

For the individual tables, rights to gavo and untrusted are granted by
gavoimp, so you do not need to specify any rights for them.


Configuration
*************

You can adapt the gavo environment through a configuraton file.  It is
located by default in ``/etc/gavo.rc``.  You can change that location
using the GAVOSETTINGS environment variable (or ``defaultSettingsPath`` in
``config.py``, though that will break on updates).  You can override
global settings in your private settings file, ``~/.gavorc`` (you can
override that location through GAVOCUSTOM).

The built-in configuration looks like this:

::

.. !!python -c "from gavo import config;print config._builtinConfig" | sed -e 's/^/  /'

You see that gavorcs have a roughly INI-style format with sections in
square brackets and keys beyond them.  To override a setting, you need
to have the same key in the same section.

Most of the settings either not very relevant or are covered in the sections
dealing the the configured items.  Let's just look at three sections:

The DEFAULT Section
...................

The keys in here can be accessed from other sections as ``%(key)s``.

This mainly sets paths.  The most important is ``rootDir``, a directory
most other paths are relative to.  This is the one you'll most likely
want to change.  If you, e.g., wanted to have a private gavo tree, you
could put 

::

  [DEFAULT]
  rootDir: /home/user/gavo

into your ~/.gavorc.

The other paths in this section are relative to rootDir, but absolute
paths are legal, too.

You may want to set tempDir and cacheDir to a directory local to your
maching if rootDir is mounted via a network.  Also note that we do
no synchronization for writing to the log (and never will -- we will
provide syslog based logging if necessary), so you may want to tweak
this too to keep actions from seperate users seperate.

The db Section
..............

In the db section, some global properties of the database access layer
are defined.  Currently, the most releveant one is profilePath.  This is
a colon-separated list of rootDir-relative paths in which we look for
database profiles (expansion of home directories is supported).  The
first match in any of these directories wins.  This is useful when you
have a test setup and a production setup -- just say ``include dsn`` in
the common profiles (by default in configDir) and have separate dsn
files in the ~/.gavo directories of the accounts feeding the test and
production databases.

The profiles Section
....................

The profile section maps profile names to file names.  These file names
are relative to any of the directories in db.profilePath.

The file names contain a specification of the access to the database in
(unfortunately yet another, but simple) language.  Each line in 
such a profile is either a comment (starting with #), an assignment
(with "=") or an instruction (consisting of a command and arguments,
separated by whitespace).

Keywords available for assignment are

* host -- the host the database resides on.  Leave empty for a Unix
  socket connection. 
* port -- the port the database listens on.  Leave empty for default
  5432.
* database -- the database your tables live in.
* user -- the user through which the db is accessed.  
* password -- the password of user.

Commands available are

* include -- read assignments and instructions from the profile given in
  the argument
* addAllRole -- add the database role in the argument (that has to be
  known to the database) to the list of roles that get full access ("ALL
  PRIVILEGES") to any database object created.
* addReadRole -- add the database role in the argument to the list of
  roles that get read access (select or usage privilege) to any database
  object created.

A typical setup with two users admin, devel feeding the database, a user
gavo querying the database (e.g., used for querulator) and a user
untrusted that queries only particular, public tables, could look like
this (all files located in configDir, e.g., /var/gavo/etc):

A file dsn:

::

  host = computer.doma.in
  port = 5432
  database = test

A file feed, as mentioned in the default config:

::

  include dsn
  user=devel
  password=mySecretPassword
  addAllRole devel
  addAllRole admin
  addReadRole: gavo

A file feedpublic, to load public tables:

::

  include feed
  addReadRole untrusted

A file trustedquery (for querulator):

::

  include dsn
  user=gavo
  password=dontMention

A file untrustedquery (for a public SQL query interface):

::

  include dsn
  user=untrusted
  password=whyBother?


The Web Section
...............

* serverURL: the scheme and host under which the service is visible
  (like, http://vo.foo.org)
* nevowRoot: a serverURL-relative URL to the place the nevow service
  appears in the server.
* errorPage: debug or something else -- whether to show a traceback on
  the net or not.
* staticURL: a serverURL-relative URL to static the data (for
  querulator)


Binaries
********

For extended functionality, the system uses external binaries.
If your GAVO_ROOT is accessible from more than one machine and the
machines have different architectures, this may be a problem, because
one platform may not be able to execute another platform's binaries.

To fix this, set platform in the DEFAULTS section of your config file.
You can then rename any platform-dependent executable
base-<platform>, and if on the respective platform, that binary will be
used. This also works for computed resources using binaries.  


The Portal
**********

The root URL of the data center should return a page that explains what
the site is all about and ideally list what's available.  The template
for this page is
located in root.html in templateDir (which defaults to
GAVO_ROOT/web/templates and is defined in the web section of config).
A list of services in alphabetic order can be obtained using something
like

::

		<div id="servicelist" n:data="chunkedServiceList" n:render="sequence">
			<n:invisible n:pattern="item" n:render="mapping">
				<h3><n:slot name="char"/>...</h3>
				<ul n:data="chunk" n:render="sequence">
					<li n:pattern="item" n:render="mapping">
						<a n:render="serviceURL"/>
						<span n:render="ifprotected">[P]</span>
					</li>
				</ul>
			</n:invisible>
		</div>



Importing a resource
====================

To import a resource into the database, you should first make a
subdirectory of your gavo inputs directory (GAVO_HOME/inputs), the
resource directory.  The
recommended layout within this directory is:

* res/ -- a directory containing the resource descriptor plus
  additional metadata (like mapping files), if applicable
* bin/ -- if applicable, a directory containing code that may be needed
  for preprocessing and the like.  Use with care and provide explanatory
  make targets.
* <anything convenient>/ -- directory containing input source file(s)
* Makefile -- a makefile with at least one target, update, that
  rebuilds the tables generated by this resoure (details pending).
* README -- a short description of the source: what it is, where it came
  from, etc.

After unpacking the raw data, you need to create a resource
descriptor (see `Resource Descriptors`_).  Once you have that, call

::

  gavoimp <path-to-rd>

do start parsing and uploading the data.

You can give gavoimp a couple of options, mostly to aid in debugging; 
see ``gavoimp --help``.

Resource Descriptors
====================

The resource descriptors (suggested extension: .vord, for VO resource
descriptor) are XML files speficying both the syntax and the semantics
of a source.

Since they have to express quite a bit, they're a a little cumbersome.
We may want to write wrappers for standard cases.  Let's see.

The main source of information on the gory details of RDs should be the
schema file which should be lavishly commented (ahem).  The following is
meant as  a high-level introduction, but also covers topics that don't
really fit into the schema file.  In short: You'll need to inspect both
for the full story.  Start working with examples.

The root element defines the directory most other paths are relative to,
like this:

::

  <ResourceDescriptor srcdir="&master-name;/raw">

(it's usually a good idea to define a couple of entities in RDs,
master-name being one: that should be the name of your resource
directory).

Roughly, an RD mainly consists of one or more data sections.  A data
section describes one source (or set of sources) with a homogenous
structure (i.e., they share a "common data model").   

A Data section contains a grammar element describing how to parse the
input file in terms of preterminals and literals, and a Semantics
element that explains how to interpret the preterminals from the Grammar
section. The term "preterminal" here corresponds to nonterminal symbols
that directly expand into the terminal data, i.e., they give names to
certain sequences of terminal tokens.

The mappings from preterminals to token sequences (actually, strings)
are called rowdicts in the document.  The semantics describe how these
rowdicts are transformed into records.  These map names (column names
for data bases, field names for VOTables) to parsed values -- rowdicts
only have strings as values, records contain ints, floats, DateTime
objects, strings, whatever.

Grammars
********

Depending on what the input is, we want different types of grammars.
Thus, there are various grammar elements, but each Data element only has
exactly one of them.  Currently defined are:

* NullGrammar -- when you don't want to do any parsing at all.  You'll
  want that if you just need to define a table through a resource
  descriptor.  The product table (see `Products`_) is an 
  example of why this is necessary: No single resource can "own" it, but 
  something needs to own it.
* CFGrammar -- when you need context free grammars.  This is based on
  pyparsing and is currently unmaintained, mainly because I have no
  source that actually needs it.
* REGrammar -- when regular expressions plus a bit will do.  These are
  what you'll want to use for text tables.  Basically, you specify some
  way to tokenize your input and REs for the individual tokens.
* KeyValueGrammar -- used for files that give key-value pairs, one file
  per row.
* ColumnGrammar -- used for ASCII tables, somewhat analogous to the
  byte-by-byte descriptions Vizier uses.
* FitsGrammar -- used for fits files.  Currently, only the fields of the
  primary header are used, but once I have fits tables, it shouldn't be
  hard to retrofit access to other headers.

All grammars support macros (see Macros_).

ColumnGrammars
..............

ColumnGrammars expose preterminals as ranges of character indices, where
counting starts with 1 and ranges are inclusive.  You can also give a
single number, and you'll get a character.  The basic unit is always a
single physical line.

ColumnGrammars have the following attributes:

* topIgnoredLines -- skip that many lines at the top (for table headings
  and similar detritus.

Note that ColumnGrammars support symbolic names if you give them in
Macros.  If everything in a preterminal is whitespace, a NULL value is
returned.

As an example, consider

::

  <ColumnGrammar topIgnoredLines="1">
    <Macro name="linearMap" destination="mag" factor="1e-3">
      <arg name="val" source="14-18"/>
    </Macro>
  </ColumnGrammar>

A file containing

::

  12345678901234567890
  x-55 55.7689 34233 a
  x-43 43.3483       b
  
would then produce two records.  Within the first, the preterminal
``mag`` would have the value 34.233, the preterminal 1-4 would be "X-55"
the preterminal 20 would be a, etc.  In the second record, ``mag`` would
be ``NULL``, as would be 13-14, 15, etc., because all of them only
contain whitespace.


CustomGrammars
..............

These allow hand-written python classes to stand in as a grammar.  To
use a CustomGrammar, say something like

::

 		<CustomGrammar module="res/grammar" imageDimension="1000"
			firstMagpat="8" magpatFile="data/offsets" sourcePat="data/MAGPAT*"/>

This will look for a module grammar.py in the res subdirectory of the
resource directory as specified in the resource descriptor.  This module
has to define a function

::

  getGrammar(parentDD, userAttrs, initvals) -> grammar object

parentDD is the data descriptor the grammar belongs to, userAttrs is a
dictionary mapping the keywords given in the grammar declaration to
their (string) values, and initvals is a dictionary mapping the standard
grammar items (e.g., macro) to their values.  The object returned must
as least have parse(parseContext) and _setupParse(parseContext) methods.

Usually, however, you should inherit the grammar from
customgrammar.UserGrammar like this:

::

  class LCGrammar(customgrammar.UserGrammar):
    def __init__(self, parentDD, userAttrs, initvals):
      super(LCGrammar, self).__init__(parentDD, userAttrs, initvals)

This will give you attributes parentDD and userAttrs of your grammar
object to play with.

You can then override the _iterRows(parseContext) method (if necessary,
_getDocdict and _setupParse are other interesting hooks).  It should be
a generator returning dictionaries mapping the keys defined in the
associated record(s) to string values.

By defining CustomGrammars in this way, you get macro processing for
free.

With custom grammars, the token attribute of data definitions are
frequently useful.  Normally, you get one parse context per file
matched, and you cannot pass directories.  A token given in the data
descriptor declaration (<Data token="foo/bar">) is passed literally
without inspection and is available as the sourceName attribute of the
parseContext.

The current resource directory can be obtained from
self.parentDD.rD.get_resdir().

FitsGrammars
............

These grammars expose the primary FITS header keywords as preterminals.
Contrary to other grammars, the values of the preterminals are not
always strings, which may cause trouble when applying macros.  This
could be solved using type conversion macros if necessary.

FitsGrammars have one attribute

* qnd -- if True, a quick-and-dirty header parser will be used.  It
  only works for the primary header but is much faster for
  gzipped FITS files.  If your FITS files are not compressed, don't
  bother with it.

FitsGrammars currently do not support fits tables or the like.  If they
do at some point, the header keywords will be available in the document
record when not in docIsRow mode (in other words, FitsGrammars only make
sense in docIsRow mode at the moment, and it will always be the default
for them).

KeyValueGrammars
................

These grammars parse files organized in key-value pairs, i.e., files
that somewhat look like this:

::

  field1: bla
  field2= 23  # or something

KeyValueGrammars have the following attributes:

* kvSeperators -- a string containing the admissable seperator
  characters between key and value (default: colon and equals sign).
  Note that whitespace around key and value is ignored.
* pairSeperators -- a string containing characters that seperate
  individual key-value pairs (default: Linefeed)
* commentPattern -- a regular expression defining comments (default:
  ``"(?m)#.$*"``)

Note that KeyValueGrammars currently only work in docIsRow mode.  It is
unclear if and how they will ever support multiple rows in one file.

CFGrammars
..........

Don't use these for now.  If you think you need context free grammars to
parse your source, contact the author.

REGrammars
..........

Don't use these for now.  They work, but I don't like the way they do.
If you really need them, contact the the author, and we'll try to figure out a
good way of describing them.

We currently have three types of tokenizers (given in the type attribute of the element):

* split -- the content matches a separator between the fields (each row is processed by re.split()
* match -- the content matches an entire row, the fields being a groups defined. This probably is convenient with the {}-notation: (.{4}) (.{8}) (.{3}) would match a table with 4, 8, and 3 wide fields, separated by single spaces. Since this is so common, we'll probably provide an abbreviated syntax for this.
* colranges -- the content is a whitespace-seperated list of columns or
  column ranges (#-style comments allowed).  Example: 1 4-5 10-20 produces
  three tokens, one from column 1 (the start of the string), one
  consisting of the characters at positions 4 and 5 (note the difference
  to python slice notation: the end of the range is *included*), and one
  of length 11, spanning columns 10 through 20.

docIsRow
........

For certain types of files (Key-Value-Pairs, FITS files), usually an
entire file will map to a table row.  In these cases, set the attribute
docIsRow="true" on the grammar.  This works for all grammar types.  For
KeyValueGrammars and FitsGrammars, it is the default.  Thus, unless
you're doing funny things, you probably do not need to worry about it.

Macros
******

Unfortunately, the input data isn't always in the format we'd like it to
have.  Some of these issues can be solved by giving literalForms (see
Semantics_) and the like, but frequently manipulations involve more than
one token or require.  To get a halfway declarative way of defining
these, macros are used.  Macros take a set of arguments and work on
rowdicts.  Their effects are reflected in rowdicts.

To understand macros, let's look at two examples:

::

      <Macro name="mapValue" sourceName="maidanak/res/maydanak_targets.txt">
        <arg name="value" source="OBJECT"/>
        <arg name="destination" value="ARI_OBJC"/>
      </Macro>

Here, we specify that a macro called mapValue is to be applied to the
rowdicts.  This specific macro needs a file specifying the mapping,
which is given as a "construction argument" sourceName.

During the application of the macro, it takes two arguments, the value
to be mapped and a destination field.  Macro arguments can have two
sources -- either a literal, were the value given is directly passed to
the macro (the argument is the same for all rowdicts), or the value of a
preterminal.  In the former case, you use the value attribute, in the
latter, the source.  Thus, the macro above always has ARI_OBJC as value
for its destination argument, whereas the value argument is taken from
the OBJECT field of the rowdict.

::

      <Macro name="handleEquatorialPosition">
        <arg name="alpha" source="ALPHA"/>
        <arg name="delta" source="DELTA"/>
      </Macro>

This specification says: "Take the values of ALPHA and DELTA in the
rowdict and pass them as arguments alpha and delta to the macro
handleEquatorialPosition".  As a result in this case, you'll have new fields
alphaFloat, deltaFloat, c_x, c_y, c_z, and htmid in the rowdict -- this
is not visible from the macro definiton but is hardcoded in the macro.

Macros live in gavo/parsing/macros.py.  The following is documentation
generated from there.  To figure out what arguments to give in XML,
look at the constructors in the test cases.  Their second argument gives
the arguments in triples, where the first item is the argument name, the
second is a field name (attribute "source"), and the last is a constant
(attribute "value").  So, if the constructor is called like this:

::

    >>> r = ReSubstitutor(None, [("destination", "", "fixed"),
  ... ("data", "broken", ""), ("srcRe", "", r"(.) \(([^)]*)\)"),
  ... ("destRe", "", r"\2 \1")])

the corresponding XML would be

::

  <Macro name="subsRe">
    <arg name="destination" value="fixed"/>
    <arg name="data" source="broken"/>
    <arg name="srcRe" value="(.) \(([^)]*)\)"/>
    <arg name="destRe" value="\2 \1"/>
  </Macro>

Currently defined macros are:

.. !!python ../gavo/parsing/macros.py docs


Constraints
***********

To keep out "broken" records, you can formulate constraints.
Constraints can be formulated at the level of dictionaries and assert
certain properties of these.  If you embed a constraints element into a
grammar, rowdicts will be checked, if you embed them into a Record,
records will be checked.

A constraint consists of one or more conditions.  The constraint to
satisfies if any of the conditions evaluate to true.  The following
conditions are defined:

.. !! python ../gavo/parsing/conditions.py docs

A constraint should have a name attribute -- if a constraint fails to
satisfy, this name is given in the message.

If a constraint fails to satisfy, the record in question will be
skipped.  You can build a constraint with the fatal attribute which will
raise a ValidationError that will typically stop processing.

To use constraints, embed a constraints element into a Record or Grammar 
element:

::

    <constraints>
      <constraint name="subject given">
        <condition type="keyPresent" name="subject"/>
      </constraint>
      <constraint name="broken record present" fatal="True">
        <condition type="valueNotEqual" name="status" value="ok"/>
      </constraint>
    </constraints>


Semantics
*********

The semantics contain one or more record defintions, where each of these
mainly consists of field definitions.

Field Definitions
.................

The field definition defines a single field within a table. It is
described by the following attributes (only the ones marked with [*] are
strictly necessary):

* dest -- a name that is used as an identifier for this column (should be an SQL identifier) [*]
* source -- the source from which to fill the column. This can be a
  simple name, which is then used as the preterminal the field is fed
  from. If the value of source starts with an @, the rest is interpreted
  as the name of a field computer, which is called when the value of the
  field is required (see `At Expansions`_).
* dbtype -- the type of the value. We use the sql type system and 
  map it to the others involved (python, votable). Defaults to real.
* default -- a literal used as a default value.
* unit -- the unit of the value, given as given in http://vizier.u-strasbg.fr/doc/catstd-3.2.htx, defaults to empty.
* optional -- if true (in any capitalization), empty values (Python: None, SQL: NULL) are allowed for this field, defaults to True.
* ucd -- a description of the field content, as per http://www.ivoa.net/Documents/latest/UCDlist.html.
* description -- human-digestable text that describes the contents of this field.
* literalForm -- an identifier that tells ImportTools's type converter
  about special literal forms (e.g. hour angle for float, weird date
  formats). These identifiers are discussed below
  Probably the most important one is "do not touch" which instructs the
  type converter to leave the value alone.  This is important when,
  e.g., macros or field computers already deliver python types as
  values.  These typically do not appreciate being treated as strings.
* verbLevel -- an integer, usually between 0 and 30, giving the
  "importance" of the column for VOTables and the like.  Protocols like
  Simple Cone Search have parameters like ``VERB``, where ``VERB=1``
  requests a maximally terse output, ``VERB=3`` a maximally verbose
  output.  In that spirit, columns are usually only included in VOTables
  when their ``verbLevel<=VERB*10``.  Similar rules will apply to
  protocols other than SCS.  The default is 30, which means that
  the column is only included in the most verbose VOTables.
* references -- this is used in SQL data definition and follows the SQL
  syntax.
* primary -- declares this field as belonging to the primary key.

FieldDef elements may have a Values subelement.  These define legal
values and have the following attributes:

* min, max -- minimum and maximum of acceptable values.  If given, they
  have to be literals of the type given in datadef.
* nullLiteral -- a literal that maps to a null value on parsing.
* multiOk -- boolean; if true, queries to these fields may contain
  more than one value and will match any of these values.

They may have option subelements enumerating legal values (as literals
in the option element's content).  If both options and min and max are
given, the option element take precedence (i.e., min and max are
ignored).

Values can also have a formdb attribute.  currently, this has to be raw
SQL fitting into a "SELECT DISTINCT" query.  As an example, you could say 

::

  <Values fromdb="filter FROM mytable"/>

It is likely that I'll allow complete SELECT statements later.


Literal Forms
.............

In addition to the special "do not touch" discussed above, literal forms
avaliable for a Field's literalForm attribute include:

.. !!python ../gavo/parsing/typeconversion.py docs "'"

At Expansions
.............

In particular values introspecting on the parsing process (like finding
out the name of the source file) are handled through at expansions. The
reason there aren't macros for the things mentioned here is a technical
one -- macros shouldn't need to know the things the at-expansions
insert.   At expansions can be used in the value attributes of Fields
and Macros.

To use an at expansion, simply say something like <Field
value="@someexp"/>, i.e. a value with an @ in front.  If you really ever
need a literal @ in front of a "real" value, type the @ twice.

Some at expansions don't need to know about the parsing process but
instead only require information present in the resource desrciptor
itself.  These are marked below and can be used in a few spots outside 
of Records (in particular in owningConditions).

The following at expansions are known:

.. !!python ../gavo/parsing/parsehelpers.py docs "'"

Shared Tables
.............

In general, if a resource has a Record element with a table name in it,
it will assume it "owns" this table, i.e., it will manipulate the table
as it sees fit, up to and including dropping it.  This is not desirable
if a table contains material from more than one resource.  Though it's
frequently a good idea to solve this thorough a view-only resource as
discussed in the section on `Dependent Resources`_, this would be at least
inconvenient if it is not known in advance what resources the table
material may come from.  The classic example is the product table.

For cases like these, you can have Shared Tables.  These are defined
using the SharedRecord element.  This works like the usual RecordDef in
that it contains Field elements, but differs in two aspects:

- They will never create a table and never drop it
- They require an owning condition enabling the importer to know which
  items it should delete when recreating the table.

Owning conditions
'''''''''''''''''

The onwingCondition element has two attributes.  The first, colName,
gives the name of a column to match in the shared table, the second,
value, the value this column must have to mark it as generated by the
resource.  The importer will delete all rows matching this owning
condition from the shared table when it re-imports the resource.  For
example, an owning condition like

::

  <owningCondition colName="sourceTable" value="foo.bar"/>

translate into SQL like

::

  DELETE * from <shared table> WHERE sourceTable='foo.bar'

You can use general @-expansions in owning conditions (and will frequently need
to).

Dealing with shared tables
''''''''''''''''''''''''''

Each shared table needs an owning resource.  As a rule, that owning
resource will not enter any data of its own.  Here is the resource
definition for the product table as an example:

::

.. !!sed -e 's/^/  /' ../resources/inputs/__system__/products/products.vord

Since the record definition of a shared table is present in all
resource descriptors contributing to the table, it is likely
that you want to define a new interface for whatever you want to do.  
It would then take care of keeping the definitions in sync.
However, you need to change the python source to do that.  If you don't
want to do that, you can get by using entities.  If you want to go that
way, contact the authors, and we'll tell you how to go about it.


Dependent Resources
*******************

It is not an uncommon situation that resource A needs to be updated when
resource B changes.  When that happens, you can use the recreateAfter
element, which is expected as a direct child of ResourceDescriptor.  
It has one attribute, the name of a project (which is the name of the
directory relative to gavo.inputsDir).  This may look like this.

::

  <recreateAfter project="lensdemo"/>

With this specification, the importer will, after finishing its work on
the resource, enter the named directory and call "make" there.  If you
follow the recommended setup, this will cause the recreation of the
named resource's table(s).

As an example, supposed we have various tables having quite a few
fields in common without warranting an interface of their own.  You may
want to create a view over them.  To do this, write a resource
descriptor like

::

  <?xml version="1.0" encoding="utf-8"?>

  <ResourceDescriptor srcdir="lensdemo">
    <schema>views</schema>
      <Semantics>
        <Record table="lenses2" create="False">
          <Field dest="date" dbtype="date" 
            tablehead="Creation Date" ucd="time.creation"/>
          <Field dest="date_obs" dbtype="date" 
            tablehead="Obs. Date"/>
          <Field dest="type" dbtype="varchar(10)" 
            optional="false" nullvalue="None" tablehead="Obs. Type" 
            ucd="meta.code.class" 
            description="Type of observation (science, flat, bias, calib...)"/>
          <Field dest="object" dbtype="text" 
            nullvalue="" tablehead="Target Object" 
            description="Object being observed, Simbad-resolvable form"
            ucd="meta.id;meta.main"/>
          <Field dest="raw_object" source="OBJECT" dbtype="text" 
            tablehead="Target Object (Header)" description=
            "Object being observed, from fits header (unreliable)"/>
          <Field dest="telescope" source="TELESCOP" dbtype="text" 
            tablehead="Telescope" ucd="instr.tel"/>
          <Field dest="startTime" source="startTime" dbtype="timestamp"
            literalForm="do not touch" tablehead="UT of obs. start"
            ucd="time.start" unit="yr"/>
          <Field dest="filter" source="FILTER" dbtype="text" 
            tablehead="Filter" ucd="instr.filter"/>
          <Field dest="exposure" source="EXPTIME" dbtype="float" 
            tablehead="Exp. time" ucd="time.interval;obs.exposure"
            unit="s"/>
          <implements name="products"/>
        </Record>
      </Semantics>
    </Data>

    <script type="postCreation">
      -DROP VIEW views.lenses2
      CREATE VIEW views.lenses2 AS ((SELECT date, date_obs, type, object, \
        raw_object, datapath, telescope, startTime, filter, exposure, owner,\
        embargo, fsize FROM maidanak.rawframes)\
        UNION ( SELECT date, date_obs, type, object, raw_object, datapath, \
        telescope, startTime, filter, exposure, owner, embargo, fsize \
        FROM apo.frames)\
        UNION ( SELECT date, date_obs, type, object, raw_object, datapath,\
        telescope, startTime, filter, exposure, owner, embargo, fsize \
        FROM liverpool.rawframes))
      GRANT SELECT ON views.lenses2 TO gavo
    </script>

  </ResourceDescriptor>

Let this file reside in gavo.inputsDir/views/views.vord.

It defines a set of fields (the create="false" in the opening
RecordDef tag inhibits the creation of the table lenses2 -- this is
necessary here because we'll use that name later for our view) to
provide the information necessary to format the field appropriately
later on.  

In the script (see `Scripts`_), we create a view over some tables
created elsewhere.  This view will be dropped whenever one of the tables
is dropped, since gavoimp always drops CASCADE.  It therefore needs to 
be re-made when one of the tables is re-made.  To make sure it is,
create a Makefile in alongside the resource descriptor, containing

:

  update:
    gavoimp res/apo.vord

(as usual with make files, make sure the white space in front of gavoimp
is a Tab and not a space).  You can then simply say

::

  <recreateAfter project="views"/>

in each of the resource descriptors for maidanak, apo, and liverpool.

Scripts
*******

Some things simply aren't worth abstracting away.  Therefore, you can
embed raw code in resource descriptors.  Clearly, you should only do
that when there's no better choice, in particular if you use
non-portable SQL and the like.  All script elements must have one attribute,
type.  It identifies what kind of script we're dealing with and when to
run it.  Known types include:

 * preCreation -- SQL executed before a DataDescriptor is being handled.
 * postCreation -- SQL executed after a DataDescriptor or a
   ResourceDescriptor (whatever element contains the script) is
   imported.
 * processTable -- a python function with access to the data descriptor
   dataDesc and the dbapi2 connection connection that can postprocess
   or fill a table using python statements.

Scripts should have a second attribute, name.  Its value is up to you
and is only used for display purposes.

SQL scripts
...........

In general, try to write portable SQL in scripts, and always remember
that at some point you may want to export your data to, say, VOTables
and they may never see SQL.  So, only do database housekeeping stuff in
these scripts (I'm aware that the two advices sort of contradict each
other).

Anyway, SQL scripts are executed one line at a time.  Lines can be
continued by ending them with a backslash (\).  If there is a minus
("-") in front of a line, errors will be ignored, otherwise a line
causing an error aborts the entire script.

Note that, when suppressing errors, you'll usually want to enclose the
optionally-failing statements into a BEGIN/END pair of their own.  The
reason is that many
errors will cause the transaction to end.  The consequences of this
depend on the database interface used and are practically never what you
want.  Putting the statement(s) into transactions of their own leads to
predictable behavior.

Another problem is the creation of database objects.  Since the script
runner doesn't check what objects you create, these will typically not
have the privileges set they way you defined in your profiles.  To work
around this, say

::

	@@@TABLERIGHTS("<table name>")@@@
	@@@SCHEMARIGHTS("<schema name>")@@@

whenever you create a table or schema, respectively.
  



SQL scripts may come in various script elements, but right now only one
is defined:

* postCreation -- the script is executed after all tables are imported.


Interfaces
**********

Row interfaces work a bit like java interfaces in that they define
certain columns the presence of which is guaranteed, and possibly
further aspects of a resource's appearance in the database like entries
in certain tables.   You basically say something like "this table
describes things with positions" or "this table describes files
(products)".

The reason for defining interfaces is that you don't just put stuff into
the database, you want to get them out as well.  The interfaces allow
clients to rely on certain data structures for tables satisfying an
interface.

In a resource descriptor, you declare that you implement an interface
using the impements element within the Record definition of the table
that should implement the interface.  For example, you'd say

::

    <Semantics>
      <Record table="frames">
        <Field dest="date" source="DATE" dbtype="date" 
        [...]
        <implements name="positions"/>
        <implements name="products"/>
      </Record>
    [...]

to have the frames table implement both the positions and the products
interface.

The following interfaces are defined:

.. !!python ../gavo/interfaces.py docs


TODO: row processors, schema, meta, constraints,


Meta Information
================

Stuff to be put into the VO needs lots of meta information.  Meta
information is typically provided in resource descriptors, and there in
the  ResourceDescriptor, Data, and Record elements.  These can carry
meta elements the content of which becomes the value belonging to a
certain meta key.  You can set a format attribute to one of the
following values:

* literal (the default) -- no processing will be done on the value.
* plain -- string values will be whitespace-normalized, empty lines will
  be converted into paragraph separators in formats that support that.
* restructured -- string values will be interpreted as Restructured
  Text.

You can also use the compute attribute.  The value is a call to an
@-expansion (without the leading @).

We don't enforce any meta names, so you can put anything into meta.
However, you *should* use the keys defined in [RMI]_ (registration depends
on it), and it's advisable to use names with leading underscorces for
key not mentioned in RMI.

Recommended local keys (some of them are used in the default template)
include:

* _ legal -- human-readable copyright information, usage conditions, and
  the like.
* _title -- a title for presentation purposes (where it deviates from
  the RMI title).
* _intro -- a more verbose description of the entity.
* _infolink -- a link to static information on the service, the result,
  or whatever.
* _query_status -- an InfoItem for SIAP-like error reporting (you can't
  currently generate these using resource descriptors)


Metadata
========

Metadata is given in meta elements in the resource descriptor or in the
meta section of the config file.  Metadata queries are cascaded, i.e.,
handed "upwards" until something can satisfy them.  The meta items in
the config file are always toplevel and thus give a "fallback".  Take
care what you put there.

Metadata is always a mapping from a string and mostly goes to a simple
string.  In resource descriptors, you can specify a format for metadata.
This currently can be plain (which causes whitespace to be normalized
at usage time; thus, you'll probably want this as soon
as there's a line break in the value) or literal (the default, leaving
all whitespace as it is).

The standard metadata keys are taken from Dublin core and
[RMI]_.  There are a few local metadata keys, and with custom templates
you can also use your own metadata keys.

Here is a list of the standard metadata keys used by the registry and/or
web infrastructure:

* contributor [DC]
* coverage [DC]  -- not currently evaluated
* creator [DC]
* date [DC] -- not used, since date information always comes from other
  sources
* description [DC]
* format [DC] -- not currently evaluated
* identifier [DC] -- not used, always computed from shortName
* language [DC]
* publisher [DC]
* relation [DC] -- not currently evaluated
* rights [DC]
* source [DC] -- not currently evaluated
* subject [DC]
* title [DC]
* type [DC]

The Web Interface
=================

Most aspects of the web interface are configured in the resource
descriptor.  XXX yadda.

... To include all default fields (which are those that have a queriable
type, a verbLevel of less than 21 and a displayHint different from
suppress), say ``<autoCondDescs/>`` in the service or input adapter.  

To include a single field, say ``<condDesc name="fieldName">``.  This
will choose an appropriate widget in HTML forms.  In particular, an
enumerated data field (see `Field Definitions`_) will be shown as a
drop-down box, floats, dates and strings will become inputs with
vizier-like expressions.


Templates
*********

The templates referred to in service definitions are valid XHTML.  Data
is filled in through namespaced elements and attributes.  Right now,
there is only one namespace, viz., http://nevow.com/ns/nevow/0.1.  It
contains names understood by nevow (the web framework we build on)
proper.  You should define that namespace on your html element, like
this:

::

  <!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN"
     "http://www.w3.org/TR/html4/loose.dtd">
  <html xmlns:n="http://nevow.com/ns/nevow/0.1">
  ...
  </html>

In the following, I assume you chose n as the prefix, as in the example
definition.

The most important names in there are:

* n:data -- puts some data in a "context"
* n:render -- chooses how that data is being rendered
* n:slot -- select some data from an appropriate context

Common Renderers
................

* meta -- renders a meta item (see `Meta Information`_) given in the child as text, as in
  ``<p n:render="meta">curation.publisher</p>``.
* render="metahtml" -- as render="meta", only the meta item will be
  rendered as html (this is useful when the meta item can contain rich
  text)
* render="rootlink" -- works on things having a href or src attribute.  This
  attribute will get nevowRoot prepended.  You
  should use this whenever you give an absolute link within the service,
  e.g., to a style sheet or a different service, like this:  
  ``<link href="/formal.css" type="text/css" rel="stylesheet"
  n:render="rootlink"/>`` to make things work when the service doesn't
  run on the server root.

Form Templates
..............

In From templates, , you can use render="form genForm" to insert the generated
default form in form templates (this will usually be in a div tag), like
this:

::

		<div id="form" n:render="form genForm"/>
  

Result Templates
................

For query results, put data="result" on the root element of your
document.  This puts the query result in scope, and this object also
contains information on the query.  You can then access the following
data:

* data="querypars" -- puts the query parameters as given by the user 
  into the scope [slot].
* data="queryseq" -- puts a sequence of user-submitted query parameters
  into the the scope.  There's a parpair renderer that can handle these
  pairs.
* data="resultmeta" -- puts a dictionary of meta information on the
  result in the context.  XXX explain what's in there (itemsMatched, etc).
* data="inputRec" -- puts the document record of the query (i.e. the
  "global" processed query parameters) in scope.  What's in there can
  be seen by looking at the fields in the input adapter's data
  definition [slot].

The items in data elements marked with [slot] should be used with
rend="mapping" and <slot name=...>.

The following additional renderers are available:

* render="resulttable" -- inserts the generated default table in result
  templates (this will usually be in a div tag).
* render="warnTrunc" -- gives a warning if the query was truncated in
  the context tag.
* render="parpair" -- renders a key-value pair.  This may be handy for
  queryseq data.
* render="warnTrunc" -- this only works when data="query" and will
  render a warning if a query was truncated due to reaching the match
  limit.


Common Error Conditions
=======================

* "primary key <whatever> could not be added" -- this is most likely due
  to duplicate entries with the same values for the primary key.  To
  find out which these are, import the table without the primary
  definition and then do a query like ``select counts.f1, counts.f1 from
  (select f1, f2, count(*) as size from mytable group by f1,f2) as 
  counts where counts.size>1`` -- this would be for a primary key
  consisting of f1 and f2 on mytable.

References
==========

.. [RMI] Hanish et al, `Resource Metadata for the Virtual Observatory <http://www.ivoa.net/Documents/latest/RM.html>`_

.. vi:et:tw=72:
