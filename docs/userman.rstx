==================================
GAVO python software user's manual
==================================

python-gavo is a package containing support code and a couple of entry
points (i.e., user-visible scripts) for building and running servers
with astronomical data ("VO nodes").

Installation
============

The preferred way to run python-gavo is on a dpkg-based system (apt-get
yaddayadda).

If this is not an option for you, you can install python-gavo based on
easy_install.  You first need a working python installation that can
build python extensions.  If you installed python yourself and have a C
compiler, that should already be the case, on some distributions you
need packages like python-dev, python-devel or somesuch installed in
addition to the binary python.  Once you have that, get the source
tar.gz, unpack it somewhere, cd into resulting directory and then say

::

  sudo python setup.py install

(there are various options to get the stuff installed when you're not
root; refer to the `setuptools
documentation <http://peak.telecommunity.com/DevCenter/EasyInstall>`_ if
necessary).

Setup
=====

The tools want a "root directory" for GAVO, a directory in which the
sources reside, ancillary data is kept, etc.  By default, this is
``/var/gavo``.  If you want something else, set the environment variable
``$GAVO_HOME``.

Static Files
************

Within that directory, you need some data files: static files for the
web server, interface definitions, etc.  In an svn checkout, you can go
to the resources subdirectory, say "make install" (while having
GAVO_HOME defined if you don't like /var/gavo) and be done if you have
rsync (you should, it's a fine and useful piece of software).  Otherwise,
we provide a tar file of the necessary data that you can just untar in
$GAVO_HOME.

Logging
*******

The tools will write a log (by default ``$GAVO_HOME/logs/gavoops``).  It
may be a good idea to keep that log on a local file system and not share
it with other instances, not the least because we do not provide for any
kind of locking on that file.  To do that, set the environment variable
GAVO_LOGDIR to a directory you want the logs in.  That directory should,
of course, only be writable by you.

If you insist, it wouldn't be hard to make the tools log to syslog.

Database Setup
**************

To make things work, you will also need to set up a database and create
at least one Settings_ file.

Currently, we only support postgres.  You'll need a postgres
installation.  You should have one user that may create tables and even
schemas, so you probably need a dedicated database.

We can't help you much with postgres (if you have an administrator, she
should know how to set a database and the necessary users for you), but 
with a more or less usual installation something like the following should 
work:

::

  sudo -u postgres createdb gavo  # creates the database that'll hold your data
  # create the user that feeds the db...
  sudo -u postgres createuser -P -ADS gavo-admin
  # and a user that usually has no write privileges
  sudo -u postgres createuser -P -ADS gavo

Note down both passwords, you will need them when setting up your
gavosettings files.

Note that running a database server always is a potential security
liability.  You should make sure you understand what your pg_hba.conf
says.

Settings
========

Most operations will eventually lead to database accesses.  The tools
will get access information from a gavosettings file.  By default, it is
searched in ``$HOME/.gavosettings`` (or ``/no_home/.gavosettings`` if
``HOME`` is not defined).  You can override the path to this file via
the environment variable GAVOSETTINGS.

This file contains ini-style configuration, i.e., sections (in square
brackets) or key-value pairs.  Currently, there is only one section, db.
Within that section, you can set

* dsn -- the "path" to the database, in the format <host>:<port>:<db>.
  Example: ``computer.doma.in:5432:test``
* user -- the user through which the db is accessed.  Example:
  ``me``
* password -- the password of user.  Example: ``mySecretPassword``
* allRoles -- a comma seperated list of database user ids that will get
  all privileges on newly created schemas and tables.  Example:
  ``me, admin1, devel``.
* readRoles -- a comma seperated list of database user ids that will
  get usage/read access on newly created schemas and tables.  Example:
  ``gavo,op0``

A gavosettings file that sets all mentioned parameters would then look
like this:

::

  [db]
  dsn: computer.doma.in:5432:test
  user: me
  password: mySecretPassword
  allRoles: me, admin1, devel
  readRoles: gavo,op0

If you provide web services, you should at least have one non-privileged
account (gavo in this case).  Web services should always use a
gavosettings-File of their own, logging them in as that non-privileged
user.

If you followed the instructions above, your private .gavosettings file
should look like this:

::

  [db]
  dsn: localhost:5432:gavo
  user: gavo-admin
  password: (whatever you've noted above)
  allRoles: gavo-admin
  readRoles: gavo

You'll need a gavosettings file for the querulator later; you might put
it under $GAVO_HOME/web/gavosettings and make it readable to the
webserver and your group only.  If your web server runs on the machine
with the database (which is recommended until you know your way around), 
it would look like this:

::

  [db]
  dsn: localhost:5432:gavo
  user: gavo
  password: (whatever you've noted above)

Note that you don't need any role specifications here since the
querulator will never write to the database.

Importing a resource
====================

To import a resource into the database, you should first make a
subdirectory of your gavo inputs directory (GAVO_HOME/inputs), the
resource directory.  The
recommended layout within this directory is:

* res/ -- a directory containing the resource descriptor plus
  additional metadata (like mapping files), if applicable
* bin/ -- if applicable, a directory containing code that may be needed
  for preprocessing and the like.  Use with care and provide explanatory
  make targets.
* <anything convenient>/ -- directory containing input source file(s)
* Makefile -- a makefile with at least one target, update, that
  rebuilds the tables generated by this resoure (details pending).
* README -- a short description of the source: what it is, where it came
  from, etc.

After unpacking the raw data, you need to create a resource
descriptor (see `Resource Descriptors`_).  Once you have that, call

::

  gavoimp <path-to-rd>

do start parsing and uploading the data.

You can give gavoimp a couple of options, mostly to aid in debugging; 
see ``gavoimp --help``.

Resource Descriptors
====================

The resource descriptors (suggested extension: .vord, for VO resource
descriptor) are XML files speficying both the syntax and the semantics
of a source.

Since they have to express quite a bit, they're a a little cumbersome.
We may want to write wrappers for standard cases.  Let's see.

The main source of information on the gory details of RDs should be the
schema file which should be lavishly commented (ahem).  The following is
meant as  a high-level introduction, but also covers topics that don't
really fit into the schema file.  In short: You'll need to inspect both
for the full story.  Start working with examples :-)

The root element defines the directory most other paths are relative to,
like this:

::

  <ResourceDescriptor srcdir="&master-name;/raw">

(it's usually a good idea to define a couple of entities in RDs,
master-name being one: that should be the name of your resource
directory).

Roughly, an RD mainly consists of one or more data sections.  A data
section describes one source (or set of sources) with a homogenous
structure (i.e., they share a "common data model").   

A Data section contains a grammar element describing how to parse the
input file in terms of preterminals and literals, and a Semantics
element that explains how to interpret the preterminals from the Grammar
section. The term "preterminal" here corresponds to nonterminal symbols
that directly expand into the terminal data, i.e., they give names to
certain sequences of terminal tokens.

The mappings from preterminals to token sequences (actually, strings)
are called rowdicts in the document.  The semantics describe how these
rowdicts are transformed into records.  These map names (column names
for data bases, field names for VOTables) to parsed values -- rowdicts
only have strings as values, records contain ints, floats, DateTime
objects, strings, whatever.

Grammars
********

Depending on what the input is, we want different types of grammars.
Thus, there are various grammar elements, but each Data element only has
exactly one of them.  Currently defined are:

* NullGrammar -- when you don't want to do any parsing at all.  You'll
  want that if you just need to define a table through a resource
  descriptor.  The product table (see `The Products Interface`_) is an 
  example of why this is necessary: No single resource can "own" it, but 
  something needs to own it.
* CFGrammar -- when you need context free grammars.  This is based on
  pyparsing and is currently unmaintained, mainly because I have no
  source that actually needs it.
* REGrammar -- when regular expressions plus a bit will do.  These are
  what you'll want to use for text tables.  Basically, you specify some
  way to tokenize your input and REs for the individual tokens.
* KeyValueGrammar -- used for files that give key-value pairs, one file
  per row
* FitsGrammar -- used for fits files.  Currently, only the fields of the
  primary header are used, but once I have fits tables, it shouldn't be
  hard to retrofit that.

All grammars support macros (see Macros_).

Tokenizers available to REGrammars
..................................

We currently have three types of tokenizers (given in the type attribute of the element):

* split -- the content matches a separator between the fields (each row is processed by re.split()
* match -- the content matches an entire row, the fields being a groups defined. This probably is convenient with the {}-notation: (.{4}) (.{8}) (.{3}) would match a table with 4, 8, and 3 wide fields, separated by single spaces. Since this is so common, we'll probably provide an abbreviated syntax for this.
* colranges -- the content is a whitespace-seperated list of columns or
  column ranges (#-style comments allowed).  Example: 1 4-5 10-20 produces
  three tokens, one from column 1 (the start of the string), one
  consisting of the characters at positions 4 and 5 (note the difference
  to python slice notation: the end of the range is *included*), and one
  of length 11, spanning columns 10 through 20.

Macros
******

Unfortunately, the input data isn't always in the format we'd like it to
have.  Some of these issues can be solved by giving literalForms (see
Semantics_) and the like, but frequently manipulations involve more than
one token or require.  To get a halfway declarative way of defining
these, macros are used.  Macros take a set of arguments and work on
rowdicts.  Their effects are reflected in rowdicts.

To understand macros, let's look at two examples:

::

			<Macro name="mapValue" sourceName="maidanak/res/maydanak_targets.txt">
				<arg name="value" source="OBJECT"/>
				<arg name="destination" value="ARI_OBJC"/>
			</Macro>

Here, we specify that a macro called mapValue is to be applied to the
rowdicts.  This specific macro needs a file specifying the mapping,
which is given as a "construction argument" sourceName.

During the application of the macro, it takes two arguments, the value
to be mapped and a destination field.  Macro arguments can have two
sources -- either a literal, were the value given is directly passed to
the macro (the argument is the same for all rowdicts), or the value of a
preterminal.  In the former case, you use the value attribute, in the
latter, the source.  Thus, the macro above always has ARI_OBJC as value
for its destination argument, whereas the value argument is taken from
the OBJECT field of the rowdict.

::

			<Macro name="handleEquatorialPosition">
				<arg name="alpha" source="ALPHA"/>
				<arg name="delta" source="DELTA"/>
			</Macro>

This specification says: "Take the values of ALPHA and DELTA in the
rowdict and pass them as arguments alpha and delta to the macro
handleEquatorialPosition".  As a result in this case, you'll have new fields
alphaFloat, deltaFloat, c_x, c_y, c_z, and htmid in the rowdict -- this
is not visible from the macro definiton but is hardcoded in the macro.

Macros live in gavo/parsing/macros.py.  The following is documentation
generated from there.  To figure out what arguments to give in XML,
look at the constructors in the test cases.  Their second argument gives
the arguments in triples, where the first is the argument name, the
second is a field name (attribute source), and the last is a constant
(attribute value).  So, if the constructor is called like this:

::

  	>>> r = ReSubstitutor(None, [("destination", "", "fixed"),
	... ("data", "broken", ""), ("srcRe", "", r"(.) \(([^)]*)\)"),
	... ("destRe", "", r"\2 \1")])

the corresponding XML would be

::

  <Macro name="subsRe">
		<arg name="destination" value="fixed"/>
		<arg name="data" source="broken"/>
		<arg name="srcRe" value="(.) \(([^)]*)\)"/>
		<arg name="destRe" value="\2 \1"/>
  </Macro>

You may want to update the docs by replacing them with
the output of ``python gavo/parsing/macros.py docs`` now and then.  If
you're in vi in a source checkout, you can go to the next headline and
then say ``:.,/.. END AUTO/!python ../gavo/parsing/macros.py docs``.


combineTimestamps
.................

::

  is a macro that takes a date and a time in various formats
  and produces a mx.DateTime object from it.
  
  Use its result like
  <Field dest="someTimestamp" dbtype="timestamp" source="myTimestamp"
  	literalForm="do not touch"/>
  
  (where myTimestamp was the value of destination).
  
  Arguments:
  
  * destination -- the name of the field the result is to be put in
  * date -- a date literal
  * time -- a time literal
  * dateFormat -- format of date using strptime(3)-compatible conversions
  * timeFormat -- format of time using strptime(3)-compatible conversions
  
  >>> m = TimestampCombiner(None, [("destination", "", "stamp"),
  ...   ("date", "date", ""), ("dateFormat", "", "%d.%m.%Y"),
  ...   ("time", "time", ""), ("timeFormat", "", "%H:%M:%S")])
  >>> rec = {"date": "4.6.1969", "time": "4:22:33"}
  >>> m(rec)
  >>> rec["stamp"].strftime()
  'Wed Jun  4 04:22:33 1969'
  >>> m = TimestampCombiner(None, [("destination", "", "stamp"),
  ...   ("date", "date", ""), ("dateFormat", "", "%d.%m.%Y"),
  ...   ("time", "time", ""), ("timeFormat", "", "!!secondsSinceMidnight")])
  >>> rec = {"date": "4.6.1969", "time": "32320"}
  >>> m(rec)
  >>> rec["stamp"].strftime()
  'Wed Jun  4 08:58:40 1969'
  

concat
......

::

  is a macro that concatenates values from various rows and puts
  the resulting value in destinationRow.
  
  Construction Argument:
  
  * joiner -- a string used to glue the individual values together.  Optional,
    defaults to the empty string.
  
  Arguments:
  
  * destination -- a name under which the concatenated value should be stored
  * sources -- a comma seperated list of source field names
  
  >>> v = ValueCatter(None, joiner="<>", argTuples=[("destination", "", "cat"),
  ... ("sources", "", "src1,src2,src3")])
  >>> r = {"src1": "opener", "src2": "catfood", "src3": "can"}
  >>> v(r)
  >>> r["cat"]
  'opener<>catfood<>can'
  

handleEquatorialPosition
........................

::

  is a macro that compute several derived quantities from 
  sexagesimal equatorial coordinates.
  
  Specifically, it generates alphaFloat, deltaFloat as well as
  c_x, c_y, c_z (cartesian coordinates of the intersection of the 
  direction vector with the unit sphere) and htmind (an HTM index
  for the position -- needs to be fleshed out a bit).
  
  TODO: Equinox handling (this will probably be handled through an
  optional arguments srcEquinox and destEquinox, both J2000.0 by default).
  
  Arguments: 
   
   * alpha -- sexagesimal right ascension as hour angle
   * delta -- sexagesimal declination as dms
   * alphaSepChar (optional) -- seperator for alpha, defaults to whitespace
   * deltaSepChar (optional) -- seperator for delta, defaults to whitespace
  
  The field structure generated here is reflected in 
  __common__/positionfields.template.  If you change anything here,
  change it there, too.  And use that template when you use this macro.
  
  >>> m = EquatorialPositionConverter(None, [("alpha", "alphaRaw", ""),
  ... ("delta", "deltaRaw", "")])
  >>> r = {"alphaRaw": "00 02 32", "deltaRaw": "+45 30.6"}
  >>> m(r)
  >>> str(r["alphaFloat"]), str(r["deltaFloat"]), str(r["c_x"]), str(r["c_y"])
  ('0.633333333333', '45.51', '0.700741955529', '0.00774614323406')
  >>> m = EquatorialPositionConverter(None, [("alpha", "alphaRaw", ""),
  ... ("delta", "deltaRaw", ""), ("alphaSepChar", "", "-"), 
  ... ("deltaSepChar", "", ":")])
  >>> r = {"alphaRaw": "10-37-19.544070", "deltaRaw": "+35:34:20.45713"}
  >>> m(r)
  >>> str(r["alphaFloat"]), str(r["deltaFloat"]), str(r["c_z"])
  ('159.331433625', '35.5723492028', '0.581730502028')
  >>> r = {"alphaRaw": "4-38-54", "deltaRaw": "-12:7.4"}
  >>> m(r)
  >>> str(r["alphaFloat"]), str(r["deltaFloat"])
  ('69.725', '-12.1233333333')
  

interpolateStrings
..................

::

  is a macro that exposes %-type string interpolations.
  
  Arguments:
  
  * destination: name of the field the result should be put in
  * format: a format with %-conversions (see the python manual.
    Note, however, that rowdict values usually are strings).
  * sources: a comma-seperated list of preterminal names.  The
    values of these preterminals constitute the tuple to fill
    the conversions from.
  
  Clearly, you have to have as many items in sources as you have conversions 
  in format.
  >>> s = StringInterpolator(None, [("destination", "", "baz"), 
  ... ("format", "", "no %s in %s"), ("sources", "", "bar,foo")])
  >>> r = {"foo": "42", "bar": "23"}
  >>> s(r); r["baz"]
  'no 23 in 42'
  

mapToNone
.........

::

  is a macro that maps a certain literal to None.
  
  In general, this isn't necessary since you can define null values in fields.
  However, when another macro needs Nones to signify null values, you need
  this macro, because macros are applied before fields are even looked at.
  
  Arguments:
  
  * colName -- the name of the column to operate on
  * nullvalue -- the value that should be mapped to Null
  
  >>> n = NullValuator(None, [("colName", "", "foo"), ("nullvalue", "",
  ... "EMPTY")])
  >>> r = {"foo": "bar", "baz": "bang"}
  >>> n(r); r["foo"]
  'bar'
  >>> r["foo"] = "EMPTY"
  >>> n(r); print r["foo"]
  None
  

mapValue
........

::

  is a macro that translates vaules via a utils.NameMap
  
  Construction arguments:
  
  * sourceName -- an inputsDir-relative path to the NameMap source file,
  * logFailures (optional) -- if somehow true, non-resolved names will 
    be logged
  
  Arguments:
  
  * value -- the value to be mapped.
  * destination -- the field the mapped value should be written into.
  
  If an object cannot be resolved, a null value is entered (i.e., you
  shouldn't get an exception out of this macro but can weed out "bad"
  records through notnull-conditions later if you wish).
  
  Destination may of course be the source field (though that messes
  up idempotency of macro expansion, which shouldn't usually hurt).
  
  The format of the mapping file is
  
  <target key><tab><source keys>
  
  where source keys is a whitespace-seperated list of values that should
  be mapped to target key (sorry the sequence's a bit unusual).
  
  A source key must be encoded quoted-printable.  This usually doesn't
  matter except when it contains whitespace (a blank becomes =20) or equal
  signs (which become =3D).
  

setProdtblValues
................

::

  is a macro that provides all values requried for the product table.
  
  See the documentation on the product interface.
  
  Arguments:
  * prodtblKey -- the value that identifies the product in the product table
    (usually, but not always, it's the path to the product)
  * prodtblOwner -- the owner of the record
  * prodtblEmbargo -- date when the resource will become freely accessible
  * prodtblPath -- path to the product
  * prodtblFsize -- size of the product (optional)
  
  This has to reflect any changes to gavo.inputsDir/products/products.vord.
  

subsRe
......

::

  is a macro that exposes re.sub.
  
  In short, you can substitue PCRE-compatible regular expressions in a
  string, complete with backreferences and all.  Don't overdo it.
  
  Arguments:
  
  * destination -- the name of the field the result should be put in
  * data -- the value the re should be applied to
  * srcRe -- the source regular expression
  * destRe -- the replacement pattern
  
  >>> m = ReSubstitutor(None, [("destination", "", "fixed"),
  ... ("data", "broken", ""), ("srcRe", "", r"(.) \(([^)]*)\)"),
  ... ("destRe", "", r"\2 \1")])
  >>> r = {"broken": "r (Gunn)"}
  >>> m(r); r["fixed"]
  'Gunn r'
  >>> r = {"broken": "Any ol' junk"}
  >>> m(r); r["fixed"]
  "Any ol' junk"
  


.. END AUTO



Semantics
*********

The semantics contain one or more record defintions, where each of these
mainly consists of field definitions.

Field Definitions
.................

The field definition defines a single field within a table. It is
described by the following attributes (only the ones marked with [*] are
strictly necessary):

* dest -- a name that is used as an identifier for this column (should be an SQL identifier) [*]
* source -- the source from which to fill the column. This can be a simple name, which is then used as the preterminal the field is fed from. If the value of source starts with an @, the rest is interpreted as the name of a field computer, which is called when the value of the field is required (see below).
* dbtype -- the type of the value. We use the sql type system and map it to the others involved (python, votable). Defaults to real.
* default -- a literal used as a default value.
* unit -- the unit of the value, given as given in http://vizier.u-strasbg.fr/doc/catstd-3.2.htx, defaults to empty.
* optional -- if true (in any capitalization), empty values (Python: None, SQL: NULL) are allowed for this field, defaults to True.
* ucd -- a description of the field content, as per http://www.ivoa.net/Documents/latest/UCDlist.html.
* description -- human-digestable text that describes the contents of this field.
* nullvalue -- a literal that is interpreted as SQL NULL.
* literalForm -- an identifier that tells ImportTools's type converter
  about special literal forms (e.g. hour angle for float, weird date
  formats). These identifiers are defined in typeconversion.py.
  Probably the most important one is "do not touch" which instructs the
  type converter to leave the value alone.  This is important when,
  e.g., macros or field computers already deliver python types as
  values.  These typically do not appreciate being treated as strings.


Row interfaces
==============

Row interfaces work a bit like java interfaces in that they define
certain columns that have to be present in tables to make them suitable
for standard queries.   You basically say something like "this table
describes things with positions" or "this table describes files
(products)".

You don't just put stuff into the database, you want to get them out as
well.  The interfaces allow clients to rely on the presence of certain
fields with a defined semantics.

As far as resource descriptors are concerned, the interfaces are defined
through XML fragments targetted for Records and usually macros preparing
the data.  The XML fragments reside in ``gavo.inputsDir/__common__``.
To use one of these, you usually have to define entities refering to
them and then put the entities to the appropriate places.

Btw, when some instruction tells you to do this or that in your
"internal subset", what is meant is that you need to put something like

::

<!DOCTYPE GavoResourceDescriptor [
	... anything that's here is the internal subset ...
]>

at the top of your XML file (right after an XML declaration, if you have
one).

Note that I'll probably change the declaration of interfaces at some
point -- they shouldn't be spilled all over the place.  Instead, we
probably want to have one single place in which there's everything an
interface wants, field definitions, macros, additional tables and so on.

The Products interface
**********************

The products interface is defined in productfields.template.  Refer to
this file for a list of defined fields and usage information.

Tables providing products must also enter their data into the product
table, a system-global table mapping keys to files (which is usually
trivial, since the product key is the path in most of the cases).
The main point of the product table is to allow programs to check the
ownership status (i.e., who owns the thing and when will it become free) 
of a product without needing more information than the key.  The
definition of this producttable is in producttable.template.  The
producttable entity needs to be in a Semantics element (it expands into
a Record element).  It also needs to be lexically above the parent table
element.  This ensures that products are entered into the product table
first and are available for being referenced in the productfields
interface.

In sum, a resource descriptor for a table containing products should
look something like this:

::

  <?xml version="1.0" encoding="utf-8"?>

  <!DOCTYPE GavoResourceDescriptor [
    <!ENTITY producttable SYSTEM "producttable">
    <!ENTITY productfields SYSTEM "productfields">
    <!ENTITY prodtbl-sourcetable "XXXXXXXXXXX">
  ]>


  <ResourceDescriptor srcdir="XXXXXXXXXXX">
    
    <Data...>
      <Grammar...>
        <Macro name="setProdtblValues">
          <arg name="prodtblKey" value="@inputRelativePath"/>
          <arg name="prodtblOwner" value="XXXXXX"/>
          <arg name="prodtblEmbargo" value="XXXX-12-31"/>
          <arg name="prodtblPath" value="@inputRelativePath"/>
          <arg name="prodtblFsize" value="@inputSize"/>
        </Macro>
        ...
      </Grammar>
      <Semantics>
        &producttable;

        <Record table="...">
          ... Field definitions ...

          &productfields;
        </Record>
      </Semantics>
    </Data>
  </ResourceDescriptor>


The Positions interface
***********************

The positions interface applies to anything that has a position.  Its
definition is contained in positionfields.template.  Note that client
software may assume processed positions are given in J2000.0
coordinates.  If you ignore this, you'll have to live with the
consequences.

A short documentation is contained in the template file.  A resource
descriptor containing positions might look like this:

::

  <?xml version="1.0" encoding="utf-8"?>

  <!DOCTYPE GavoResourceDescriptor [
    <!ENTITY positionfields SYSTEM "positionfields">
  ]>

  ...
      <Grammar ...>
        ...
        <Macro name="handleEquatorialPosition">
          <arg name="alpha" source="ALPHA RAW"/>
          <arg name="delta" source="DELTA RAW"/>
        </Macro>
        ...
      </Grammar>
     
      ...

        <Record table="data">
          ...
          &productfields;
        </Record>
  ...




TODO: Fields, @-expansions, row processors, schema, meta, recreateAfter,
common elements...


.. vi:et:tw=72:
